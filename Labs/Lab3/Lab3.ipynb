{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b849fcfb",
   "metadata": {},
   "source": [
    "# Lab 3: Mathematical Foundations of Generative Models - Hands-On Implementation\n",
    "**Course: Diffusion Models: Theory and Applications**  \n",
    "**Duration: 90 minutes**  \n",
    "**Team Size: 2 students (same teams from Labs 1-2)**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, students will be able to:\n",
    "1. **Implement** the complete ELBO derivation from first principles\n",
    "2. **Build** KL divergence calculations and explore their asymmetric properties\n",
    "3. **Create** Jensen's inequality demonstrations showing how lower bounds work\n",
    "4. **Construct** the two-forces analysis of reconstruction vs regularization\n",
    "5. **Connect** mathematical theory to practical optimization algorithms\n",
    "6. **Prepare** the foundation for understanding diffusion model mathematics\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup and Mathematical Framework\n",
    "\n",
    "### Part 1: Team Setup & Mathematical Mission (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebda8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical foundations setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from typing import Tuple, Dict\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducible mathematics\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Mathematical computing on: {device}\")\n",
    "\n",
    "# Create synthetic 2D data for testing\n",
    "def create_test_data(n_samples: int = 500) -> torch.Tensor:\n",
    "    \"\"\"Create 2D spiral data for testing our mathematical implementations\"\"\"\n",
    "    t = torch.linspace(0, 3*math.pi, n_samples)\n",
    "    x = t * torch.cos(t) + 0.1 * torch.randn(n_samples)\n",
    "    y = t * torch.sin(t) + 0.1 * torch.randn(n_samples)\n",
    "    data = torch.stack([x, y], dim=1)\n",
    "    return data.to(device)\n",
    "\n",
    "# Generate test data\n",
    "test_data = create_test_data(500)\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Visualize test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_data[:, 0].cpu(), test_data[:, 1].cpu(), alpha=0.6, s=20)\n",
    "plt.title('Test Data: 2D Spiral')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dded760",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Intractable Likelihood Crisis (20 minutes)\n",
    "\n",
    "### Task 2.1: Experience Why Direct Likelihood Fails\n",
    "\n",
    "**Your Mission**: Implement Monte Carlo likelihood estimation and see why it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d959f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntractableLikelihoodDemo:\n",
    "    \"\"\"\n",
    "    Demonstrate why direct likelihood computation fails for generative models.\n",
    "    You'll implement the mathematical computations to see the problems firsthand.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int = 2, data_dim: int = 2):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # Simple generative model: p(x|z) = N(f_θ(z), σ²I)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, data_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.noise_std = 0.2\n",
    "        \n",
    "    def sample_prior(self, n_samples: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement prior sampling\n",
    "        \n",
    "        Sample from p(z) = N(0, I)\n",
    "        \n",
    "        Returns:\n",
    "            z_samples: (n_samples, latent_dim)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "        \n",
    "    def likelihood_given_z(self, x: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement p(x|z) computation\n",
    "        \n",
    "        Compute the likelihood p(x|z) = N(x; f_θ(z), σ²I)\n",
    "        \n",
    "        Steps:\n",
    "        1. Pass z through decoder to get mean μ = f_θ(z)\n",
    "        2. Compute Gaussian likelihood with fixed variance σ²\n",
    "        3. Return likelihood values (not log-likelihood)\n",
    "        \n",
    "        Args:\n",
    "            x: Data points (batch_size, data_dim)\n",
    "            z: Latent codes (batch_size, latent_dim)\n",
    "            \n",
    "        Returns:\n",
    "            likelihoods: p(x|z) for each pair (batch_size,)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Hint: Use Normal distribution from torch.distributions\n",
    "        pass\n",
    "    \n",
    "    def approximate_marginal_likelihood(self, x: torch.Tensor, n_samples: int = 1000) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement Monte Carlo approximation of p(x)\n",
    "        \n",
    "        Approximate p(x) = ∫ p(x|z)p(z) dz ≈ (1/K) Σ p(x|z_k) where z_k ~ p(z)\n",
    "        \n",
    "        This will fail for high dimensions, showing why we need variational inference!\n",
    "        \n",
    "        Steps:\n",
    "        1. Sample many z values from prior\n",
    "        2. For each data point x_i, compute p(x_i|z_k) for all sampled z_k\n",
    "        3. Average these likelihood values\n",
    "        4. Return the Monte Carlo estimate\n",
    "        \n",
    "        Args:\n",
    "            x: Data points (batch_size, data_dim)\n",
    "            n_samples: Number of Monte Carlo samples\n",
    "            \n",
    "        Returns:\n",
    "            approx_p_x: Approximated p(x) for each data point (batch_size,)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        pass\n",
    "    \n",
    "    def demonstrate_failure(self, test_data: torch.Tensor):\n",
    "        \"\"\"Run the demonstration showing why direct likelihood computation fails\"\"\"\n",
    "        print(\"=== Demonstrating Why Direct Likelihood Fails ===\\n\")\n",
    "        \n",
    "        # Test with increasing numbers of Monte Carlo samples\n",
    "        sample_counts = [10, 100, 1000, 5000]\n",
    "        test_points = test_data[:3]  # Test on 3 points\n",
    "        \n",
    "        for n_samples in sample_counts:\n",
    "            print(f\"Using {n_samples} Monte Carlo samples:\")\n",
    "            \n",
    "            # Time the computation\n",
    "            start_time = time.time()\n",
    "            approx_likelihood = self.approximate_marginal_likelihood(test_points, n_samples)\n",
    "            computation_time = time.time() - start_time\n",
    "            \n",
    "            # Run multiple times to show variance\n",
    "            estimates = []\n",
    "            for trial in range(5):\n",
    "                estimate = self.approximate_marginal_likelihood(test_points[:1], n_samples)\n",
    "                estimates.append(estimate.item())\n",
    "            \n",
    "            # Print results and analysis\n",
    "            print(f\"  Time: {computation_time:.3f}s\")\n",
    "            print(f\"  Estimates: {approx_likelihood.detach().cpu().numpy()}\")\n",
    "            print(f\"  Variance across trials: {np.var(estimates):.6f}\")\n",
    "            print()\n",
    "        \n",
    "        print(\"Analysis:\")\n",
    "        print(\"❌ Computation time grows linearly with samples\")\n",
    "        print(\"❌ High variance in estimates\")\n",
    "        print(\"❌ Most random z give very low p(x|z)\")\n",
    "        print(\"❌ Completely impractical for high-dimensional z\")\n",
    "\n",
    "# Test your implementation (uncomment after implementing TODOs)\n",
    "# demo = IntractableLikelihoodDemo()\n",
    "# demo.demonstrate_failure(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c916c3",
   "metadata": {},
   "source": [
    "### Task 2.2: Understand the Bayes Rule Circular Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_bayes_circularity():\n",
    "    \"\"\"\n",
    "    Show the circular dependency problem in Bayes' rule\n",
    "    \"\"\"\n",
    "    print(\"=== The Bayes Rule Circular Dependency ===\\n\")\n",
    "    \n",
    "    print(\"Bayes' rule: p(z|x) = p(x|z) * p(z) / p(x)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"What we can compute:\")\n",
    "    print(\"  ✓ p(x|z): Decoder/likelihood model\")\n",
    "    print(\"  ✓ p(z): Prior distribution (we choose this)\")\n",
    "    print()\n",
    "    print(\"What we can't compute:\")\n",
    "    print(\"  ✗ p(x): The marginal likelihood = ∫ p(x|z)p(z) dz\")\n",
    "    print()\n",
    "    \n",
    "    print(\"The circular dependency:\")\n",
    "    print(\"  1. We want p(z|x) to learn about latent factors\")\n",
    "    print(\"  2. Bayes rule requires p(x) in the denominator\")\n",
    "    print(\"  3. But p(x) is the same intractable integral!\")\n",
    "    print(\"  4. Can't compute what we need to learn\")\n",
    "    print()\n",
    "    \n",
    "    print(\"💡 Solution: Variational inference!\")\n",
    "    print(\"   Approximate p(z|x) with tractable q(z|x)\")\n",
    "\n",
    "# Run the demonstration\n",
    "demonstrate_bayes_circularity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b2808",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: KL Divergence Implementation (20 minutes)\n",
    "\n",
    "### Task 3.1: Build KL Divergence from Scratch\n",
    "\n",
    "**Your Mission**: Implement KL divergence calculations and understand their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd21b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceBuilder:\n",
    "    \"\"\"\n",
    "    Build KL divergence calculations from first principles.\n",
    "    You'll implement the core mathematical formulas.\n",
    "    \"\"\"\n",
    "    \n",
    "    def monte_carlo_kl(self, p_samples: torch.Tensor, p_logprob_fn, q_logprob_fn) -> float:\n",
    "        \"\"\"\n",
    "        TODO: Implement KL(p||q) using Monte Carlo estimation\n",
    "        \n",
    "        Formula: KL(p||q) = E_p[log p(x) - log q(x)]\n",
    "        Monte Carlo: KL(p||q) ≈ (1/N) Σ [log p(x_i) - log q(x_i)] where x_i ~ p\n",
    "        \n",
    "        Args:\n",
    "            p_samples: Samples from distribution p\n",
    "            p_logprob_fn: Function to compute log p(x)\n",
    "            q_logprob_fn: Function to compute log q(x)\n",
    "            \n",
    "        Returns:\n",
    "            kl_estimate: Estimated KL(p||q)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the Monte Carlo KL estimation\n",
    "        # Step 1: Compute log p(x) for all samples\n",
    "        # Step 2: Compute log q(x) for all samples  \n",
    "        # Step 3: Compute log p(x) - log q(x)\n",
    "        # Step 4: Take the mean\n",
    "        pass\n",
    "    \n",
    "    def gaussian_kl_closed_form(self, mu1: torch.Tensor, sigma1: torch.Tensor,\n",
    "                                mu2: torch.Tensor, sigma2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement closed-form KL for Gaussians\n",
    "        \n",
    "        For p = N(μ₁, σ₁²) and q = N(μ₂, σ₂²):\n",
    "        KL(p||q) = log(σ₂/σ₁) + (σ₁² + (μ₁-μ₂)²)/(2σ₂²) - 1/2\n",
    "        \n",
    "        This closed form is crucial for practical VI.\n",
    "        \n",
    "        Args:\n",
    "            mu1, sigma1: Parameters of first Gaussian\n",
    "            mu2, sigma2: Parameters of second Gaussian\n",
    "            \n",
    "        Returns:\n",
    "            kl: KL divergence\n",
    "        \"\"\"\n",
    "        # TODO: Implement the closed-form formula\n",
    "        # Hint: Be careful with the formula - there are multiple equivalent forms\n",
    "        pass\n",
    "    \n",
    "    def standard_normal_kl(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement KL divergence to standard normal N(0,I)\n",
    "        \n",
    "        This is the most common case in VAEs.\n",
    "        For p = N(μ, σ²I) and q = N(0, I):\n",
    "        KL(p||q) = 0.5 * Σ[μ² + σ² - 1 - log(σ²)]\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean vector (..., latent_dim)\n",
    "            logvar: Log variance vector (..., latent_dim)  [Note: logvar = log(σ²)]\n",
    "            \n",
    "        Returns:\n",
    "            kl: KL divergence for each sample (...,)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the standard normal KL formula\n",
    "        # Remember: logvar = log(σ²), so σ² = exp(logvar)\n",
    "        pass\n",
    "    \n",
    "    def explore_kl_asymmetry(self):\n",
    "        \"\"\"\n",
    "        Demonstrate KL divergence asymmetry\n",
    "        \"\"\"\n",
    "        print(\"=== KL Divergence Asymmetry ===\\n\")\n",
    "        \n",
    "        # Define two different Gaussians\n",
    "        mu1, sigma1 = torch.tensor(0.0), torch.tensor(0.5)  # Narrow\n",
    "        mu2, sigma2 = torch.tensor(1.0), torch.tensor(1.5)  # Wide\n",
    "        \n",
    "        print(f\"Distribution p: N({mu1:.1f}, {sigma1:.1f}²) [narrow]\")\n",
    "        print(f\"Distribution q: N({mu2:.1f}, {sigma2:.1f}²) [wide]\")\n",
    "        print()\n",
    "        \n",
    "        # Compute both directions (after implementing the TODO above)\n",
    "        kl_p_q = self.gaussian_kl_closed_form(mu1, sigma1, mu2, sigma2)\n",
    "        kl_q_p = self.gaussian_kl_closed_form(mu2, sigma2, mu1, sigma1)\n",
    "        \n",
    "        print(f\"KL(p||q): {kl_p_q:.4f}\")\n",
    "        print(f\"KL(q||p): {kl_q_p:.4f}\")\n",
    "        print(f\"Asymmetry: {abs(kl_p_q - kl_q_p):.4f}\")\n",
    "        \n",
    "        # Verify with Monte Carlo\n",
    "        n_samples = 10000\n",
    "        p_dist = Normal(mu1, sigma1)\n",
    "        q_dist = Normal(mu2, sigma2)\n",
    "        p_samples = p_dist.sample((n_samples,))\n",
    "        \n",
    "        kl_mc = self.monte_carlo_kl(p_samples, p_dist.log_prob, q_dist.log_prob)\n",
    "        print(f\"Monte Carlo verification: {kl_mc:.4f}\")\n",
    "        \n",
    "        # Create visualization showing the asymmetry\n",
    "        self.visualize_asymmetry(mu1, sigma1, mu2, sigma2)\n",
    "    \n",
    "    def visualize_asymmetry(self, mu1, sigma1, mu2, sigma2):\n",
    "        \"\"\"Create plots showing KL asymmetry effects\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Plot 1: The two distributions\n",
    "        x = torch.linspace(-3, 5, 1000)\n",
    "        p_dist = Normal(mu1, sigma1)\n",
    "        q_dist = Normal(mu2, sigma2)\n",
    "        \n",
    "        p_vals = torch.exp(p_dist.log_prob(x))\n",
    "        q_vals = torch.exp(q_dist.log_prob(x))\n",
    "        \n",
    "        ax1.plot(x, p_vals, 'b-', linewidth=2, label=f'p: N({mu1:.1f}, {sigma1:.1f}²)')\n",
    "        ax1.plot(x, q_vals, 'r-', linewidth=2, label=f'q: N({mu2:.1f}, {sigma2:.1f}²)')\n",
    "        ax1.set_title('Distributions p and q')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Log ratio p/q\n",
    "        log_ratio_pq = p_dist.log_prob(x) - q_dist.log_prob(x)\n",
    "        ax2.plot(x, log_ratio_pq, 'g-', linewidth=2)\n",
    "        ax2.set_title('log(p/q) - used in KL(p||q)')\n",
    "        ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Log ratio q/p  \n",
    "        log_ratio_qp = q_dist.log_prob(x) - p_dist.log_prob(x)\n",
    "        ax3.plot(x, log_ratio_qp, 'orange', linewidth=2)\n",
    "        ax3.set_title('log(q/p) - used in KL(q||p)')\n",
    "        ax3.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Weighted contributions\n",
    "        p_samples = p_dist.sample((1000,))\n",
    "        q_samples = q_dist.sample((1000,))\n",
    "        \n",
    "        ax4.hist(p_samples, bins=30, alpha=0.5, label='Samples from p', density=True)\n",
    "        ax4.hist(q_samples, bins=30, alpha=0.5, label='Samples from q', density=True)\n",
    "        ax4.set_title('Sample distributions')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKey insight:\")\n",
    "        print(\"• KL(p||q): 'mode-covering' - q must cover all of p\")\n",
    "        print(\"• KL(q||p): 'mode-seeking' - q focuses on high-density regions of p\")\n",
    "\n",
    "# Test your KL divergence implementations (uncomment after implementing TODOs)\n",
    "# kl_builder = KLDivergenceBuilder()\n",
    "# kl_builder.explore_kl_asymmetry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39705c1",
   "metadata": {},
   "source": [
    "### Task 3.2: Validate Your KL Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_kl_implementations():\n",
    "    \"\"\"\n",
    "    Test your KL divergence implementations\n",
    "    \n",
    "    Verify that your formulas are correct by comparing different methods.\n",
    "    \"\"\"\n",
    "    print(\"=== Testing KL Implementations ===\\n\")\n",
    "    \n",
    "    kl_builder = KLDivergenceBuilder()\n",
    "    \n",
    "    # Test case 1: Simple Gaussians\n",
    "    mu1, sigma1 = torch.tensor(1.0), torch.tensor(0.8)\n",
    "    mu2, sigma2 = torch.tensor(0.0), torch.tensor(1.0)\n",
    "    \n",
    "    print(\"Test 1: Gaussian KL\")\n",
    "    print(f\"p = N({mu1:.1f}, {sigma1:.1f}²)\")\n",
    "    print(f\"q = N({mu2:.1f}, {sigma2:.1f}²)\")\n",
    "    \n",
    "    # Test your closed-form implementation\n",
    "    kl_closed = kl_builder.gaussian_kl_closed_form(mu1, sigma1, mu2, sigma2)\n",
    "    print(f\"Your implementation: {kl_closed:.6f}\")\n",
    "    \n",
    "    # Test against known analytical result\n",
    "    # Manual calculation for verification\n",
    "    analytical = 0.5 * ((sigma1/sigma2)**2 + ((mu1-mu2)/sigma2)**2 - 1 - 2*torch.log(sigma1/sigma2))\n",
    "    print(f\"Expected result: {analytical:.6f}\")\n",
    "    print(f\"Match: {torch.allclose(kl_closed, analytical, atol=1e-5) if kl_closed is not None else 'Implement TODO first'}\")\n",
    "    \n",
    "    # Test your Monte Carlo implementation\n",
    "    p_dist = Normal(mu1, sigma1)\n",
    "    q_dist = Normal(mu2, sigma2)\n",
    "    p_samples = p_dist.sample((10000,))\n",
    "    kl_mc = kl_builder.monte_carlo_kl(p_samples, p_dist.log_prob, q_dist.log_prob)\n",
    "    print(f\"Monte Carlo: {kl_mc}\")\n",
    "    \n",
    "    # Test case 2: Standard normal KL\n",
    "    print(f\"\\nTest 2: KL to standard normal\")\n",
    "    mu_test = torch.tensor([1.0, -0.5])\n",
    "    logvar_test = torch.tensor([0.5, -0.2])\n",
    "    \n",
    "    # Test your standard normal KL\n",
    "    kl_std = kl_builder.standard_normal_kl(mu_test, logvar_test)\n",
    "    print(f\"Standard normal KL: {kl_std}\")\n",
    "    \n",
    "    # Expected result for verification\n",
    "    expected_std = 0.5 * (mu_test**2 + torch.exp(logvar_test) - 1 - logvar_test).sum()\n",
    "    print(f\"Expected: {expected_std:.6f}\")\n",
    "    \n",
    "    print(\"\\n✓ All tests completed!\")\n",
    "\n",
    "# Run your tests (uncomment after implementing TODOs)\n",
    "# test_kl_implementations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a917b6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Jensen's Inequality Demonstration (15 minutes)\n",
    "\n",
    "### Task 4.1: Implement Jensen's Inequality\n",
    "\n",
    "**Your Mission**: Build the mathematical foundation that makes ELBO possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1eb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JensensInequalityDemo:\n",
    "    \"\"\"\n",
    "    Implement Jensen's inequality demonstrations.\n",
    "    This is the key mathematical tool that makes variational inference work.\n",
    "    \"\"\"\n",
    "    \n",
    "    def demonstrate_concave_property(self):\n",
    "        \"\"\"\n",
    "        TODO: Show Jensen's inequality for the logarithm\n",
    "        \n",
    "        For concave functions like log: f(E[X]) ≥ E[f(X)]\n",
    "        Specifically: log(E[X]) ≥ E[log(X)]\n",
    "        \"\"\"\n",
    "        print(\"=== Jensen's Inequality for Logarithm ===\\n\")\n",
    "        \n",
    "        # Test with different distributions\n",
    "        test_cases = [\n",
    "            (\"Uniform [1,4]\", torch.distributions.Uniform(1, 4)),\n",
    "            (\"Exponential(1)\", torch.distributions.Exponential(1.0)),\n",
    "            (\"Gamma(2,1)\", torch.distributions.Gamma(2.0, 1.0))\n",
    "        ]\n",
    "        \n",
    "        n_samples = 5000\n",
    "        \n",
    "        print(\"Testing Jensen's inequality: log(E[X]) ≥ E[log(X)]\")\n",
    "        print()\n",
    "        \n",
    "        for name, dist in test_cases:\n",
    "            # TODO: Sample from distribution\n",
    "            samples = dist.sample((n_samples,))\n",
    "            \n",
    "            # TODO: Compute both sides of Jensen's inequality\n",
    "            log_expectation = torch.log(samples.mean())\n",
    "            expectation_log = torch.log(samples).mean()\n",
    "            gap = log_expectation - expectation_log\n",
    "            \n",
    "            print(f\"{name:15s}: log(E[X])={log_expectation:.4f}, E[log(X)]={expectation_log:.4f}, gap={gap:.4f}\")\n",
    "        \n",
    "        print(f\"\\nKey insight: The gap = log(E[X]) - E[log(X)] is our lower bound!\")\n",
    "        print(f\"This gap becomes the tractable ELBO bound on intractable log p(x)\")\n",
    "    \n",
    "    def create_jensen_visualization(self):\n",
    "        \"\"\"\n",
    "        Create geometric visualization of Jensen's inequality\n",
    "        \n",
    "        Show why concave functions create lower bounds.\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Jensen's Inequality Visualization ===\")\n",
    "        \n",
    "        # Create x values and compute log function\n",
    "        x = torch.linspace(0.5, 4, 1000)\n",
    "        log_x = torch.log(x)\n",
    "        \n",
    "        # Pick two points and show linear interpolation vs function value\n",
    "        x1, x2 = 1.0, 3.0\n",
    "        lambda_val = 0.3\n",
    "        \n",
    "        # Compute the key values\n",
    "        x_mix = lambda_val * x1 + (1 - lambda_val) * x2\n",
    "        log_x_mix = torch.log(torch.tensor(x_mix))\n",
    "        mix_log = lambda_val * torch.log(torch.tensor(x1)) + (1 - lambda_val) * torch.log(torch.tensor(x2))\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x, log_x, 'b-', linewidth=3, label='log(x) [concave]')\n",
    "        \n",
    "        # Add points and lines showing Jensen's inequality\n",
    "        plt.plot([x1, x2], [torch.log(torch.tensor(x1)), torch.log(torch.tensor(x2))], 'ro', markersize=8)\n",
    "        plt.plot([x1, x2], [torch.log(torch.tensor(x1)), torch.log(torch.tensor(x2))], 'r--', alpha=0.7, label='Linear interpolation')\n",
    "        plt.plot(x_mix, log_x_mix, 'go', markersize=10, label=f'log(E[X]) = {log_x_mix:.3f}')\n",
    "        plt.plot(x_mix, mix_log, 'mo', markersize=10, label=f'E[log(X)] = {mix_log:.3f}')\n",
    "        \n",
    "        # Add arrow showing the gap\n",
    "        plt.annotate('', xy=(x_mix, log_x_mix), xytext=(x_mix, mix_log),\n",
    "                    arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
    "        plt.text(x_mix + 0.1, (log_x_mix + mix_log)/2, f'Gap = {log_x_mix - mix_log:.3f}',\n",
    "                fontsize=12, color='red', weight='bold')\n",
    "        \n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('log(x)')\n",
    "        plt.title('Jensen\\'s Inequality: log(E[X]) ≥ E[log(X)]')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print the verification\n",
    "        print(f\"Jensen verification: {log_x_mix:.4f} ≥ {mix_log:.4f} ? {log_x_mix >= mix_log}\")\n",
    "\n",
    "# Test Jensen's inequality\n",
    "jensen = JensensInequalityDemo()\n",
    "jensen.demonstrate_concave_property()\n",
    "jensen.create_jensen_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb683c4b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: ELBO Framework Implementation (25 minutes)\n",
    "\n",
    "### Task 5.1: Build the Complete ELBO\n",
    "\n",
    "**Your Mission**: Implement the ELBO framework that makes variational inference practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELBOFramework:\n",
    "    \"\"\"\n",
    "    Implement the complete ELBO framework from scratch.\n",
    "    This is the universal solution for generative model training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int = 2, data_dim: int = 2):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # VAE components (architectures provided, you'll implement the math)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(data_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2 * latent_dim)  # Output mean and logvar\n",
    "        ).to(device)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, data_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.decoder_logvar = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Extract variational parameters q(z|x) = N(μ(x), σ²(x))\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = h.chunk(2, dim=-1)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement the reparameterization trick\n",
    "        \n",
    "        Sample z ~ N(μ, σ²I) using reparameterization: z = μ + σ * ε where ε ~ N(0,I)\n",
    "        \n",
    "        This makes random sampling differentiable!\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean parameters\n",
    "            logvar: Log variance parameters (logvar = log(σ²))\n",
    "            \n",
    "        Returns:\n",
    "            z: Reparameterized samples\n",
    "        \"\"\"\n",
    "        # TODO: Implement reparameterization\n",
    "        # Step 1: Convert logvar to std: σ = exp(0.5 * logvar)\n",
    "        # Step 2: Sample ε ~ N(0,I)  \n",
    "        # Step 3: Compute z = μ + σ * ε\n",
    "        pass\n",
    "    \n",
    "    def reconstruction_loss(self, x: torch.Tensor, x_recon_mu: torch.Tensor, \n",
    "                           x_recon_logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement reconstruction term E_q[log p(x|z)]\n",
    "        \n",
    "        Compute the expected log-likelihood of data under the decoder.\n",
    "        Model: p(x|z) = N(x; μ_decoder(z), σ²_decoder)\n",
    "        \n",
    "        Args:\n",
    "            x: Original data\n",
    "            x_recon_mu: Decoder mean output\n",
    "            x_recon_logvar: Decoder log variance\n",
    "            \n",
    "        Returns:\n",
    "            reconstruction_loss: E_q[log p(x|z)] for each sample\n",
    "        \"\"\"\n",
    "        # TODO: Implement Gaussian log-likelihood\n",
    "        # Formula: log p(x|z) = -0.5 * [log(2π) + log(σ²) + (x-μ)²/σ²]\n",
    "        # Remember to sum over data dimensions\n",
    "        pass\n",
    "    \n",
    "    def kl_regularization(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement KL regularization term KL(q(z|x) || p(z))\n",
    "        \n",
    "        Compute KL divergence between variational posterior and prior.\n",
    "        Assume p(z) = N(0, I) and q(z|x) = N(μ(x), σ²(x)I)\n",
    "        \n",
    "        Args:\n",
    "            mu: Variational mean\n",
    "            logvar: Variational log variance\n",
    "            \n",
    "        Returns:\n",
    "            kl_loss: KL divergence for each sample\n",
    "        \"\"\"\n",
    "        # TODO: Use your standard_normal_kl implementation from earlier\n",
    "        # Or implement directly: KL = 0.5 * Σ[μ² + σ² - 1 - log(σ²)]\n",
    "        pass\n",
    "    \n",
    "    def compute_elbo(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the complete ELBO computation\n",
    "        \n",
    "        ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))\n",
    "             = Reconstruction - Regularization\n",
    "        \n",
    "        Walk through the complete mathematical derivation step by step.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with ELBO components\n",
    "        \"\"\"\n",
    "        # TODO: Step 1 - Encode to get variational parameters\n",
    "        mu_q, logvar_q = self.encode(x)\n",
    "        \n",
    "        # TODO: Step 2 - Sample using reparameterization trick\n",
    "        z = self.reparameterize(mu_q, logvar_q)\n",
    "        \n",
    "        # TODO: Step 3 - Decode to get reconstruction parameters\n",
    "        x_recon_mu = self.decoder(z)\n",
    "        x_recon_logvar = self.decoder_logvar.expand_as(x_recon_mu)\n",
    "        \n",
    "        # TODO: Step 4 - Compute reconstruction term\n",
    "        recon_term = self.reconstruction_loss(x, x_recon_mu, x_recon_logvar)\n",
    "        \n",
    "        # TODO: Step 5 - Compute regularization term\n",
    "        kl_term = self.kl_regularization(mu_q, logvar_q)\n",
    "        \n",
    "        # TODO: Step 6 - Combine for ELBO\n",
    "        elbo = recon_term - kl_term\n",
    "        \n",
    "        return {\n",
    "            'elbo': elbo,\n",
    "            'reconstruction': recon_term,\n",
    "            'kl_divergence': kl_term,\n",
    "            'mu': mu_q,\n",
    "            'logvar': logvar_q,\n",
    "            'z': z,\n",
    "            'x_recon': x_recon_mu\n",
    "        }\n",
    "    \n",
    "    def train_vae(self, data: torch.Tensor, epochs: int = 50, lr: float = 1e-3):\n",
    "        \"\"\"\n",
    "        Train VAE using ELBO - demonstration of practical optimization\n",
    "        \n",
    "        Show how mathematical theory becomes practical optimization.\n",
    "        \"\"\"\n",
    "        print(\"=== Training VAE with ELBO ===\\n\")\n",
    "        \n",
    "        # Set up optimizer\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        losses = {'total': [], 'reconstruction': [], 'kl': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute ELBO (after students implement compute_elbo)\n",
    "            results = self.compute_elbo(data)\n",
    "            elbo = results['elbo']\n",
    "            \n",
    "            # Loss is negative ELBO (we want to maximize ELBO)\n",
    "            loss = -elbo.mean()\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            losses['total'].append(loss.item())\n",
    "            losses['reconstruction'].append(-results['reconstruction'].mean().item())\n",
    "            losses['kl'].append(results['kl_divergence'].mean().item())\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Loss = {loss.item():.3f}, Recon = {losses['reconstruction'][-1]:.3f}, KL = {losses['kl'][-1]:.3f}\")\n",
    "        \n",
    "        # Plot training curves\n",
    "        self.plot_training_curves(losses)\n",
    "        return losses\n",
    "    \n",
    "    def plot_training_curves(self, losses):\n",
    "        \"\"\"Visualize training progress\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        epochs = range(len(losses['total']))\n",
    "        \n",
    "        # Plot 1: Total loss\n",
    "        ax1.plot(epochs, losses['total'], 'b-', linewidth=2, label='Total Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training Loss (Negative ELBO)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot 2: ELBO components\n",
    "        ax2.plot(epochs, losses['reconstruction'], 'r-', linewidth=2, label='Reconstruction')\n",
    "        ax2.plot(epochs, losses['kl'], 'g-', linewidth=2, label='KL Divergence')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss Component')\n",
    "        ax2.set_title('ELBO Components')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Training insights:\")\n",
    "        print(f\"• Final reconstruction loss: {losses['reconstruction'][-1]:.3f}\")\n",
    "        print(f\"• Final KL divergence: {losses['kl'][-1]:.3f}\")\n",
    "        print(f\"• Balance shows reconstruction vs regularization tradeoff\")\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"Helper to get all model parameters\"\"\"\n",
    "        params = list(self.encoder.parameters()) + list(self.decoder.parameters()) + [self.decoder_logvar]\n",
    "        return params\n",
    "\n",
    "# Test your ELBO implementation (uncomment after implementing TODOs)\n",
    "# elbo_model = ELBOFramework(latent_dim=2, data_dim=2)\n",
    "\n",
    "# Test ELBO computation\n",
    "# print(\"Testing ELBO computation...\")\n",
    "# results = elbo_model.compute_elbo(test_data[:10])\n",
    "# print(\"✓ ELBO computation successful!\")\n",
    "\n",
    "# Train the VAE\n",
    "# print(\"Training VAE...\")\n",
    "# losses = elbo_model.train_vae(test_data[:100], epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cc62b",
   "metadata": {},
   "source": [
    "### Task 5.2: The Two Forces Analysis\n",
    "\n",
    "**Your Mission**: Implement the analysis of reconstruction vs regularization tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74897b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoForcesAnalysis:\n",
    "    \"\"\"\n",
    "    Analyze the fundamental tradeoff in variational inference.\n",
    "    You'll implement the β-VAE analysis to understand force balance.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.kl_builder = KLDivergenceBuilder()\n",
    "    \n",
    "    def analyze_beta_vae(self, data: torch.Tensor, beta_values: list = [0.0, 0.1, 1.0, 5.0, 10.0]):\n",
    "        \"\"\"\n",
    "        TODO: Implement β-VAE analysis\n",
    "        \n",
    "        Test different weightings: Loss = -Reconstruction + β * KL\n",
    "        Show how β controls the reconstruction vs regularization tradeoff.\n",
    "        \n",
    "        Args:\n",
    "            data: Training data\n",
    "            beta_values: Different β weights to test\n",
    "        \"\"\"\n",
    "        print(\"=== Two Forces Analysis: β-VAE Experiment ===\\n\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for beta in beta_values:\n",
    "            print(f\"Training with β = {beta}\")\n",
    "            \n",
    "            # Create a new model for each β\n",
    "            model = ELBOFramework(latent_dim=2, data_dim=2)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            \n",
    "            # Train with modified loss\n",
    "            for epoch in range(20):  # Shorter training for comparison\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # TODO: Students implement compute_elbo, then this works\n",
    "                elbo_results = model.compute_elbo(data)\n",
    "                reconstruction = elbo_results['reconstruction'].mean()\n",
    "                kl_divergence = elbo_results['kl_divergence'].mean()\n",
    "                \n",
    "                # Modified loss with β weighting\n",
    "                loss = -reconstruction + beta * kl_divergence\n",
    "                \n",
    "                # Optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Evaluate final model\n",
    "            with torch.no_grad():\n",
    "                final_results = model.compute_elbo(data)\n",
    "                results[beta] = {\n",
    "                    'reconstruction': final_results['reconstruction'].mean().item(),\n",
    "                    'kl_divergence': final_results['kl_divergence'].mean().item(),\n",
    "                    'latent_samples': final_results['z'].cpu(),\n",
    "                    'reconstructions': final_results['x_recon'].cpu()\n",
    "                }\n",
    "        \n",
    "        # Visualize the results\n",
    "        self.visualize_beta_effects(results, data)\n",
    "    \n",
    "    def visualize_beta_effects(self, results: dict, original_data: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create visualizations showing β effects\n",
    "        \n",
    "        Show how different β values affect:\n",
    "        1. Reconstruction vs KL tradeoff\n",
    "        2. Latent space organization  \n",
    "        3. Reconstruction quality\n",
    "        \"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Metrics vs β\n",
    "        betas = list(results.keys())\n",
    "        recon_losses = [results[b]['reconstruction'] for b in betas]\n",
    "        kl_losses = [results[b]['kl_divergence'] for b in betas]\n",
    "        \n",
    "        ax1.plot(betas, recon_losses, 'ro-', linewidth=2, markersize=8, label='Reconstruction Loss')\n",
    "        ax1.set_xlabel('β (KL weight)')\n",
    "        ax1.set_ylabel('Reconstruction Loss', color='r')\n",
    "        ax1.tick_params(axis='y', labelcolor='r')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax1_twin = ax1.twinx()\n",
    "        ax1_twin.plot(betas, kl_losses, 'bo-', linewidth=2, markersize=8, label='KL Divergence')\n",
    "        ax1_twin.set_ylabel('KL Divergence', color='b')\n",
    "        ax1_twin.tick_params(axis='y', labelcolor='b')\n",
    "        \n",
    "        ax1.set_title('Two Forces Tradeoff')\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax1_twin.legend(loc='upper right')\n",
    "        \n",
    "        # Plot 2: Latent space organization for different β\n",
    "        colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "        for i, (beta, color) in enumerate(zip([0.0, 1.0, 10.0], colors[:3])):\n",
    "            if beta in results:\n",
    "                z_samples = results[beta]['latent_samples']\n",
    "                ax2.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.6, s=20, \n",
    "                           color=color, label=f'β={beta}')\n",
    "        \n",
    "        ax2.set_xlabel('Latent Dimension 1')\n",
    "        ax2.set_ylabel('Latent Dimension 2')\n",
    "        ax2.set_title('Latent Space Organization')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Original data\n",
    "        ax3.scatter(original_data[:, 0].cpu(), original_data[:, 1].cpu(), \n",
    "                   alpha=0.6, s=20, color='black')\n",
    "        ax3.set_title('Original Data')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Reconstruction examples for different β\n",
    "        sample_indices = torch.randperm(len(original_data))[:5]\n",
    "        for i, beta in enumerate([0.0, 1.0, 10.0]):\n",
    "            if beta in results:\n",
    "                recons = results[beta]['reconstructions'][sample_indices]\n",
    "                ax4.scatter(recons[:, 0], recons[:, 1], alpha=0.8, s=60,\n",
    "                           color=colors[i], label=f'β={beta} recons')\n",
    "        \n",
    "        # Show original points for comparison\n",
    "        orig_sample = original_data[sample_indices]\n",
    "        ax4.scatter(orig_sample[:, 0].cpu(), orig_sample[:, 1].cpu(), \n",
    "                   alpha=0.8, s=60, color='black', marker='x', s=100, label='Original')\n",
    "        \n",
    "        ax4.set_title('Reconstruction Quality')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKey insights from β-VAE analysis:\")\n",
    "        print(\"• β = 0: Perfect reconstruction, random latent structure\")\n",
    "        print(\"• β = 1: Balanced tradeoff (standard VAE)\")  \n",
    "        print(\"• β >> 1: Structured latents, poor reconstruction\")\n",
    "        print(\"• The 'sweet spot' depends on your application goals\")\n",
    "    \n",
    "    def explain_two_forces(self):\n",
    "        \"\"\"\n",
    "        Explain the fundamental two-forces concept\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Understanding the Two Forces ===\\n\")\n",
    "        \n",
    "        print(\"ELBO = E_q[log p(x|z)] - KL(q(z|x) || p(z))\")\n",
    "        print(\"     = Reconstruction  - Regularization\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Force 1 - Reconstruction: E_q[log p(x|z)]\")\n",
    "        print(\"  • Wants: Perfect reconstruction of input data\")\n",
    "        print(\"  • Pushes: Encoder to preserve all information\")\n",
    "        print(\"  • Says: 'Use whatever latent codes work best!'\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Force 2 - Regularization: KL(q(z|x) || p(z))\")\n",
    "        print(\"  • Wants: Latent codes close to prior distribution\")\n",
    "        print(\"  • Pushes: Encoder toward structured representations\")\n",
    "        print(\"  • Says: 'Stay close to simple distributions!'\")\n",
    "        print()\n",
    "        \n",
    "        print(\"The beautiful tension:\")\n",
    "        print(\"  ⚖️  Perfect reconstruction vs structured latents\")\n",
    "        print(\"  💎 This tension creates meaningful representations\")\n",
    "        print(\"  🎯 Balance determines model behavior\")\n",
    "        print()\n",
    "        \n",
    "        # Create a conceptual visualization\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        \n",
    "        # Draw the two forces as arrows\n",
    "        ax.arrow(0.2, 0.5, 0.25, 0, head_width=0.05, head_length=0.02, \n",
    "                fc='red', ec='red', linewidth=3)\n",
    "        ax.text(0.32, 0.6, 'Reconstruction\\nForce', ha='center', fontsize=12, \n",
    "                color='red', weight='bold')\n",
    "        \n",
    "        ax.arrow(0.8, 0.5, -0.25, 0, head_width=0.05, head_length=0.02,\n",
    "                fc='blue', ec='blue', linewidth=3)\n",
    "        ax.text(0.68, 0.6, 'Regularization\\nForce', ha='center', fontsize=12,\n",
    "                color='blue', weight='bold')\n",
    "        \n",
    "        # Show the balance point\n",
    "        ax.plot(0.5, 0.5, 'go', markersize=15, label='Optimal Balance')\n",
    "        ax.text(0.5, 0.35, 'ELBO\\nOptimum', ha='center', fontsize=12,\n",
    "                color='green', weight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0.2, 0.8)\n",
    "        ax.set_title('The Two Forces in ELBO', fontsize=16, weight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run two forces analysis (uncomment after implementing TODOs)\n",
    "two_forces = TwoForcesAnalysis()\n",
    "two_forces.explain_two_forces()\n",
    "# two_forces.analyze_beta_vae(test_data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c756651e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Mathematical Validation and Testing (10 minutes)\n",
    "\n",
    "### Task 6.1: Validate Your Implementations\n",
    "\n",
    "**Your Mission**: Test that your mathematical implementations are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ee84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_validation():\n",
    "    \"\"\"\n",
    "    Implement comprehensive testing of all mathematical components\n",
    "    \n",
    "    Verify that your implementations match theoretical predictions.\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive Mathematical Validation ===\\n\")\n",
    "    \n",
    "    # Test 1: KL Divergence Validation\n",
    "    print(\"Test 1: KL Divergence Implementation\")\n",
    "    \n",
    "    # Create test distributions\n",
    "    mu1, sigma1 = torch.tensor(0.5), torch.tensor(0.8)\n",
    "    mu2, sigma2 = torch.tensor(0.0), torch.tensor(1.0)\n",
    "    \n",
    "    # Test your implementation against known formula\n",
    "    kl_builder = KLDivergenceBuilder()\n",
    "    kl_implemented = kl_builder.gaussian_kl_closed_form(mu1, sigma1, mu2, sigma2)\n",
    "    \n",
    "    # Manual calculation for verification\n",
    "    kl_manual = 0.5 * ((sigma1/sigma2)**2 + ((mu1-mu2)/sigma2)**2 - 1 - 2*torch.log(sigma1/sigma2))\n",
    "    \n",
    "    print(f\"Your implementation: {kl_implemented}\")\n",
    "    print(f\"Manual calculation:  {kl_manual:.6f}\")\n",
    "    print(f\"Match: {torch.allclose(kl_implemented, kl_manual, atol=1e-5) if kl_implemented is not None else 'Implement TODO first'}\")\n",
    "    \n",
    "    # Test 2: Jensen's Inequality\n",
    "    print(f\"\\nTest 2: Jensen's Inequality\")\n",
    "    \n",
    "    # Test with simple example\n",
    "    values = torch.tensor([1.0, 4.0, 9.0])\n",
    "    log_mean = torch.log(values.mean())\n",
    "    mean_log = torch.log(values).mean()\n",
    "    \n",
    "    print(f\"log(E[X]): {log_mean:.6f}\")\n",
    "    print(f\"E[log(X)]: {mean_log:.6f}\")\n",
    "    print(f\"Jensen satisfied: {log_mean >= mean_log}\")\n",
    "    print(f\"Gap (lower bound): {log_mean - mean_log:.6f}\")\n",
    "    \n",
    "    # Test 3: ELBO Decomposition\n",
    "    print(f\"\\nTest 3: ELBO Components\")\n",
    "    \n",
    "    # Test that ELBO = reconstruction - KL\n",
    "    elbo_model = ELBOFramework(latent_dim=2, data_dim=2)\n",
    "    test_x = torch.randn(5, 2).to(device)\n",
    "    \n",
    "    # Verify ELBO decomposition (after students implement compute_elbo)\n",
    "    try:\n",
    "        results = elbo_model.compute_elbo(test_x)\n",
    "        elbo_direct = results['elbo']\n",
    "        elbo_components = results['reconstruction'] - results['kl_divergence']\n",
    "        \n",
    "        print(f\"ELBO direct: {elbo_direct.mean():.6f}\")\n",
    "        print(f\"ELBO from components: {elbo_components.mean():.6f}\")\n",
    "        print(f\"Match: {torch.allclose(elbo_direct, elbo_components, atol=1e-5)}\")\n",
    "    except:\n",
    "        print(\"ELBO test pending - implement compute_elbo first\")\n",
    "    \n",
    "    # Test 4: Reparameterization Trick\n",
    "    print(f\"\\nTest 4: Reparameterization Trick\")\n",
    "    \n",
    "    # Test that reparameterized samples have correct statistics\n",
    "    mu_test = torch.tensor([1.0, -0.5]).to(device)\n",
    "    logvar_test = torch.tensor([0.5, -0.2]).to(device)\n",
    "    \n",
    "    # Generate many samples and check statistics\n",
    "    n_samples = 10000\n",
    "    try:\n",
    "        samples = []\n",
    "        for _ in range(n_samples):\n",
    "            sample = elbo_model.reparameterize(mu_test.unsqueeze(0), logvar_test.unsqueeze(0))\n",
    "            samples.append(sample.squeeze())\n",
    "        \n",
    "        samples = torch.stack(samples)\n",
    "        empirical_mean = samples.mean(dim=0)\n",
    "        empirical_var = samples.var(dim=0)\n",
    "        expected_var = torch.exp(logvar_test)\n",
    "        \n",
    "        print(f\"Expected mean: {mu_test}\")\n",
    "        print(f\"Empirical mean: {empirical_mean}\")\n",
    "        print(f\"Expected var: {expected_var}\")\n",
    "        print(f\"Empirical var: {empirical_var}\")\n",
    "        print(f\"Mean close: {torch.allclose(mu_test, empirical_mean, atol=0.1)}\")\n",
    "        print(f\"Var close: {torch.allclose(expected_var, empirical_var, atol=0.1)}\")\n",
    "    except:\n",
    "        print(\"Reparameterization test pending - implement reparameterize first\")\n",
    "    \n",
    "    print(\"\\n🎓 Validation completed!\")\n",
    "    print(\"If your implementations pass these tests, your mathematical foundation is solid!\")\n",
    "\n",
    "# Run comprehensive validation\n",
    "comprehensive_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07141d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Connection to Diffusion Models (5 minutes)\n",
    "\n",
    "### Task 7.1: Bridge to Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionMathematicalBridge:\n",
    "    \"\"\"\n",
    "    Connect today's mathematical foundations to diffusion models.\n",
    "    Show how the ELBO framework extends to sequence generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_vae_vs_diffusion(self):\n",
    "        \"\"\"\n",
    "        Compare mathematical frameworks\n",
    "        \"\"\"\n",
    "        print(\"=== VAE vs Diffusion: Mathematical Comparison ===\\n\")\n",
    "        \n",
    "        print(\"VAE Framework:\")\n",
    "        print(\"  • Latent space: Single z ~ q(z|x)\")\n",
    "        print(\"  • Variational distribution: Learn q(z|x) with encoder\")\n",
    "        print(\"  • ELBO: E[log p(x|z)] - KL(q(z|x) || p(z))\")\n",
    "        print(\"  • Challenge: Approximation quality of q(z|x)\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Diffusion Framework:\")\n",
    "        print(\"  • Latent space: Sequence x₁, x₂, ..., xₜ\")\n",
    "        print(\"  • Variational distribution: Fixed q(x₁:ₜ|x₀) by design\")\n",
    "        print(\"  • ELBO: More complex, but perfectly tractable\")\n",
    "        print(\"  • Brilliance: No approximation error in q!\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Key insight: Diffusion eliminates the variational approximation problem!\")\n",
    "        \n",
    "        # Create comparison visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # VAE diagram\n",
    "        ax1.text(0.5, 0.8, 'VAE', ha='center', fontsize=16, weight='bold')\n",
    "        ax1.text(0.2, 0.6, 'x', ha='center', fontsize=14, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "        ax1.text(0.5, 0.6, 'Encoder\\nq(z|x)', ha='center', fontsize=12)\n",
    "        ax1.text(0.8, 0.6, 'z', ha='center', fontsize=14, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "        ax1.text(0.5, 0.4, 'Decoder\\np(x|z)', ha='center', fontsize=12)\n",
    "        ax1.text(0.2, 0.2, 'x̂', ha='center', fontsize=14, bbox=dict(boxstyle=\"round\", facecolor='lightcoral'))\n",
    "        \n",
    "        ax1.arrow(0.25, 0.6, 0.15, 0, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "        ax1.arrow(0.75, 0.6, -0.15, 0, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "        ax1.arrow(0.45, 0.35, -0.15, -0.1, head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "        \n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title('Single-Step Generation')\n",
    "        \n",
    "        # Diffusion diagram\n",
    "        ax2.text(0.5, 0.9, 'Diffusion', ha='center', fontsize=16, weight='bold')\n",
    "        \n",
    "        positions = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        labels = ['x₀', 'x₁', '...', 'xₜ₋₁', 'xₜ']\n",
    "        \n",
    "        for i, (pos, label) in enumerate(zip(positions, labels)):\n",
    "            if i == 2:  # dots\n",
    "                ax2.text(pos, 0.5, label, ha='center', fontsize=14)\n",
    "            else:\n",
    "                ax2.text(pos, 0.5, label, ha='center', fontsize=12, \n",
    "                        bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "            \n",
    "            if i < len(positions) - 1 and i != 1:  # skip arrow after dots\n",
    "                ax2.arrow(pos + 0.05, 0.5, 0.1, 0, head_width=0.02, head_length=0.02, \n",
    "                         fc='black', ec='black')\n",
    "        \n",
    "        ax2.set_xlim(0, 1)\n",
    "        ax2.set_ylim(0.3, 1)\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Sequential Generation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def preview_diffusion_elbo(self):\n",
    "        \"\"\"\n",
    "        Preview how ELBO extends to sequences\n",
    "        \"\"\"\n",
    "        print(\"=== Preview: Diffusion ELBO ===\\n\")\n",
    "        \n",
    "        print(\"VAE ELBO (single step):\")\n",
    "        print(\"  log p(x) ≥ E_q[log p(x|z)] - KL(q(z|x) || p(z))\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Diffusion ELBO (sequence):\")\n",
    "        print(\"  log p(x₀) ≥ E_q[log p(x₀|x₁)] - Σₜ KL(...)\")\n",
    "        print(\"           = Three terms we'll learn next time:\")\n",
    "        print(\"           = Reconstruction + Prior Matching + Consistency\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Mathematical progression:\")\n",
    "        print(\"  1. Today: Single latent variable z\")\n",
    "        print(\"  2. Next: Sequence of latent variables x₁, x₂, ..., xₜ\")\n",
    "        print(\"  3. Same ELBO principle, extended to Markov chains\")\n",
    "        print(\"  4. Fixed variational distribution eliminates approximation error\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Next lab will show:\")\n",
    "        print(\"  • How to derive the diffusion ELBO\")\n",
    "        print(\"  • Why noise prediction becomes natural\")\n",
    "        print(\"  • How your math today enables diffusion training\")\n",
    "\n",
    "# Connect to diffusion models\n",
    "bridge = DiffusionMathematicalBridge()\n",
    "bridge.compare_vae_vs_diffusion()\n",
    "bridge.preview_diffusion_elbo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c2ef6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Reflection and Summary (5 minutes)\n",
    "\n",
    "### Task 8.1: Mathematical Insights Summary\n",
    "\n",
    "**Discussion Questions** (Work with your partner):\n",
    "\n",
    "1. **Implementation Challenges**:\n",
    "   - Which mathematical concept was most difficult to implement?\n",
    "   - How did implementing the formulas change your understanding?\n",
    "\n",
    "2. **ELBO Insights**:\n",
    "   - Why does Jensen's inequality create a useful lower bound?\n",
    "   - How do the two forces shape learned representations?\n",
    "\n",
    "3. **Practical Connections**:\n",
    "   - How does mathematical theory connect to practical training?\n",
    "   - What would happen if we didn't have the reparameterization trick?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mathematical_journey():\n",
    "    \"\"\"\n",
    "    Reflect on the mathematical concepts you've implemented\n",
    "    \"\"\"\n",
    "    print(\"=== Your Mathematical Journey Today ===\\n\")\n",
    "    \n",
    "    concepts_implemented = [\n",
    "        \"❌ Intractable likelihood computation (and why it fails)\",\n",
    "        \"🔄 KL divergence calculations (asymmetric properties)\",\n",
    "        \"📐 Jensen's inequality (creating lower bounds)\",\n",
    "        \"🧮 ELBO framework (universal variational inference)\",\n",
    "        \"⚖️  Two forces analysis (reconstruction vs regularization)\",\n",
    "        \"🔧 Reparameterization trick (making randomness differentiable)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Mathematical concepts you implemented:\")\n",
    "    for concept in concepts_implemented:\n",
    "        print(f\"  {concept}\")\n",
    "    \n",
    "    print(f\"\\n🎓 You now understand the mathematical foundation of:\")\n",
    "    print(f\"   • Variational Autoencoders (VAEs)\")\n",
    "    print(f\"   • Generative Adversarial Networks (GANs) - different approach\")\n",
    "    print(f\"   • Diffusion Models - optimal variational choice\")\n",
    "    print(f\"   • All modern generative models!\")\n",
    "    \n",
    "    print(f\"\\n🔬 Key mathematical insights:\")\n",
    "    print(f\"   • Why direct likelihood optimization fails\")\n",
    "    print(f\"   • How Jensen's inequality saves the day\")\n",
    "    print(f\"   • Why the two-forces tension creates good representations\")\n",
    "    print(f\"   • How mathematical elegance enables practical algorithms\")\n",
    "    \n",
    "    # Create a summary visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Mathematical concept hierarchy\n",
    "    levels = {\n",
    "        'Problem': ['Intractable p(x)', 'Circular Bayes Rule'],\n",
    "        'Solution': ['Jensen\\'s Inequality', 'KL Divergence'],\n",
    "        'Framework': ['ELBO = Recon - KL'],\n",
    "        'Implementation': ['Reparameterization', 'Two Forces Balance'],\n",
    "        'Applications': ['VAEs', 'Diffusion Models']\n",
    "    }\n",
    "    \n",
    "    y_positions = [0.8, 0.65, 0.5, 0.35, 0.2]\n",
    "    colors = ['red', 'orange', 'green', 'blue', 'purple']\n",
    "    \n",
    "    for i, (level, concepts) in enumerate(levels.items()):\n",
    "        y = y_positions[i]\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Level label\n",
    "        ax.text(0.05, y, level, fontsize=14, weight='bold', color=color)\n",
    "        \n",
    "        # Concepts\n",
    "        x_positions = [0.25 + j * 0.2 for j in range(len(concepts))]\n",
    "        for j, concept in enumerate(concepts):\n",
    "            ax.text(x_positions[j], y, concept, fontsize=11, ha='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "    \n",
    "    # Add arrows showing progression\n",
    "    for i in range(len(y_positions) - 1):\n",
    "        ax.arrow(0.5, y_positions[i] - 0.05, 0, -0.05, head_width=0.02, head_length=0.01,\n",
    "                fc='gray', ec='gray', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Mathematical Foundation Hierarchy', fontsize=16, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summarize your learning\n",
    "summarize_mathematical_journey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5ecae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "### Core Mathematical Functions (Students Implement):\n",
    "\n",
    "**✅ Essential TODOs:**\n",
    "- [ ] `sample_prior()` - Prior sampling from N(0,I)\n",
    "- [ ] `likelihood_given_z()` - Gaussian likelihood p(x|z)\n",
    "- [ ] `approximate_marginal_likelihood()` - Monte Carlo estimation\n",
    "- [ ] `monte_carlo_kl()` - KL via sampling\n",
    "- [ ] `gaussian_kl_closed_form()` - Analytical KL for Gaussians\n",
    "- [ ] `standard_normal_kl()` - KL to N(0,I)\n",
    "- [ ] `reparameterize()` - Reparameterization trick\n",
    "- [ ] `reconstruction_loss()` - ELBO reconstruction term\n",
    "- [ ] `kl_regularization()` - ELBO KL term\n",
    "- [ ] `compute_elbo()` - Complete ELBO computation\n",
    "- [ ] `analyze_beta_vae()` - β-VAE force analysis\n",
    "\n",
    "**✅ Provided Starter Code:**\n",
    "- [ ] All visualization functions with complete plotting code\n",
    "- [ ] Training loops and optimization\n",
    "- [ ] Data generation and testing infrastructure\n",
    "- [ ] Mathematical validation and comparison functions\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "Submit your completed Jupyter notebook (.ipynb file) with:\n",
    "\n",
    "**✅ Mathematical Implementations:**\n",
    "- All TODO functions implemented with correct mathematical formulas\n",
    "- Clear comments explaining each mathematical step\n",
    "- Proper tensor operations and broadcasting\n",
    "\n",
    "**✅ Validation Results:**\n",
    "- Numerical verification that implementations match theory\n",
    "- Comparison of analytical vs Monte Carlo methods\n",
    "- Testing edge cases and error handling\n",
    "\n",
    "**✅ Analysis and Insights:**\n",
    "- β-VAE analysis showing two forces tradeoff\n",
    "- KL asymmetry exploration with explanations\n",
    "- Jensen's inequality demonstrations\n",
    "\n",
    "**✅ Documentation:**\n",
    "- Clear explanations of each mathematical concept\n",
    "- Discussion of implementation challenges encountered\n",
    "- Connection between theory and practical algorithms\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Key Mathematical Formulas\n",
    "\n",
    "### For Implementation Reference:\n",
    "\n",
    "**KL Divergence to Standard Normal:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL(N(μ,σ²) || N(0,I)) = 0.5 * Σ[μ² + σ² - 1 - log(σ²)]\n",
    "kl = 0.5 * (mu**2 + torch.exp(logvar) - 1 - logvar).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edfee79",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Gaussian Log-Likelihood:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a5ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log p(x|z) = -0.5 * [log(2π) + log(σ²) + (x-μ)²/σ²]\n",
    "log_likelihood = -0.5 * (math.log(2*math.pi) + logvar + (x - mu)**2 / torch.exp(logvar))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9790ce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Reparameterization Trick:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = μ + σ * ε, where ε ~ N(0,I) and σ = exp(0.5 * log(σ²))\n",
    "std = torch.exp(0.5 * logvar)\n",
    "eps = torch.randn_like(std)\n",
    "z = mu + std * eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7d2c9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Gaussian KL Closed Form:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ddd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KL(N(μ₁,σ₁²) || N(μ₂,σ₂²))\n",
    "kl = torch.log(sigma2/sigma1) + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2) - 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109d20c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Implementation Issues & Solutions\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "**NaN Gradients:**\n",
    "- Check for `log(0)` in likelihood computations\n",
    "- Ensure positive variances: use `torch.exp(logvar)` not `logvar`\n",
    "- Add small epsilon: `torch.exp(logvar) + 1e-8`\n",
    "\n",
    "**Dimension Mismatches:**\n",
    "- KL should sum over latent dimensions: `.sum(dim=-1)`\n",
    "- Reconstruction should sum over data dimensions: `.sum(dim=-1)`\n",
    "- Batch operations: keep batch dimension intact\n",
    "\n",
    "**Training Instability:**\n",
    "- Start with small learning rates (1e-4 to 1e-3)\n",
    "- Monitor KL collapse: if KL → 0, increase β weight\n",
    "- Check reconstruction scale: normalize data to [-1,1] or [0,1]\n",
    "\n",
    "---\n",
    "\n",
    "## Final Implementation Notes\n",
    "\n",
    "### Expected Behavior After Implementation:\n",
    "\n",
    "**Intractable Likelihood Demo:**\n",
    "- Monte Carlo estimates should have high variance\n",
    "- Computation time should grow with sample count\n",
    "- Multiple runs should give different estimates\n",
    "\n",
    "**KL Divergence:**\n",
    "- `KL(p||q) ≠ KL(q||p)` - should see clear asymmetry\n",
    "- Closed-form and Monte Carlo should match (within 0.01)\n",
    "- Standard normal KL should be positive for non-standard inputs\n",
    "\n",
    "**Jensen's Inequality:**\n",
    "- `log(E[X]) ≥ E[log(X)]` should hold for all test cases\n",
    "- Gap should be positive and meaningful (> 0.001)\n",
    "\n",
    "**ELBO Training:**\n",
    "- Loss should decrease over epochs\n",
    "- Reconstruction and KL should show clear tradeoff\n",
    "- β-VAE should show dramatic differences in latent organization\n",
    "\n",
    "**Validation:**\n",
    "- All mathematical relationships should hold within numerical precision\n",
    "- Reparameterized samples should match expected statistics\n",
    "- ELBO decomposition should be internally consistent\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
