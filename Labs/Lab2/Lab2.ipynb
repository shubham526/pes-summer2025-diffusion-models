{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed72d66",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Lab 2: Mastering the Forward Diffusion Process - Mathematical Implementation\n",
    "**Course: Diffusion Models: Theory and Applications**  \n",
    "**Duration: 90 minutes**  \n",
    "**Team Size: 2 students (same teams from Lab 1)**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, students will be able to:\n",
    "1. **Implement** the reparameterization trick and understand its crucial role in making diffusion trainable\n",
    "2. **Build** different noise schedules (linear, cosine, exponential) and analyze their effects\n",
    "3. **Derive and implement** the forward jump formula from first principles\n",
    "4. **Create** efficient training data generation pipelines using Gaussian arithmetic\n",
    "5. **Analyze** signal-to-noise ratios and variance evolution throughout the diffusion process\n",
    "6. **Compare** computational complexity with and without forward jumps\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Lab 1 (basic diffusion implementation)\n",
    "- Understanding of multivariate Gaussian distributions\n",
    "- Familiarity with the reparameterization trick concept\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup and Environment\n",
    "\n",
    "### Part 1: Team Reunion & Mathematical Setup (10 minutes)\n",
    "\n",
    "#### Task 1.1: Reconnect with Your Lab Partner\n",
    "- Same teams as Lab 1 - compare your experiences since last lab\n",
    "- **Today's Mission**: Build mathematically rigorous diffusion components\n",
    "- **Success Criteria**: Implement forward jumps that are 1000x faster than sequential corruption\n",
    "\n",
    "#### Task 1.2: Mathematical Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical imports for rigorous implementation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import time\n",
    "from torch.distributions import Normal\n",
    "from scipy import stats\n",
    "\n",
    "# Set seeds for reproducible mathematics\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load MNIST for testing our mathematical implementations\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Get a test image for our mathematical experiments\n",
    "test_image = next(iter(train_loader))[0][:1].to(device)\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test image range: [{test_image.min():.3f}, {test_image.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c687e63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Implement the Reparameterization Trick (20 minutes)\n",
    "\n",
    "### Task 2.1: Understanding Why We Need Reparameterization\n",
    "\n",
    "**Your Mission**: Implement both sampling approaches and see why one enables gradients while the other doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparameterizationDemo:\n",
    "    \"\"\"\n",
    "    This class will demonstrate why the reparameterization trick is essential\n",
    "    for training neural networks with stochastic operations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We'll use a simple linear transformation to show gradient flow\n",
    "        self.linear = nn.Linear(1, 1).to(device)\n",
    "        \n",
    "    def direct_sampling_approach(self, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement direct sampling from N(mu, sigma^2)\n",
    "        \n",
    "        This approach samples directly from the Gaussian distribution.\n",
    "        \n",
    "        Implementation steps:\n",
    "        1. Create a Normal distribution with given mu and sigma\n",
    "        2. Sample from it using .sample()\n",
    "        3. Return the sample\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean of the distribution (can be parameterized by neural network)\n",
    "            sigma: Standard deviation (should be positive)\n",
    "            \n",
    "        Returns:\n",
    "            sample: A sample from N(mu, sigma^2)\n",
    "            \n",
    "        Note: This approach breaks gradient flow!\n",
    "        \"\"\"\n",
    "        # TODO: Implement direct sampling\n",
    "        # Hint: Use torch.distributions.Normal\n",
    "        pass\n",
    "    \n",
    "    def reparameterized_approach(self, mu: torch.Tensor, sigma: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement the reparameterization trick\n",
    "        \n",
    "        This approach separates the randomness from the parameters.\n",
    "        \n",
    "        Implementation steps:\n",
    "        1. Sample epsilon from standard normal: eps ~ N(0, 1)\n",
    "        2. Transform: z = mu + sigma * eps\n",
    "        3. Return z\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean (can have gradients)\n",
    "            sigma: Standard deviation (can have gradients)\n",
    "            \n",
    "        Returns:\n",
    "            sample: Equivalent sample but with gradient flow preserved\n",
    "        \"\"\"\n",
    "        # TODO: Implement reparameterization\n",
    "        # Hint: Sample eps ~ N(0,1), then transform to z = mu + sigma * eps\n",
    "        pass\n",
    "    \n",
    "    def test_gradient_flow(self):\n",
    "        \"\"\"\n",
    "        Test which approach allows gradient computation\n",
    "        \"\"\"\n",
    "        print(\"=== Testing Gradient Flow ===\\n\")\n",
    "        \n",
    "        # Create input with gradients\n",
    "        x = torch.tensor([[1.0]], requires_grad=True, device=device)\n",
    "        \n",
    "        # Get parameterized mean\n",
    "        mu = self.linear(x)\n",
    "        sigma = torch.tensor(0.5, device=device)  # Fixed sigma for simplicity\n",
    "        \n",
    "        print(f\"Input x: {x.item():.3f}\")\n",
    "        print(f\"Parameterized mu: {mu.item():.3f}\")\n",
    "        print(f\"Fixed sigma: {sigma.item():.3f}\\n\")\n",
    "        \n",
    "        # Test direct sampling approach\n",
    "        try:\n",
    "            print(\"Testing direct sampling approach...\")\n",
    "            sample1 = self.direct_sampling_approach(mu, sigma)\n",
    "            loss1 = sample1.mean()\n",
    "            print(f\"Sample: {sample1.item():.3f}, Loss: {loss1.item():.3f}\")\n",
    "            \n",
    "            # Try to compute gradients\n",
    "            self.linear.zero_grad()\n",
    "            loss1.backward()\n",
    "            \n",
    "            # Check if gradients exist\n",
    "            grad1 = self.linear.weight.grad\n",
    "            print(f\"Gradient for linear.weight: {grad1}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with direct sampling: {e}\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        # Test reparameterized approach\n",
    "        try:\n",
    "            print(\"Testing reparameterized approach...\")\n",
    "            sample2 = self.reparameterized_approach(mu, sigma)\n",
    "            loss2 = sample2.mean()\n",
    "            print(f\"Sample: {sample2.item():.3f}, Loss: {loss2.item():.3f}\")\n",
    "            \n",
    "            # Try to compute gradients\n",
    "            self.linear.zero_grad()\n",
    "            loss2.backward()\n",
    "            \n",
    "            # Check if gradients exist\n",
    "            grad2 = self.linear.weight.grad\n",
    "            print(f\"Gradient for linear.weight: {grad2}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with reparameterization: {e}\")\n",
    "\n",
    "# Test statistical equivalence (provided function)\n",
    "def test_reparameterization_equivalence(reparam_demo, x0, num_trials=1000):\n",
    "    \"\"\"Test that both approaches produce statistically equivalent results\"\"\"\n",
    "    print(\"\\n=== Testing Statistical Equivalence ===\\n\")\n",
    "    \n",
    "    mu = torch.zeros_like(x0)\n",
    "    sigma = torch.ones_like(x0) * 0.5\n",
    "    \n",
    "    # Generate samples with both methods\n",
    "    direct_samples = []\n",
    "    reparam_samples = []\n",
    "    \n",
    "    for _ in range(num_trials):\n",
    "        try:\n",
    "            direct_sample = reparam_demo.direct_sampling_approach(mu, sigma)\n",
    "            direct_samples.append(direct_sample)\n",
    "        except:\n",
    "            pass  # Skip if not implemented\n",
    "            \n",
    "        try:\n",
    "            reparam_sample = reparam_demo.reparameterized_approach(mu, sigma)\n",
    "            reparam_samples.append(reparam_sample)\n",
    "        except:\n",
    "            pass  # Skip if not implemented\n",
    "    \n",
    "    if direct_samples and reparam_samples:\n",
    "        direct_samples = torch.stack(direct_samples[:100])  # Use first 100 samples\n",
    "        reparam_samples = torch.stack(reparam_samples[:100])\n",
    "        \n",
    "        # Compare statistics\n",
    "        direct_mean = direct_samples.mean(dim=0)\n",
    "        reparam_mean = reparam_samples.mean(dim=0)\n",
    "        direct_std = direct_samples.std(dim=0)\n",
    "        reparam_std = reparam_samples.std(dim=0)\n",
    "        \n",
    "        print(f\"Direct approach - Mean: {direct_mean.mean():.4f}, Std: {direct_std.mean():.4f}\")\n",
    "        print(f\"Reparam approach - Mean: {reparam_mean.mean():.4f}, Std: {reparam_std.mean():.4f}\")\n",
    "        print(f\"Mean difference: {(direct_mean - reparam_mean).abs().mean():.6f}\")\n",
    "        print(f\"Std difference: {(direct_std - reparam_std).abs().mean():.6f}\")\n",
    "        \n",
    "        # Visualize distributions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.hist(direct_samples.flatten().cpu(), bins=50, alpha=0.7, label='Direct', density=True)\n",
    "        ax1.hist(reparam_samples.flatten().cpu(), bins=50, alpha=0.7, label='Reparameterized', density=True)\n",
    "        ax1.set_title('Sample Distributions')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-Q plot for distribution comparison\n",
    "        sample_data = reparam_samples.flatten().cpu().numpy()\n",
    "        stats.probplot(sample_data, dist=\"norm\", plot=ax2)\n",
    "        ax2.set_title('Q-Q Plot: Reparameterized Samples vs Normal')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test your implementation\n",
    "demo = ReparameterizationDemo()\n",
    "demo.test_gradient_flow()\n",
    "\n",
    "# Test statistical equivalence (uncomment after implementing both approaches)\n",
    "# test_reparameterization_equivalence(demo, test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0eb7b",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement Diffusion-Specific Reparameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a595503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionReparameterization:\n",
    "    \"\"\"\n",
    "    Implement reparameterization specifically for diffusion steps.\n",
    "    This class implements the core mathematical transformation that makes\n",
    "    diffusion models trainable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward_step_sampling(self, x_prev: torch.Tensor, beta_t: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement the WRONG way (direct sampling)\n",
    "        \n",
    "        Direct sampling from: q(x_t | x_{t-1}) = N(sqrt(1-beta_t) * x_{t-1}, beta_t * I)\n",
    "        \n",
    "        This approach will not allow gradients to flow through the sampling operation.\n",
    "        \n",
    "        Steps:\n",
    "        1. Compute mean: sqrt(1-beta_t) * x_prev\n",
    "        2. Compute std: sqrt(beta_t)\n",
    "        3. Create Normal distribution and sample\n",
    "        \n",
    "        Args:\n",
    "            x_prev: Previous state x_{t-1}\n",
    "            beta_t: Noise schedule value at timestep t\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Next state (without gradient flow)\n",
    "        \"\"\"\n",
    "        # TODO: Implement direct sampling\n",
    "        # Hint: Use math.sqrt and torch.distributions.Normal\n",
    "        pass\n",
    "    \n",
    "    def forward_step_reparameterized(self, x_prev: torch.Tensor, beta_t: float) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the RIGHT way (reparameterization trick)\n",
    "        \n",
    "        Reparameterized form: x_t = sqrt(1-beta_t) * x_{t-1} + sqrt(beta_t) * epsilon\n",
    "        where epsilon ~ N(0, I)\n",
    "        \n",
    "        This preserves gradient flow and is the foundation of trainable diffusion.\n",
    "        \n",
    "        Steps:\n",
    "        1. Sample epsilon from standard normal\n",
    "        2. Compute signal coefficient: sqrt(1-beta_t)\n",
    "        3. Compute noise coefficient: sqrt(beta_t)\n",
    "        4. Apply transformation: signal * x_prev + noise * epsilon\n",
    "        5. Return both x_t and the epsilon used (important for training!)\n",
    "        \n",
    "        Args:\n",
    "            x_prev: Previous state x_{t-1}\n",
    "            beta_t: Noise schedule value at timestep t\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Next state (with gradient flow)\n",
    "            epsilon: The noise that was added (needed for training targets)\n",
    "        \"\"\"\n",
    "        # TODO: Implement reparameterized version\n",
    "        # Hint: epsilon = torch.randn_like(x_prev), then apply the transformation\n",
    "        pass\n",
    "    \n",
    "    def compare_approaches(self, x_input: torch.Tensor, beta_t: float = 0.01):\n",
    "        \"\"\"\n",
    "        Compare both approaches and verify they produce same statistics\n",
    "        \"\"\"\n",
    "        print(f\"=== Comparing Sampling Approaches (beta_t = {beta_t}) ===\\n\")\n",
    "        \n",
    "        # Theoretical values\n",
    "        theoretical_mean = math.sqrt(1 - beta_t) * x_input\n",
    "        theoretical_var = beta_t * torch.ones_like(x_input)\n",
    "        \n",
    "        print(f\"Theoretical mean: {theoretical_mean.mean().item():.4f}\")\n",
    "        print(f\"Theoretical variance: {theoretical_var.mean().item():.4f}\\n\")\n",
    "        \n",
    "        # Test approaches if implemented\n",
    "        try:\n",
    "            direct_result = self.forward_step_sampling(x_input, beta_t)\n",
    "            print(f\"Direct sampling result shape: {direct_result.shape}\")\n",
    "            print(f\"Direct sampling mean: {direct_result.mean().item():.4f}\")\n",
    "        except:\n",
    "            print(\"Direct sampling not yet implemented\")\n",
    "        \n",
    "        try:\n",
    "            reparam_result, epsilon = self.forward_step_reparameterized(x_input, beta_t)\n",
    "            print(f\"Reparameterized result shape: {reparam_result.shape}\")\n",
    "            print(f\"Reparameterized mean: {reparam_result.mean().item():.4f}\")\n",
    "            print(f\"Epsilon shape: {epsilon.shape}\")\n",
    "        except:\n",
    "            print(\"Reparameterized approach not yet implemented\")\n",
    "\n",
    "# Test diffusion reparameterization\n",
    "diffusion_reparam = DiffusionReparameterization()\n",
    "diffusion_reparam.compare_approaches(test_image, beta_t=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ba859",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Implement and Compare Noise Schedules (25 minutes)\n",
    "\n",
    "### Task 3.1: Build Different Noise Schedule Types\n",
    "\n",
    "**Your Mission**: Implement the three main types of noise schedules and understand their mathematical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb87671",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    \"\"\"\n",
    "    Implement mathematically rigorous noise schedules.\n",
    "    This class generates the beta and alpha schedules that control\n",
    "    how aggressively we corrupt data at each timestep.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_timesteps: int = 1000, device: str = 'cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Will store our schedules\n",
    "        self.betas = None\n",
    "        self.alphas = None\n",
    "        self.alpha_cumprod = None\n",
    "        self.sqrt_alpha_cumprod = None\n",
    "        self.sqrt_one_minus_alpha_cumprod = None\n",
    "        \n",
    "    def linear_schedule(self, beta_start: float = 1e-4, beta_end: float = 0.02) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement linear noise schedule\n",
    "        \n",
    "        Formula: beta_t = beta_start + (t-1)/(T-1) * (beta_end - beta_start)\n",
    "        \n",
    "        This creates a linearly increasing schedule from beta_start to beta_end.\n",
    "        \n",
    "        Steps:\n",
    "        1. Create timestep indices from 0 to T-1\n",
    "        2. Apply linear interpolation formula\n",
    "        3. Ensure all betas are in valid range (0, 1)\n",
    "        \n",
    "        Args:\n",
    "            beta_start: Starting noise level (should be very small)\n",
    "            beta_end: Ending noise level (should be substantial but < 1)\n",
    "            \n",
    "        Returns:\n",
    "            betas: Tensor of shape (num_timesteps,) with beta values\n",
    "        \"\"\"\n",
    "        # TODO: Implement linear schedule\n",
    "        # Hint: Use torch.arange and linear interpolation\n",
    "        pass\n",
    "    \n",
    "    def cosine_schedule(self, s: float = 0.008) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement cosine noise schedule\n",
    "        \n",
    "        This is the schedule from \"Improved Denoising Diffusion Probabilistic Models\".\n",
    "        It provides more gradual corruption early and faster corruption later.\n",
    "        \n",
    "        Formula is complex but gives better training dynamics:\n",
    "        1. Define f(t) = cos((t/T + s)/(1 + s) * π/2)^2\n",
    "        2. alpha_cumprod_t = f(t) / f(0)\n",
    "        3. alpha_t = alpha_cumprod_t / alpha_cumprod_{t-1}\n",
    "        4. beta_t = 1 - alpha_t\n",
    "        \n",
    "        Steps:\n",
    "        1. Create timestep array including 0 to T\n",
    "        2. Compute f(t) values using cosine function\n",
    "        3. Normalize by f(0)\n",
    "        4. Compute alphas from cumulative products\n",
    "        5. Convert to betas: beta_t = 1 - alpha_t\n",
    "        6. Clip to prevent numerical issues\n",
    "        \n",
    "        Args:\n",
    "            s: Small offset to prevent beta_t = 0 at t = 0\n",
    "            \n",
    "        Returns:\n",
    "            betas: Tensor of shape (num_timesteps,) with beta values\n",
    "        \"\"\"\n",
    "        # TODO: Implement cosine schedule\n",
    "        # This is more challenging - research the exact formula!\n",
    "        # Hint: Use torch.cos and handle the cumulative product carefully\n",
    "        pass\n",
    "    \n",
    "    def exponential_schedule(self, beta_start: float = 1e-4, beta_end: float = 0.02) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement exponential noise schedule\n",
    "        \n",
    "        This schedule increases exponentially, giving very gentle corruption early\n",
    "        and very aggressive corruption later.\n",
    "        \n",
    "        Formula: beta_t = exp(log(beta_start) + t/(T-1) * log(beta_end/beta_start))\n",
    "        \n",
    "        Steps:\n",
    "        1. Create timestep indices\n",
    "        2. Compute log-space interpolation\n",
    "        3. Exponentiate to get betas\n",
    "        \n",
    "        Args:\n",
    "            beta_start: Starting noise level\n",
    "            beta_end: Ending noise level\n",
    "            \n",
    "        Returns:\n",
    "            betas: Tensor of shape (num_timesteps,) with beta values\n",
    "        \"\"\"\n",
    "        # TODO: Implement exponential schedule\n",
    "        # Hint: Use torch.log, torch.exp, and log-space interpolation\n",
    "        pass\n",
    "    \n",
    "    def precompute_schedule(self, schedule_type: str = \"linear\", **kwargs):\n",
    "        \"\"\"\n",
    "        TODO: Precompute all derived quantities for efficient forward jumps\n",
    "        \n",
    "        Once we have betas, we need to compute all the derived quantities:\n",
    "        - alphas = 1 - betas\n",
    "        - alpha_cumprod = cumulative product of alphas\n",
    "        - sqrt_alpha_cumprod = square root for signal coefficient\n",
    "        - sqrt_one_minus_alpha_cumprod = square root for noise coefficient\n",
    "        \n",
    "        These precomputed values enable O(1) forward jumps.\n",
    "        \n",
    "        Steps:\n",
    "        1. Get betas using specified schedule\n",
    "        2. Compute alphas = 1 - betas\n",
    "        3. Compute cumulative product: alpha_cumprod[t] = product of alphas[0:t+1]\n",
    "        4. Compute square roots for forward jump formula\n",
    "        5. Store all quantities for later use\n",
    "        \n",
    "        Args:\n",
    "            schedule_type: \"linear\", \"cosine\", or \"exponential\"\n",
    "            **kwargs: Additional parameters for specific schedules\n",
    "        \"\"\"\n",
    "        # Get betas using specified schedule\n",
    "        if schedule_type == \"linear\":\n",
    "            self.betas = self.linear_schedule(**kwargs)\n",
    "        elif schedule_type == \"cosine\":\n",
    "            self.betas = self.cosine_schedule(**kwargs)\n",
    "        elif schedule_type == \"exponential\":\n",
    "            self.betas = self.exponential_schedule(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown schedule type: {schedule_type}\")\n",
    "        \n",
    "        # TODO: Compute derived quantities\n",
    "        # Hint: alphas = 1 - betas, then use torch.cumprod for cumulative product\n",
    "        pass\n",
    "        \n",
    "        if self.betas is not None:\n",
    "            print(f\"Precomputed {schedule_type} schedule:\")\n",
    "            print(f\"  Beta range: [{self.betas.min():.6f}, {self.betas.max():.6f}]\")\n",
    "            if self.alpha_cumprod is not None:\n",
    "                print(f\"  Alpha cumprod range: [{self.alpha_cumprod.min():.6f}, {self.alpha_cumprod.max():.6f}]\")\n",
    "\n",
    "# Visualization function (provided)\n",
    "def plot_noise_schedules(schedulers):\n",
    "    \"\"\"Complete plotting function for schedule comparison\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Beta schedules\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.betas is not None:\n",
    "            axes[0, 0].plot(scheduler.betas.cpu(), label=stype, linewidth=2)\n",
    "    axes[0, 0].set_title('Beta Schedules')\n",
    "    axes[0, 0].set_xlabel('Timestep')\n",
    "    axes[0, 0].set_ylabel('Beta_t')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Alpha cumulative product\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.alpha_cumprod is not None:\n",
    "            axes[0, 1].plot(scheduler.alpha_cumprod.cpu(), label=stype, linewidth=2)\n",
    "    axes[0, 1].set_title('Alpha Cumulative Product')\n",
    "    axes[0, 1].set_xlabel('Timestep')\n",
    "    axes[0, 1].set_ylabel('Alpha_cumprod')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Signal coefficient\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.sqrt_alpha_cumprod is not None:\n",
    "            axes[0, 2].plot(scheduler.sqrt_alpha_cumprod.cpu(), label=stype, linewidth=2)\n",
    "    axes[0, 2].set_title('Signal Coefficient: √(alpha_cumprod)')\n",
    "    axes[0, 2].set_xlabel('Timestep')\n",
    "    axes[0, 2].set_ylabel('Signal Coefficient')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Noise coefficient\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.sqrt_one_minus_alpha_cumprod is not None:\n",
    "            axes[1, 0].plot(scheduler.sqrt_one_minus_alpha_cumprod.cpu(), label=stype, linewidth=2)\n",
    "    axes[1, 0].set_title('Noise Coefficient: √(1-alpha_cumprod)')\n",
    "    axes[1, 0].set_xlabel('Timestep')\n",
    "    axes[1, 0].set_ylabel('Noise Coefficient')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Signal-to-noise ratio\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.alpha_cumprod is not None:\n",
    "            snr = scheduler.alpha_cumprod / (1 - scheduler.alpha_cumprod + 1e-8)\n",
    "            axes[1, 1].plot(snr.cpu(), label=stype, linewidth=2)\n",
    "    axes[1, 1].set_title('Signal-to-Noise Ratio')\n",
    "    axes[1, 1].set_xlabel('Timestep')\n",
    "    axes[1, 1].set_ylabel('SNR')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Noise schedule comparison\n",
    "    timesteps = range(1000)\n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.alpha_cumprod is not None:\n",
    "            # Plot percentage of original signal remaining\n",
    "            signal_remaining = scheduler.alpha_cumprod.cpu() * 100\n",
    "            axes[1, 2].plot(signal_remaining, label=f'{stype} (signal %)', linewidth=2)\n",
    "    axes[1, 2].set_title('Signal Preservation Over Time')\n",
    "    axes[1, 2].set_xlabel('Timestep')\n",
    "    axes[1, 2].set_ylabel('% Original Signal')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test all three schedules\n",
    "scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "\n",
    "print(\"Testing different noise schedules...\\n\")\n",
    "schedulers = {}\n",
    "\n",
    "for schedule_type in [\"linear\", \"cosine\", \"exponential\"]:\n",
    "    test_scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "    test_scheduler.precompute_schedule(schedule_type)\n",
    "    schedulers[schedule_type] = test_scheduler\n",
    "    print()\n",
    "\n",
    "# Plot comparison (uncomment after implementing schedules)\n",
    "# plot_noise_schedules(schedulers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea55da",
   "metadata": {},
   "source": [
    "### Task 3.2: Analyze Schedule Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb5a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_noise_schedules(schedulers):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of different noise schedules\n",
    "    \"\"\"\n",
    "    print(\"=== Schedule Analysis ===\")\n",
    "    \n",
    "    for stype, scheduler in schedulers.items():\n",
    "        if scheduler.alpha_cumprod is not None:\n",
    "            print(f\"\\n{stype.upper()} Schedule:\")\n",
    "            \n",
    "            # Find key milestones\n",
    "            alpha_cumprod = scheduler.alpha_cumprod.cpu()\n",
    "            \n",
    "            # At what timestep does signal drop below 50%?\n",
    "            signal_50_idx = torch.where(alpha_cumprod < 0.5)[0]\n",
    "            if len(signal_50_idx) > 0:\n",
    "                print(f\"  Signal drops below 50% at timestep: {signal_50_idx[0].item() + 1}\")\n",
    "            \n",
    "            # At what timestep is SNR = 1 (equal signal and noise)?\n",
    "            snr = alpha_cumprod / (1 - alpha_cumprod)\n",
    "            snr_1_idx = torch.where(snr < 1.0)[0]\n",
    "            if len(snr_1_idx) > 0:\n",
    "                print(f\"  SNR drops below 1.0 at timestep: {snr_1_idx[0].item() + 1}\")\n",
    "            \n",
    "            # Rate of corruption (how quickly signal drops)\n",
    "            signal_drop_rate = -torch.diff(alpha_cumprod).mean()\n",
    "            print(f\"  Average signal drop rate: {signal_drop_rate:.6f} per timestep\")\n",
    "            \n",
    "            # Final corruption level\n",
    "            final_signal = alpha_cumprod[-1]\n",
    "            print(f\"  Final signal level: {final_signal:.6f} ({final_signal*100:.3f}%)\")\n",
    "\n",
    "# Run analysis (uncomment after implementing schedules)\n",
    "# analyze_noise_schedules(schedulers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ee58e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Implement and Derive Forward Jumps (25 minutes)\n",
    "\n",
    "### Task 4.1: Sequential vs Direct Implementation\n",
    "\n",
    "**Your Mission**: Implement both approaches and measure the dramatic computational difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardJumpImplementation:\n",
    "    \"\"\"\n",
    "    Implement both sequential and direct forward jump approaches.\n",
    "    This class demonstrates the computational revolution that makes\n",
    "    diffusion models practical.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scheduler: NoiseScheduler):\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def sequential_forward_process(self, x0: torch.Tensor, target_timestep: int) -> Tuple[torch.Tensor, list]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the SLOW way - sequential application\n",
    "        \n",
    "        This simulates the actual Markov chain by applying each step sequentially.\n",
    "        This is how you would have to do it without the forward jump property.\n",
    "        \n",
    "        Steps:\n",
    "        1. Start with x0\n",
    "        2. For each timestep from 1 to target_timestep:\n",
    "           a. Get beta_t for current timestep\n",
    "           b. Apply reparameterized step: x_t = sqrt(1-beta_t) * x_{t-1} + sqrt(beta_t) * eps\n",
    "           c. Store intermediate result\n",
    "        3. Return final result and all intermediate states\n",
    "        \n",
    "        Args:\n",
    "            x0: Clean starting image\n",
    "            target_timestep: How many steps to apply\n",
    "            \n",
    "        Returns:\n",
    "            x_final: Final corrupted image after target_timestep steps\n",
    "            intermediates: List of all intermediate states (for visualization)\n",
    "        \"\"\"\n",
    "        # TODO: Implement sequential process\n",
    "        # Hint: Use a loop from 1 to target_timestep, apply reparameterized steps\n",
    "        current = x0.clone()\n",
    "        intermediates = [current.clone()]\n",
    "        \n",
    "        # TODO: Implement the sequential loop here\n",
    "        \n",
    "        return current, intermediates\n",
    "    \n",
    "    def direct_forward_jump(self, x0: torch.Tensor, target_timestep: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the FAST way - direct jump using precomputed coefficients\n",
    "        \n",
    "        This uses the mathematical property that multiple Gaussian steps\n",
    "        can be collapsed into a single equivalent step.\n",
    "        \n",
    "        Formula: x_t = sqrt(alpha_cumprod_t) * x0 + sqrt(1 - alpha_cumprod_t) * epsilon\n",
    "        \n",
    "        Steps:\n",
    "        1. Get precomputed coefficients for target timestep\n",
    "        2. Sample fresh noise\n",
    "        3. Apply single transformation\n",
    "        4. Return result and noise used\n",
    "        \n",
    "        Args:\n",
    "            x0: Clean starting image  \n",
    "            target_timestep: Target corruption level\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Corrupted image at target timestep\n",
    "            epsilon: The noise that was added\n",
    "        \"\"\"\n",
    "        # TODO: Implement direct forward jump\n",
    "        if target_timestep == 0:\n",
    "            return x0, torch.zeros_like(x0)\n",
    "        \n",
    "        # TODO: Get precomputed coefficients and apply the forward jump formula\n",
    "        # Hint: Use self.scheduler.sqrt_alpha_cumprod and self.scheduler.sqrt_one_minus_alpha_cumprod\n",
    "        pass\n",
    "\n",
    "# Benchmarking function (provided)\n",
    "def benchmark_forward_approaches(forward_impl, x0, timesteps_to_test):\n",
    "    \"\"\"Complete benchmarking with timing and visualization\"\"\"\n",
    "    print(\"=== Performance Benchmark ===\\n\")\n",
    "    \n",
    "    results = {'sequential': [], 'direct': [], 'speedups': [], 'timesteps': []}\n",
    "    \n",
    "    for target_t in timesteps_to_test:\n",
    "        if target_t > forward_impl.scheduler.num_timesteps:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Testing timestep {target_t}:\")\n",
    "        \n",
    "        # Time sequential approach\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            x_seq, _ = forward_impl.sequential_forward_process(x0, target_t)\n",
    "            seq_time = time.time() - start_time\n",
    "        except:\n",
    "            print(\"  Sequential approach not implemented\")\n",
    "            continue\n",
    "        \n",
    "        # Time direct approach  \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            x_direct, _ = forward_impl.direct_forward_jump(x0, target_t)\n",
    "            direct_time = time.time() - start_time\n",
    "        except:\n",
    "            print(\"  Direct approach not implemented\")\n",
    "            continue\n",
    "        \n",
    "        # Compare results\n",
    "        mse = torch.mean((x_seq - x_direct) ** 2).item()\n",
    "        speedup = seq_time / direct_time if direct_time > 0 else float('inf')\n",
    "        \n",
    "        results['sequential'].append(seq_time)\n",
    "        results['direct'].append(direct_time)\n",
    "        results['speedups'].append(speedup)\n",
    "        results['timesteps'].append(target_t)\n",
    "        \n",
    "        print(f\"  Sequential time: {seq_time:.4f}s\")\n",
    "        print(f\"  Direct time: {direct_time:.4f}s\") \n",
    "        print(f\"  Speedup: {speedup:.1f}x\")\n",
    "        print(f\"  MSE difference: {mse:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    if results['timesteps']:\n",
    "        # Plot results\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.plot(results['timesteps'], results['sequential'], 'r-o', label='Sequential', linewidth=2)\n",
    "        ax1.plot(results['timesteps'], results['direct'], 'b-o', label='Direct', linewidth=2)\n",
    "        ax1.set_xlabel('Timestep')\n",
    "        ax1.set_ylabel('Time (seconds)')\n",
    "        ax1.set_title('Computation Time Comparison')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_yscale('log')\n",
    "        \n",
    "        ax2.plot(results['timesteps'], results['speedups'], 'g-o', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Timestep')\n",
    "        ax2.set_ylabel('Speedup Factor')\n",
    "        ax2.set_title('Direct vs Sequential Speedup')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Average speedup: {np.mean(results['speedups']):.1f}x\")\n",
    "        print(f\"Maximum speedup: {max(results['speedups']):.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Visualization function for corruption process (provided)\n",
    "def visualize_corruption_process(forward_impl, x0, timesteps_to_show):\n",
    "    \"\"\"Visualize corruption at different timesteps\"\"\"\n",
    "    fig, axes = plt.subplots(2, len(timesteps_to_show), figsize=(3*len(timesteps_to_show), 6))\n",
    "    \n",
    "    if len(timesteps_to_show) == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    \n",
    "    for i, t in enumerate(timesteps_to_show):\n",
    "        if t == 0:\n",
    "            corrupted = x0\n",
    "            noise_level = 0.0\n",
    "        else:\n",
    "            try:\n",
    "                corrupted, epsilon = forward_impl.direct_forward_jump(x0, t)\n",
    "                if forward_impl.scheduler.alpha_cumprod is not None:\n",
    "                    signal_level = forward_impl.scheduler.alpha_cumprod[t-1].item()\n",
    "                    noise_level = 1 - signal_level\n",
    "                else:\n",
    "                    noise_level = t / 1000  # Rough estimate\n",
    "            except:\n",
    "                corrupted = x0\n",
    "                noise_level = 0.0\n",
    "        \n",
    "        # Display original and corrupted\n",
    "        axes[0, i].imshow(x0[0, 0].cpu(), cmap='gray')\n",
    "        axes[0, i].set_title(f'Original')\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        axes[1, i].imshow(corrupted[0, 0].cpu(), cmap='gray')\n",
    "        axes[1, i].set_title(f't={t}\\nNoise: {noise_level:.2f}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test forward jump implementation (use scheduler from previous section)\n",
    "# First ensure a scheduler is set up\n",
    "if 'scheduler' not in locals() or scheduler.betas is None:\n",
    "    scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "    scheduler.precompute_schedule(\"linear\")\n",
    "\n",
    "forward_jump = ForwardJumpImplementation(scheduler)\n",
    "\n",
    "# Visualize corruption process (uncomment after implementing direct_forward_jump)\n",
    "timesteps_to_show = [0, 100, 300, 500, 700, 999]\n",
    "# visualize_corruption_process(forward_jump, test_image, timesteps_to_show)\n",
    "\n",
    "# Benchmark performance (uncomment after implementing both approaches)\n",
    "timesteps_to_test = [10, 50, 100, 500, 1000]\n",
    "# results = benchmark_forward_approaches(forward_jump, test_image, timesteps_to_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d721862",
   "metadata": {},
   "source": [
    "### Task 4.2: Mathematical Derivation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4126324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardJumpDerivation:\n",
    "    \"\"\"\n",
    "    Implement step-by-step mathematical derivation.\n",
    "    This class helps you understand exactly how the forward jump formula\n",
    "    emerges from Gaussian arithmetic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def two_step_expansion(self, x0: torch.Tensor, beta1: float, beta2: float) -> dict:\n",
    "        \"\"\"\n",
    "        TODO: Manually expand the first two steps to see the pattern\n",
    "        \n",
    "        Starting from:\n",
    "        x1 = sqrt(1-beta1) * x0 + sqrt(beta1) * eps0\n",
    "        x2 = sqrt(1-beta2) * x1 + sqrt(beta2) * eps1\n",
    "        \n",
    "        Expand x2 by substituting x1, then use Gaussian arithmetic to\n",
    "        combine the noise terms.\n",
    "        \n",
    "        Steps:\n",
    "        1. Substitute x1 into x2 equation\n",
    "        2. Expand and collect terms\n",
    "        3. Identify signal coefficient for x0\n",
    "        4. Identify combined noise coefficient\n",
    "        5. Use Gaussian arithmetic: sqrt(a)*eps1 + sqrt(b)*eps2 = sqrt(a+b)*eps\n",
    "        \n",
    "        Args:\n",
    "            x0: Starting clean image\n",
    "            beta1: First timestep noise level\n",
    "            beta2: Second timestep noise level\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains analytical and computed results for comparison\n",
    "        \"\"\"\n",
    "        print(f\"=== Two-Step Mathematical Derivation ===\")\n",
    "        print(f\"beta1 = {beta1:.4f}, beta2 = {beta2:.4f}\\n\")\n",
    "        \n",
    "        # TODO: Step 1 - Compute x1 using the single-step formula\n",
    "        # Hint: x1 = sqrt(1-beta1) * x0 + sqrt(beta1) * eps0\n",
    "        \n",
    "        # TODO: Step 2 - Substitute x1 into the x2 equation and expand\n",
    "        # Hint: Replace x1 in the x2 formula and collect terms\n",
    "        \n",
    "        # TODO: Step 3 - Apply Gaussian arithmetic to combine noise terms\n",
    "        # Hint: Independent Gaussians add their variances\n",
    "        \n",
    "        # TODO: Step 4 - Verify your result matches the theoretical formula\n",
    "        # Hint: Should equal sqrt(alpha1*alpha2) * x0 + sqrt(1-alpha1*alpha2) * eps\n",
    "        \n",
    "        # Provided: Show the mathematical steps for reference\n",
    "        alpha1 = 1 - beta1\n",
    "        alpha2 = 1 - beta2\n",
    "        \n",
    "        print(f\"Mathematical structure:\")\n",
    "        print(f\"  x1 = sqrt({alpha1:.4f}) * x0 + sqrt({beta1:.4f}) * eps0\")\n",
    "        print(f\"  x2 = sqrt({alpha2:.4f}) * x1 + sqrt({beta2:.4f}) * eps1\")\n",
    "        print(f\"  After substitution, identify the coefficients...\")\n",
    "        \n",
    "        combined_noise_var = alpha2 * beta1 + beta2\n",
    "        theoretical_noise_var = 1 - alpha1 * alpha2\n",
    "        \n",
    "        print(f\"\\nGaussian arithmetic:\")\n",
    "        print(f\"  Combined noise variance = {alpha2*beta1:.4f} + {beta2:.4f} = {combined_noise_var:.4f}\")\n",
    "        print(f\"  Theoretical variance = 1 - {alpha1*alpha2:.4f} = {theoretical_noise_var:.4f}\")\n",
    "        print(f\"  Match: {abs(combined_noise_var - theoretical_noise_var) < 1e-10}\")\n",
    "        \n",
    "        # TODO: Implement your derivation and return results\n",
    "        results = {\n",
    "            'signal_coeff_expanded': None,    # TODO: Compute from your expansion\n",
    "            'signal_coeff_direct': None,      # TODO: Compute from direct formula\n",
    "            'noise_var_expanded': None,       # TODO: Compute combined noise variance\n",
    "            'noise_var_theoretical': None,    # TODO: Compute theoretical variance\n",
    "            'coefficients_match': None,       # TODO: Check if they match\n",
    "        }\n",
    "        \n",
    "        return results Gaussian arithmetic to combine noise terms\n",
    "        # Combined variance: alpha2 * beta1 + beta2\n",
    "        # combined_noise_var = alpha2 * beta1 + beta2\n",
    "        # combined_noise_std = math.sqrt(combined_noise_var)\n",
    "        \n",
    "        # TODO: Step 4 - Verify this equals 1 - alpha1*alpha2\n",
    "        # theoretical_noise_var = 1 - alpha1 * alpha2\n",
    "        # theoretical_noise_std = math.sqrt(theoretical_noise_var)\n",
    "        \n",
    "        # TODO: Step 5 - Create direct formula version\n",
    "        # alpha_cumprod_2 = alpha1 * alpha2\n",
    "        # eps_combined = torch.randn_like(x0)\n",
    "        # x2_direct = math.sqrt(alpha_cumprod_2) * x0 + math.sqrt(1 - alpha_cumprod_2) * eps_combined\n",
    "        \n",
    "        # Show the mathematical steps\n",
    "        alpha1 = 1 - beta1\n",
    "        alpha2 = 1 - beta2\n",
    "        \n",
    "        print(f\"Step-by-step expansion:\")\n",
    "        print(f\"  x1 = sqrt({alpha1:.4f}) * x0 + sqrt({beta1:.4f}) * eps0\")\n",
    "        print(f\"  x2 = sqrt({alpha2:.4f}) * x1 + sqrt({beta2:.4f}) * eps1\")\n",
    "        print(f\"  x2 = sqrt({alpha2:.4f}) * [sqrt({alpha1:.4f}) * x0 + sqrt({beta1:.4f}) * eps0] + sqrt({beta2:.4f}) * eps1\")\n",
    "        print(f\"  x2 = sqrt({alpha1*alpha2:.4f}) * x0 + sqrt({alpha2*beta1:.4f}) * eps0 + sqrt({beta2:.4f}) * eps1\")\n",
    "        \n",
    "        combined_noise_var = alpha2 * beta1 + beta2\n",
    "        theoretical_noise_var = 1 - alpha1 * alpha2\n",
    "        \n",
    "        print(f\"\\nGaussian arithmetic:\")\n",
    "        print(f\"  Combined noise variance = {alpha2*beta1:.4f} + {beta2:.4f} = {combined_noise_var:.4f}\")\n",
    "        print(f\"  Theoretical variance = 1 - {alpha1*alpha2:.4f} = {theoretical_noise_var:.4f}\")\n",
    "        print(f\"  Match: {abs(combined_noise_var - theoretical_noise_var) < 1e-10}\")\n",
    "        \n",
    "        results = {\n",
    "            'signal_coeff_expanded': math.sqrt(alpha1 * alpha2),\n",
    "            'signal_coeff_direct': math.sqrt(alpha1 * alpha2),\n",
    "            'noise_var_expanded': combined_noise_var,\n",
    "            'noise_var_theoretical': theoretical_noise_var,\n",
    "            'coefficients_match': abs(combined_noise_var - theoretical_noise_var) < 1e-10,\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def general_derivation_proof(self, num_steps: int = 5):\n",
    "        \"\"\"\n",
    "        TODO: Prove the general case by induction\n",
    "        \n",
    "        Show that if the formula holds for n steps, it also holds for n+1 steps.\n",
    "        This proves the forward jump formula for any number of timesteps.\n",
    "        \n",
    "        Base case: x1 = sqrt(alpha1) * x0 + sqrt(1-alpha1) * eps\n",
    "        Inductive step: If x_n = sqrt(alpha_cumprod_n) * x0 + sqrt(1-alpha_cumprod_n) * eps\n",
    "                       Then x_{n+1} = sqrt(alpha_cumprod_{n+1}) * x0 + sqrt(1-alpha_cumprod_{n+1}) * eps\n",
    "        \n",
    "        Args:\n",
    "            num_steps: Number of steps to verify the pattern\n",
    "        \"\"\"\n",
    "        print(f\"=== General Derivation Proof ===\\n\")\n",
    "        \n",
    "        betas = torch.tensor([0.01, 0.02, 0.03, 0.04, 0.05])[:num_steps]\n",
    "        \n",
    "        for n in range(1, num_steps + 1):\n",
    "            print(f\"Step {n}:\")\n",
    "            \n",
    "            # TODO: Compute alpha_cumprod for n steps and verify properties\n",
    "            # Hint: Use torch.prod for cumulative product\n",
    "            \n",
    "            # Show what the pattern should be\n",
    "            alphas = 1 - betas[:n]\n",
    "            alpha_cumprod_n = torch.prod(alphas)\n",
    "            \n",
    "            signal_coeff_sq = alpha_cumprod_n\n",
    "            noise_coeff_sq = 1 - alpha_cumprod_n\n",
    "            total_variance = signal_coeff_sq + noise_coeff_sq\n",
    "            \n",
    "            print(f\"  Alpha cumprod: {alpha_cumprod_n:.6f}\")\n",
    "            print(f\"  Signal coeff²: {signal_coeff_sq:.6f}\")\n",
    "            print(f\"  Noise coeff²: {noise_coeff_sq:.6f}\")\n",
    "            print(f\"  Total variance: {total_variance:.6f} (should be 1.0)\")\n",
    "            print()\n",
    "\n",
    "# Test mathematical derivation\n",
    "derivation = ForwardJumpDerivation()\n",
    "\n",
    "# Test two-step case\n",
    "results = derivation.two_step_expansion(test_image, beta1=0.01, beta2=0.02)\n",
    "print(f\"\\nResults summary:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test general case\n",
    "derivation.general_derivation_proof(num_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949a997",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Efficient Training Data Generation (15 minutes)\n",
    "\n",
    "### Task 5.1: Build Production-Ready Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd90ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionTrainingDataGenerator:\n",
    "    \"\"\"\n",
    "    Implement efficient training data generation pipeline.\n",
    "    This is the core component that enables practical diffusion model training.\n",
    "    It must be fast, memory-efficient, and mathematically correct.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scheduler: NoiseScheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def generate_training_sample(self, x0: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Generate a single training sample using random timestep sampling\n",
    "        \n",
    "        This is the core training data generation function that creates\n",
    "        unlimited training data from any clean image.\n",
    "        \n",
    "        Steps:\n",
    "        1. Sample random timestep t from [1, T]\n",
    "        2. Sample random noise epsilon ~ N(0, I)\n",
    "        3. Apply forward jump: x_t = sqrt(alpha_cumprod_t) * x0 + sqrt(1-alpha_cumprod_t) * epsilon\n",
    "        4. Return (x_t, t, epsilon) as training triple\n",
    "        \n",
    "        The neural network will learn to predict epsilon given (x_t, t).\n",
    "        \n",
    "        Args:\n",
    "            x0: Clean image(s) - shape (batch_size, channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Noisy image at random timestep\n",
    "            t: The timestep that was sampled\n",
    "            epsilon: The noise that was added (training target)\n",
    "        \"\"\"\n",
    "        batch_size = x0.shape[0]\n",
    "        \n",
    "        # TODO: Sample random timesteps for each image in batch\n",
    "        # Hint: Use torch.randint(1, self.scheduler.num_timesteps + 1, (batch_size,), device=x0.device)\n",
    "        \n",
    "        # TODO: Sample noise\n",
    "        # Hint: Use torch.randn_like(x0)\n",
    "        \n",
    "        # TODO: Get coefficients for sampled timesteps (convert to 0-indexed)\n",
    "        # Hint: Use self.scheduler.sqrt_alpha_cumprod and self.scheduler.sqrt_one_minus_alpha_cumprod\n",
    "        \n",
    "        # TODO: Reshape coefficients for broadcasting with image dimensions\n",
    "        # Hint: Use .view(-1, 1, 1, 1) for 4D tensors\n",
    "        \n",
    "        # TODO: Apply forward jump formula\n",
    "        # Hint: x_t = sqrt_alpha_cumprod_t * x0 + sqrt_one_minus_alpha_cumprod_t * epsilon\n",
    "        \n",
    "        # return x_t, t, epsilon\n",
    "        pass\n",
    "\n",
    "# Analysis functions (provided)\n",
    "def analyze_training_distribution(generator, test_image, num_samples=5000):\n",
    "    \"\"\"Analyze the distribution of training data\"\"\"\n",
    "    print(f\"=== Training Data Distribution Analysis ===\\n\")\n",
    "    \n",
    "    all_timesteps = []\n",
    "    all_snrs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples // 100):  # Generate in batches\n",
    "            try:\n",
    "                batch_x0 = test_image.repeat(100, 1, 1, 1)\n",
    "                x_t, t, epsilon = generator.generate_training_sample(batch_x0)\n",
    "                \n",
    "                all_timesteps.extend(t.cpu().tolist())\n",
    "                \n",
    "                # Compute SNR for each sample\n",
    "                for i in range(len(t)):\n",
    "                    t_idx = t[i] - 1\n",
    "                    if generator.scheduler.alpha_cumprod is not None:\n",
    "                        alpha_cumprod = generator.scheduler.alpha_cumprod[t_idx]\n",
    "                        snr = alpha_cumprod / (1 - alpha_cumprod)\n",
    "                        all_snrs.append(snr.item())\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Training sample generation not implemented: {e}\")\n",
    "                return\n",
    "    \n",
    "    if all_timesteps:\n",
    "        # Plot timestep distribution\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].hist(all_timesteps, bins=50, alpha=0.7, density=True, color='blue')\n",
    "        axes[0].set_title('Timestep Distribution\\n(Should be uniform)')\n",
    "        axes[0].set_xlabel('Timestep')\n",
    "        axes[0].set_ylabel('Density')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        if all_snrs:\n",
    "            axes[1].hist(all_snrs, bins=50, alpha=0.7, density=True, color='green')\n",
    "            axes[1].set_title('Signal-to-Noise Ratio Distribution')\n",
    "            axes[1].set_xlabel('SNR')\n",
    "            axes[1].set_ylabel('Density')\n",
    "            axes[1].set_yscale('log')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # SNR vs timestep scatter plot\n",
    "            timestep_snr_pairs = list(zip(all_timesteps[:1000], all_snrs[:1000]))\n",
    "            timesteps_plot, snrs_plot = zip(*timestep_snr_pairs)\n",
    "            axes[2].scatter(timesteps_plot, snrs_plot, alpha=0.5, s=1, color='red')\n",
    "            axes[2].set_title('SNR vs Timestep')\n",
    "            axes[2].set_xlabel('Timestep')\n",
    "            axes[2].set_ylabel('SNR')\n",
    "            axes[2].set_yscale('log')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"Timestep statistics:\")\n",
    "        print(f\"  Mean: {np.mean(all_timesteps):.2f} (should be ~{generator.scheduler.num_timesteps/2})\")\n",
    "        print(f\"  Std: {np.std(all_timesteps):.2f}\")\n",
    "        print(f\"  Range: [{min(all_timesteps)}, {max(all_timesteps)}]\")\n",
    "        \n",
    "        if all_snrs:\n",
    "            print(f\"\\nSNR statistics:\")\n",
    "            print(f\"  Mean: {np.mean(all_snrs):.4f}\")\n",
    "            print(f\"  Range: [{min(all_snrs):.6f}, {max(all_snrs):.2f}]\")\n",
    "\n",
    "def benchmark_generation_speed(generator, test_image, batch_sizes=[1, 32, 128, 512]):\n",
    "    \"\"\"Benchmark training data generation speed\"\"\"\n",
    "    print(\"=== Training Data Generation Benchmark ===\\n\")\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Create test batch\n",
    "        x0_batch = test_image.repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        try:\n",
    "            # Warmup\n",
    "            for _ in range(5):\n",
    "                generator.generate_training_sample(x0_batch)\n",
    "            \n",
    "            # Benchmark\n",
    "            num_iterations = 100\n",
    "            start_time = time.time()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_iterations):\n",
    "                    x_t, t, epsilon = generator.generate_training_sample(x0_batch)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            total_samples = batch_size * num_iterations\n",
    "            samples_per_second = total_samples / (end_time - start_time)\n",
    "            \n",
    "            print(f\"Batch size {batch_size:3d}: {samples_per_second:8.1f} samples/sec\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Batch size {batch_size:3d}: Not implemented ({e})\")\n",
    "\n",
    "# Test training data generation\n",
    "if 'scheduler' not in locals() or scheduler.alpha_cumprod is None:\n",
    "    scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "    scheduler.precompute_schedule(\"linear\")\n",
    "\n",
    "generator = DiffusionTrainingDataGenerator(scheduler)\n",
    "\n",
    "# Test single sample generation (uncomment after implementing)\n",
    "try:\n",
    "    x_t, t, epsilon = generator.generate_training_sample(test_image)\n",
    "    print(f\"Generated training sample:\")\n",
    "    print(f\"  Input shape: {test_image.shape}\")\n",
    "    print(f\"  Output x_t shape: {x_t.shape}\")\n",
    "    print(f\"  Timestep: {t.item()}\")\n",
    "    print(f\"  Epsilon shape: {epsilon.shape}\")\n",
    "except:\n",
    "    print(\"Training sample generation not yet implemented\")\n",
    "\n",
    "# Analyze training distribution (uncomment after implementing)\n",
    "# analyze_training_distribution(generator, test_image, num_samples=5000)\n",
    "\n",
    "# Benchmark speed (uncomment after implementing)\n",
    "# benchmark_generation_speed(generator, test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077343f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Comprehensive Testing and Validation (10 minutes)\n",
    "\n",
    "### Task 6.1: Validate Mathematical Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_validation_suite():\n",
    "    \"\"\"\n",
    "    Comprehensive validation of all components\n",
    "    \n",
    "    This function tests that all mathematical implementations are correct\n",
    "    and match theoretical expectations.\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive Validation Suite ===\\n\")\n",
    "    \n",
    "    # Setup for testing\n",
    "    if 'scheduler' not in locals() or scheduler.alpha_cumprod is None:\n",
    "        test_scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
    "        test_scheduler.precompute_schedule(\"linear\")\n",
    "    else:\n",
    "        test_scheduler = scheduler\n",
    "    \n",
    "    # Test 1: Variance preservation\n",
    "    print(\"Test 1: Variance Preservation\")\n",
    "    x0 = torch.randn(100, 1, 28, 28, device=device)  # Batch of random images\n",
    "    \n",
    "    forward_impl = ForwardJumpImplementation(test_scheduler)\n",
    "    \n",
    "    for t in [1, 100, 500, 999]:\n",
    "        try:\n",
    "            x_t, epsilon = forward_impl.direct_forward_jump(x0, t)\n",
    "            \n",
    "            # Check that variance is preserved\n",
    "            original_var = torch.var(x0)\n",
    "            corrupted_var = torch.var(x_t)\n",
    "            \n",
    "            print(f\"  Timestep {t}: Original var = {original_var:.4f}, Corrupted var = {corrupted_var:.4f}\")\n",
    "            if abs(original_var - corrupted_var) > 0.1:\n",
    "                print(f\"    ⚠️ Warning: Variance not well preserved!\")\n",
    "            else:\n",
    "                print(f\"    ✓ Variance preserved\")\n",
    "        except:\n",
    "            print(f\"  Timestep {t}: Forward jump not implemented\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 2: Boundary conditions\n",
    "    print(\"Test 2: Boundary Conditions\")\n",
    "    \n",
    "    try:\n",
    "        # Test t=0 case\n",
    "        x_0, eps_0 = forward_impl.direct_forward_jump(x0, 0)\n",
    "        if torch.allclose(x_0, x0):\n",
    "            print(\"  ✓ t=0 returns original image\")\n",
    "        else:\n",
    "            print(\"  ❌ t=0 should return original image\")\n",
    "    except:\n",
    "        print(\"  t=0 test: Not implemented\")\n",
    "    \n",
    "    try:\n",
    "        # Test t=T case (should be mostly noise)\n",
    "        x_T, eps_T = forward_impl.direct_forward_jump(x0, 999)\n",
    "        correlation = torch.corrcoef(torch.stack([x0.flatten(), x_T.flatten()]))[0, 1]\n",
    "        if abs(correlation) < 0.1:\n",
    "            print(f\"  ✓ t=T produces uncorrelated noise (correlation: {correlation:.4f})\")\n",
    "        else:\n",
    "            print(f\"  ⚠️ t=T correlation with original: {correlation:.4f} (should be ~0)\")\n",
    "    except:\n",
    "        print(\"  t=T test: Not implemented\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 3: Training data generation consistency\n",
    "    print(\"Test 3: Training Data Generation\")\n",
    "    \n",
    "    try:\n",
    "        generator = DiffusionTrainingDataGenerator(test_scheduler)\n",
    "        timesteps = []\n",
    "        \n",
    "        for _ in range(1000):\n",
    "            x_t, t, eps = generator.generate_training_sample(test_image)\n",
    "            timesteps.append(t.item())\n",
    "        \n",
    "        # Check uniform distribution of timesteps\n",
    "        timestep_std = np.std(timesteps)\n",
    "        expected_std = math.sqrt((test_scheduler.num_timesteps**2 - 1) / 12)  # Uniform distribution std\n",
    "        \n",
    "        print(f\"  Timestep std: {timestep_std:.2f}, Expected: {expected_std:.2f}\")\n",
    "        if abs(timestep_std - expected_std) < 50:\n",
    "            print(\"  ✓ Timestep distribution appears uniform\")\n",
    "        else:\n",
    "            print(\"  ⚠️ Timestep distribution may not be uniform\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Training data generation test: Not implemented ({e})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 4: Schedule properties\n",
    "    print(\"Test 4: Schedule Properties\")\n",
    "    \n",
    "    if test_scheduler.betas is not None:\n",
    "        # Test that betas are in valid range\n",
    "        if torch.all(test_scheduler.betas > 0) and torch.all(test_scheduler.betas < 1):\n",
    "            print(\"  ✓ All betas in valid range (0, 1)\")\n",
    "        else:\n",
    "            print(\"  ❌ Some betas outside valid range\")\n",
    "        \n",
    "        # Test that alpha_cumprod is decreasing\n",
    "        if test_scheduler.alpha_cumprod is not None:\n",
    "            alpha_diffs = test_scheduler.alpha_cumprod[1:] - test_scheduler.alpha_cumprod[:-1]\n",
    "            if torch.all(alpha_diffs <= 0):\n",
    "                print(\"  ✓ Alpha cumprod is non-increasing\")\n",
    "            else:\n",
    "                print(\"  ❌ Alpha cumprod should be non-increasing\")\n",
    "        else:\n",
    "            print(\"  Alpha cumprod not computed\")\n",
    "    else:\n",
    "        print(\"  Schedule not implemented\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Test 5: Reparameterization equivalence\n",
    "    print(\"Test 5: Reparameterization Equivalence\")\n",
    "    \n",
    "    try:\n",
    "        reparam_demo = ReparameterizationDemo()\n",
    "        mu = torch.zeros(1, 1, device=device)\n",
    "        sigma = torch.ones(1, 1, device=device) * 0.5\n",
    "        \n",
    "        # Test multiple samples for statistical equivalence\n",
    "        samples_direct = []\n",
    "        samples_reparam = []\n",
    "        \n",
    "        for _ in range(100):\n",
    "            try:\n",
    "                sample_d = reparam_demo.direct_sampling_approach(mu, sigma)\n",
    "                samples_direct.append(sample_d.item())\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            try:\n",
    "                sample_r = reparam_demo.reparameterized_approach(mu, sigma)\n",
    "                samples_reparam.append(sample_r.item())\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if samples_direct and samples_reparam:\n",
    "            mean_diff = abs(np.mean(samples_direct) - np.mean(samples_reparam))\n",
    "            std_diff = abs(np.std(samples_direct) - np.std(samples_reparam))\n",
    "            \n",
    "            print(f\"  Mean difference: {mean_diff:.4f}\")\n",
    "            print(f\"  Std difference: {std_diff:.4f}\")\n",
    "            \n",
    "            if mean_diff < 0.1 and std_diff < 0.1:\n",
    "                print(\"  ✓ Both approaches produce similar statistics\")\n",
    "            else:\n",
    "                print(\"  ⚠️ Approaches may not be equivalent\")\n",
    "        else:\n",
    "            print(\"  Reparameterization approaches not implemented\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Reparameterization test failed: {e}\")\n",
    "    \n",
    "    print(\"\\n🎉 Validation suite completed!\")\n",
    "\n",
    "# Run comprehensive validation\n",
    "comprehensive_validation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bca9a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Numerical Stability Analysis (5 minutes)\n",
    "\n",
    "### Task 7.1: Handle Edge Cases and Numerical Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a6a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericalStabilityAnalysis:\n",
    "    \"\"\"\n",
    "    Implement numerical stability checks and fixes.\n",
    "    Real diffusion implementations must handle numerical edge cases\n",
    "    that can break training or inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scheduler: NoiseScheduler):\n",
    "        self.scheduler = scheduler\n",
    "    \n",
    "    def analyze_extreme_timesteps(self):\n",
    "        \"\"\"\n",
    "        Analyze behavior at extreme timesteps\n",
    "        \n",
    "        Check what happens at the boundaries:\n",
    "        - t = 0: Should preserve original image exactly\n",
    "        - t = T: Should produce pure noise\n",
    "        - Very early timesteps: Barely perceptible corruption\n",
    "        - Very late timesteps: Nearly pure noise\n",
    "        \"\"\"\n",
    "        print(\"=== Extreme Timestep Analysis ===\\n\")\n",
    "        \n",
    "        extreme_timesteps = [0, 1, 2, 998, 999, 1000]\n",
    "        \n",
    "        for t in extreme_timesteps:\n",
    "            if t == 0:\n",
    "                # Special case: no corruption\n",
    "                signal_coeff = 1.0\n",
    "                noise_coeff = 0.0\n",
    "                alpha_cumprod = 1.0\n",
    "            elif t > self.scheduler.num_timesteps:\n",
    "                print(f\"Timestep {t}: Out of range\")\n",
    "                continue\n",
    "            else:\n",
    "                if self.scheduler.alpha_cumprod is not None:\n",
    "                    t_idx = t - 1\n",
    "                    alpha_cumprod = self.scheduler.alpha_cumprod[t_idx].item()\n",
    "                    signal_coeff = math.sqrt(alpha_cumprod)\n",
    "                    noise_coeff = math.sqrt(1 - alpha_cumprod)\n",
    "                else:\n",
    "                    print(f\"Timestep {t}: Schedule not computed\")\n",
    "                    continue\n",
    "            \n",
    "            # Compute SNR and analyze\n",
    "            if noise_coeff > 0:\n",
    "                snr = signal_coeff / noise_coeff\n",
    "            else:\n",
    "                snr = float('inf')\n",
    "            \n",
    "            print(f\"Timestep {t:4d}:\")\n",
    "            print(f\"  Alpha cumprod: {alpha_cumprod:.8f}\")\n",
    "            print(f\"  Signal coeff:  {signal_coeff:.8f}\")\n",
    "            print(f\"  Noise coeff:   {noise_coeff:.8f}\")\n",
    "            print(f\"  SNR:          {snr:.4f}\")\n",
    "            print()\n",
    "    \n",
    "    def test_precision_limits(self):\n",
    "        \"\"\"\n",
    "        Test numerical precision limits\n",
    "        \n",
    "        Check for potential issues:\n",
    "        - Underflow in alpha_cumprod for large t\n",
    "        - Overflow in SNR calculations\n",
    "        - Loss of precision in sqrt operations\n",
    "        \"\"\"\n",
    "        print(\"=== Numerical Precision Analysis ===\\n\")\n",
    "        \n",
    "        if self.scheduler.alpha_cumprod is None:\n",
    "            print(\"Schedule not computed - cannot analyze precision\")\n",
    "            return\n",
    "        \n",
    "        # Find where alpha_cumprod becomes dangerously small\n",
    "        dangerous_threshold = 1e-7\n",
    "        underflow_timesteps = torch.where(self.scheduler.alpha_cumprod < dangerous_threshold)[0]\n",
    "        \n",
    "        if len(underflow_timesteps) > 0:\n",
    "            first_underflow = underflow_timesteps[0].item() + 1  # Convert to 1-indexed\n",
    "            print(f\"Alpha cumprod drops below {dangerous_threshold} at timestep {first_underflow}\")\n",
    "            print(f\"This could cause numerical instability!\")\n",
    "        else:\n",
    "            print(f\"Alpha cumprod stays above {dangerous_threshold} for all timesteps\")\n",
    "        \n",
    "        # Check for NaN or Inf values\n",
    "        nan_check = torch.isnan(self.scheduler.alpha_cumprod).any()\n",
    "        inf_check = torch.isinf(self.scheduler.alpha_cumprod).any()\n",
    "        \n",
    "        print(f\"Contains NaN values: {nan_check}\")\n",
    "        print(f\"Contains Inf values: {inf_check}\")\n",
    "        \n",
    "        # Test sqrt operations near zero\n",
    "        min_alpha_cumprod = self.scheduler.alpha_cumprod.min().item()\n",
    "        print(f\"Minimum alpha_cumprod: {min_alpha_cumprod:.2e}\")\n",
    "        print(f\"sqrt(min_alpha_cumprod): {math.sqrt(min_alpha_cumprod):.2e}\")\n",
    "    \n",
    "    def propose_stability_fixes(self):\n",
    "        \"\"\"\n",
    "        Implement common stability fixes\n",
    "        \n",
    "        Suggest and implement fixes for numerical issues:\n",
    "        1. Clamping alpha_cumprod to prevent underflow\n",
    "        2. Using log-space arithmetic for very small numbers\n",
    "        3. Adding epsilon to prevent division by zero\n",
    "        \"\"\"\n",
    "        print(\"=== Stability Fixes ===\\n\")\n",
    "        \n",
    "        if self.scheduler.alpha_cumprod is None:\n",
    "            print(\"Schedule not computed - cannot propose fixes\")\n",
    "            return\n",
    "        \n",
    "        # Implement clamping fix\n",
    "        min_clip = 1e-8\n",
    "        alpha_cumprod_clipped = torch.clamp(self.scheduler.alpha_cumprod, min=min_clip)\n",
    "        \n",
    "        print(f\"Original min alpha_cumprod: {self.scheduler.alpha_cumprod.min():.2e}\")\n",
    "        print(f\"Clipped min alpha_cumprod: {alpha_cumprod_clipped.min():.2e}\")\n",
    "        \n",
    "        # Implement epsilon addition for division\n",
    "        eps = 1e-8\n",
    "        safe_snr = self.scheduler.alpha_cumprod / (1 - self.scheduler.alpha_cumprod + eps)\n",
    "        \n",
    "        print(f\"Max safe SNR: {safe_snr.max():.2e}\")\n",
    "        \n",
    "        # Show log-space alternative\n",
    "        log_alpha_cumprod = torch.log(torch.clamp(self.scheduler.alpha_cumprod, min=1e-10))\n",
    "        print(f\"Log-space range: [{log_alpha_cumprod.min():.2f}, {log_alpha_cumprod.max():.2f}]\")\n",
    "\n",
    "# Test numerical stability (use existing scheduler)\n",
    "if 'scheduler' in locals() and scheduler.alpha_cumprod is not None:\n",
    "    stability = NumericalStabilityAnalysis(scheduler)\n",
    "    stability.analyze_extreme_timesteps()\n",
    "    stability.test_precision_limits()\n",
    "    stability.propose_stability_fixes()\n",
    "else:\n",
    "    print(\"No valid scheduler available for numerical stability analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edfc92",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Reflection and Integration (10 minutes)\n",
    "\n",
    "### Task 8.1: Connect Lab to Lecture Concepts\n",
    "\n",
    "**Discussion Questions** (Work with your partner):\n",
    "\n",
    "1. **Reparameterization Impact**: \n",
    "   - How did implementing both sampling approaches help you understand why the reparameterization trick is essential?\n",
    "   - What would happen to training if we couldn't use this trick?\n",
    "\n",
    "2. **Forward Jump Power**:\n",
    "   - Quantify the computational advantage: How much faster is direct jumping vs sequential?\n",
    "   - Why is this property unique to Gaussian distributions?\n",
    "\n",
    "3. **Schedule Design**:\n",
    "   - Which noise schedule worked best for your test cases and why?\n",
    "   - How do different schedules affect the signal-to-noise ratio progression?\n",
    "\n",
    "4. **Mathematical Elegance**:\n",
    "   - What surprised you most about the mathematical structure?\n",
    "   - How does the forward process set up the reverse process for success?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_mathematical_achievements():\n",
    "    \"\"\"\n",
    "    Reflect on the mathematical concepts implemented today\n",
    "    \"\"\"\n",
    "    print(\"=== Your Mathematical Achievements Today ===\\n\")\n",
    "    \n",
    "    concepts_implemented = [\n",
    "        \"🔧 Reparameterization trick (enabling gradient flow)\",\n",
    "        \"📊 Noise schedules (linear, cosine, exponential)\",\n",
    "        \"⚡ Forward jump formula (1000x speedup)\",\n",
    "        \"🎯 Training data generation (unlimited samples)\",\n",
    "        \"📐 Mathematical derivations (Gaussian arithmetic)\",\n",
    "        \"🔬 Numerical stability analysis (production-ready)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Core mathematical concepts you implemented:\")\n",
    "    for concept in concepts_implemented:\n",
    "        print(f\"  {concept}\")\n",
    "    \n",
    "    print(f\"\\n🎓 Mathematical foundation completed:\")\n",
    "    print(f\"   • Forward diffusion process (corruption)\")\n",
    "    print(f\"   • Efficient training data generation\") \n",
    "    print(f\"   • Mathematical rigor and stability\")\n",
    "    print(f\"   • Ready for reverse process implementation!\")\n",
    "    \n",
    "    print(f\"\\n🔬 Key insights gained:\")\n",
    "    print(f\"   • Why reparameterization enables neural network training\")\n",
    "    print(f\"   • How Gaussian arithmetic leads to O(1) forward jumps\")\n",
    "    print(f\"   • Why different schedules affect training dynamics\")\n",
    "    print(f\"   • How mathematical elegance enables practical algorithms\")\n",
    "    \n",
    "    # Create a visual summary\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Mathematical progression flowchart\n",
    "    stages = {\n",
    "        'Problem': ['Sequential\\nSteps', 'No Gradients'],\n",
    "        'Solution': ['Reparameterization', 'Forward Jumps'],\n",
    "        'Implementation': ['Noise Schedules', 'Training Pipeline'],\n",
    "        'Validation': ['Numerical Stability', 'Performance Tests'],\n",
    "        'Ready': ['Efficient Training', 'Reverse Process']\n",
    "    }\n",
    "    \n",
    "    y_positions = [0.8, 0.65, 0.5, 0.35, 0.2]\n",
    "    colors = ['red', 'orange', 'blue', 'green', 'purple']\n",
    "    \n",
    "    for i, (stage, concepts) in enumerate(stages.items()):\n",
    "        y = y_positions[i]\n",
    "        color = colors[i]\n",
    "        \n",
    "        # Stage label\n",
    "        ax.text(0.05, y, stage, fontsize=14, weight='bold', color=color)\n",
    "        \n",
    "        # Concepts\n",
    "        x_positions = [0.3 + j * 0.25 for j in range(len(concepts))]\n",
    "        for j, concept in enumerate(concepts):\n",
    "            ax.text(x_positions[j], y, concept, fontsize=11, ha='center',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
    "    \n",
    "    # Add arrows showing progression\n",
    "    for i in range(len(y_positions) - 1):\n",
    "        ax.arrow(0.5, y_positions[i] - 0.05, 0, -0.05, head_width=0.02, head_length=0.01,\n",
    "                fc='gray', ec='gray', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Forward Diffusion Implementation Journey', fontsize=16, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Performance summary function (provided)\n",
    "def performance_summary():\n",
    "    \"\"\"Summarize the performance gains achieved\"\"\"\n",
    "    print(\"\\n=== Performance Gains Summary ===\\n\")\n",
    "    \n",
    "    print(\"🚀 Computational Revolution:\")\n",
    "    print(\"   • Sequential approach: O(T) operations\")\n",
    "    print(\"   • Forward jump approach: O(1) operations\")  \n",
    "    print(\"   • Typical speedup: 100-1000x for T=1000\")\n",
    "    print(\"   • Memory usage: Constant vs linear\")\n",
    "    \n",
    "    print(\"\\n⚡ Training Efficiency:\")\n",
    "    print(\"   • Unlimited training data from any image\")\n",
    "    print(\"   • Random timestep sampling\")\n",
    "    print(\"   • Batch processing support\")\n",
    "    print(\"   • GPU acceleration ready\")\n",
    "    \n",
    "    print(\"\\n🎯 Mathematical Elegance:\")\n",
    "    print(\"   • Gaussian arithmetic enables direct jumps\")\n",
    "    print(\"   • Reparameterization preserves gradients\")\n",
    "    print(\"   • Schedule design controls corruption dynamics\")\n",
    "    print(\"   • Numerical stability ensures robust training\")\n",
    "\n",
    "# Create comprehensive summary\n",
    "summarize_mathematical_achievements()\n",
    "performance_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3107aa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "### Core Mathematical Functions (Students Implement):\n",
    "\n",
    "**✅ Essential TODOs:**\n",
    "- [ ] `direct_sampling_approach()` - Direct Gaussian sampling (wrong way)\n",
    "- [ ] `reparameterized_approach()` - Reparameterization trick (right way)\n",
    "- [ ] `forward_step_reparameterized()` - Diffusion-specific reparameterization\n",
    "- [ ] `linear_schedule()` - Linear noise schedule\n",
    "- [ ] `cosine_schedule()` - Cosine noise schedule  \n",
    "- [ ] `exponential_schedule()` - Exponential noise schedule\n",
    "- [ ] `precompute_schedule()` - Derived quantities computation\n",
    "- [ ] `sequential_forward_process()` - Sequential corruption (slow way)\n",
    "- [ ] `direct_forward_jump()` - Direct jump (fast way)\n",
    "- [ ] `generate_training_sample()` - Core training data generation\n",
    "\n",
    "**✅ Provided Starter Code:**\n",
    "- [ ] All visualization functions with complete plotting\n",
    "- [ ] Benchmarking and performance analysis\n",
    "- [ ] Statistical testing and validation\n",
    "- [ ] Numerical stability analysis\n",
    "- [ ] Comprehensive test suites\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "Submit your completed Jupyter notebook (.ipynb file) with:\n",
    "\n",
    "**✅ Mathematical Implementations:**\n",
    "- All TODO functions implemented with correct formulas\n",
    "- Clear comments explaining mathematical steps\n",
    "- Proper handling of tensor operations and device placement\n",
    "\n",
    "**✅ Validation Results:**\n",
    "- Screenshots or outputs showing successful validation tests\n",
    "- Performance benchmarks comparing sequential vs direct approaches\n",
    "- Analysis of different noise schedules\n",
    "\n",
    "**✅ Understanding Demonstration:**\n",
    "- Answers to discussion questions with your partner\n",
    "- Explanation of why reparameterization is essential\n",
    "- Analysis of computational speedups achieved\n",
    "\n",
    "**✅ Code Quality:**\n",
    "- Clean, well-commented implementations\n",
    "- Proper error handling for edge cases\n",
    "- Professional coding standards\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Key Formulas Implemented\n",
    "\n",
    "**Forward Jump Formula:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_t = sqrt(alpha_cumprod_t) * x0 + sqrt(1 - alpha_cumprod_t) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ebd66a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Reparameterization Trick:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b1896",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = mu + sigma * torch.randn_like(mu)  # Instead of sampling from Normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ceecab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Alpha Cumulative Product:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff54a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_cumprod = torch.cumprod(1 - betas, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a7a78",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Training Sample Generation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8c86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(1, T+1, (batch_size,))\n",
    "epsilon = torch.randn_like(x0)\n",
    "x_t = sqrt_alpha_cumprod[t] * x0 + sqrt_one_minus_alpha_cumprod[t] * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919e3a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
