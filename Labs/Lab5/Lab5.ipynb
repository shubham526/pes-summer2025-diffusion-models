{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b31eff",
   "metadata": {},
   "source": [
    "# Lab 5: Sampling from Trained Diffusion Models - From Noise to Data\n",
    "**Course: Diffusion Models: Theory and Applications**  \n",
    "**Duration: 90 minutes**  \n",
    "**Team Size: 2 students (same teams from Labs 1-4)**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, students will be able to:\n",
    "1. **Implement** the complete DDPM stochastic sampling algorithm\n",
    "2. **Build** the DDIM deterministic sampling method with step skipping\n",
    "3. **Create** the noise schedule reconstruction approach used in DDIM\n",
    "4. **Construct** controllable stochasticity with the η parameter\n",
    "5. **Analyze** speed vs quality trade-offs in practical sampling\n",
    "6. **Optimize** sampling algorithms for real-world deployment scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup and Sampling Framework\n",
    "\n",
    "### Part 1: Team Setup & Sampling Mission (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c39320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling implementation setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.distributions import Normal\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set seeds for reproducible sampling\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Sampling experiments on: {device}\")\n",
    "\n",
    "@dataclass\n",
    "class SamplingConfig:\n",
    "    \"\"\"Configuration for sampling experiments\"\"\"\n",
    "    T: int = 100\n",
    "    beta_start: float = 1e-4\n",
    "    beta_end: float = 2e-2\n",
    "    img_size: int = 32\n",
    "    channels: int = 3\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Compute noise schedule\n",
    "        self.betas = torch.linspace(self.beta_start, self.beta_end, self.T).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = torch.cat([torch.ones(1).to(device), self.alphas_cumprod[:-1]])\n",
    "        \n",
    "        # Variance schedule for sampling\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "# Create sampling configuration\n",
    "config = SamplingConfig(T=50, img_size=8, channels=1)  # Small for faster experimentation\n",
    "print(f\"Sampling configuration: T={config.T}, image_size={config.img_size}x{config.img_size}\")\n",
    "\n",
    "# Simple 2D visualization data for understanding\n",
    "def create_sampling_test_data(n_samples: int = 200) -> torch.Tensor:\n",
    "    \"\"\"Create 2D test data for sampling visualization\"\"\"\n",
    "    # Create a simple flower pattern\n",
    "    t = torch.linspace(0, 4*math.pi, n_samples)\n",
    "    r = 2 + 0.5 * torch.sin(3*t)\n",
    "    x = r * torch.cos(t) + 0.2 * torch.randn(n_samples)\n",
    "    y = r * torch.sin(t) + 0.2 * torch.randn(n_samples)\n",
    "    data = torch.stack([x, y], dim=1)\n",
    "    return data.to(device)\n",
    "\n",
    "test_data_2d = create_sampling_test_data(200)\n",
    "print(f\"Test data shape: {test_data_2d.shape}\")\n",
    "\n",
    "# Visualize test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_data_2d[:, 0].cpu(), test_data_2d[:, 1].cpu(), alpha=0.7, s=30, c='blue')\n",
    "plt.title('Test Data: Flower Pattern for Sampling Experiments')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Pre-trained model simulation (we'll simulate with a simple network)\n",
    "class SimpleNoisePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple noise prediction network for 2D data\n",
    "    Simulates a trained diffusion model\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # Simple MLP with timestep embedding\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(data_dim + 1, 64),  # +1 for timestep\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, data_dim)\n",
    "        )\n",
    "    \n",
    "    def timestep_embedding(self, t: torch.Tensor, max_period: int = 10000) -> torch.Tensor:\n",
    "        \"\"\"Simple sinusoidal timestep embedding\"\"\"\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        \n",
    "        # Simple normalized embedding for this lab\n",
    "        return (t.float() / config.T).unsqueeze(-1)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict noise given noisy input and timestep\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        if t.dim() == 0:\n",
    "            t = t.repeat(batch_size)\n",
    "        \n",
    "        t_embed = self.timestep_embedding(t)\n",
    "        if len(t_embed.shape) == 1:\n",
    "            t_embed = t_embed.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        # Concatenate input and timestep embedding\n",
    "        input_with_time = torch.cat([x, t_embed], dim=-1)\n",
    "        return self.net(input_with_time)\n",
    "\n",
    "# Create and \"train\" a simple model (we'll use a pre-initialized model for this lab)\n",
    "pretrained_model = SimpleNoisePredictor(data_dim=2).to(device)\n",
    "print(\"✓ Pre-trained model loaded (simulated)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f628aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Understanding the Trained Model (15 minutes)\n",
    "\n",
    "### Task 2.1: Explore the Pre-trained Noise Predictor\n",
    "\n",
    "**Your Mission**: Understand how the trained model predicts noise and verify its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModelAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze the behavior of our trained noise prediction model.\n",
    "    Understanding this is crucial for implementing sampling algorithms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SamplingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "    def analyze_noise_prediction_quality(self, x_clean: torch.Tensor, timesteps: List[int]):\n",
    "        \"\"\"\n",
    "        TODO: Implement noise prediction analysis\n",
    "        \n",
    "        For given clean data and timesteps:\n",
    "        1. Add known noise to create noisy samples\n",
    "        2. Use model to predict the noise\n",
    "        3. Compare predicted vs actual noise\n",
    "        4. Analyze how prediction quality varies with timestep\n",
    "        \n",
    "        Args:\n",
    "            x_clean: Clean data samples\n",
    "            timesteps: List of timesteps to test\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: For each timestep, add noise using forward process\n",
    "        # Step 2: Use model to predict the noise\n",
    "        # Step 3: Compute MSE between predicted and actual noise\n",
    "        # Step 4: Return analysis of prediction quality vs timestep\n",
    "        pass\n",
    "    \n",
    "    def visualize_noise_predictions(self, x_clean: torch.Tensor, timesteps: List[int] = [5, 15, 25, 35, 45]):\n",
    "        \"\"\"\n",
    "        Visualize how well the model predicts noise at different timesteps\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, len(timesteps), figsize=(15, 8))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, t in enumerate(timesteps):\n",
    "                try:\n",
    "                    # Add noise to clean sample\n",
    "                    alpha_cumprod_t = self.config.alphas_cumprod[t]\n",
    "                    noise = torch.randn_like(x_clean)\n",
    "                    x_noisy = torch.sqrt(alpha_cumprod_t) * x_clean + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                    \n",
    "                    # Predict noise\n",
    "                    predicted_noise = self.model(x_noisy, torch.tensor(t).to(device))\n",
    "                    \n",
    "                    # Plot true vs predicted noise\n",
    "                    axes[0, i].scatter(noise[:, 0].cpu(), noise[:, 1].cpu(), alpha=0.6, s=20, label='True noise')\n",
    "                    axes[0, i].scatter(predicted_noise[:, 0].cpu(), predicted_noise[:, 1].cpu(), \n",
    "                                     alpha=0.6, s=20, label='Predicted noise')\n",
    "                    axes[0, i].set_title(f't={t}')\n",
    "                    axes[0, i].legend(fontsize=8)\n",
    "                    axes[0, i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                    # Plot error magnitude\n",
    "                    error = torch.norm(noise - predicted_noise, dim=1)\n",
    "                    axes[1, i].hist(error.cpu().numpy(), bins=20, alpha=0.7, color='red')\n",
    "                    axes[1, i].set_title(f'Error: mean={error.mean():.3f}')\n",
    "                    axes[1, i].grid(True, alpha=0.3)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    axes[0, i].text(0.5, 0.5, 'Model\\nanalysis\\nneeded', ha='center', va='center')\n",
    "                    axes[1, i].text(0.5, 0.5, 'Implement\\nTODOs', ha='center', va='center')\n",
    "        \n",
    "        axes[0, 0].set_ylabel('Noise Predictions')\n",
    "        axes[1, 0].set_ylabel('Error Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def test_model_consistency(self, x_clean: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Test that the model gives consistent predictions\n",
    "        \"\"\"\n",
    "        print(\"=== Model Consistency Analysis ===\\n\")\n",
    "        \n",
    "        # Test 1: Same input should give same output\n",
    "        t = 20\n",
    "        with torch.no_grad():\n",
    "            alpha_cumprod_t = self.config.alphas_cumprod[t]\n",
    "            noise = torch.randn_like(x_clean)\n",
    "            x_noisy = torch.sqrt(alpha_cumprod_t) * x_clean + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "            \n",
    "            pred1 = self.model(x_noisy, torch.tensor(t).to(device))\n",
    "            pred2 = self.model(x_noisy, torch.tensor(t).to(device))\n",
    "            \n",
    "            consistency_error = torch.norm(pred1 - pred2).item()\n",
    "            print(f\"Deterministic consistency error: {consistency_error:.8f}\")\n",
    "            print(\"✓ Model is deterministic\" if consistency_error < 1e-6 else \"❌ Model is non-deterministic\")\n",
    "        \n",
    "        # Test 2: Different timesteps should give different predictions\n",
    "        timesteps = [10, 20, 30, 40]\n",
    "        predictions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t in timesteps:\n",
    "                alpha_cumprod_t = self.config.alphas_cumprod[t]\n",
    "                x_noisy = torch.sqrt(alpha_cumprod_t) * x_clean + torch.sqrt(1 - alpha_cumprod_t) * noise\n",
    "                pred = self.model(x_noisy, torch.tensor(t).to(device))\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        # Compare predictions across timesteps\n",
    "        for i in range(len(timesteps)-1):\n",
    "            diff = torch.norm(predictions[i] - predictions[i+1]).item()\n",
    "            print(f\"Difference t={timesteps[i]} vs t={timesteps[i+1]}: {diff:.3f}\")\n",
    "        \n",
    "        print(\"✓ Model is timestep-aware\\n\")\n",
    "\n",
    "# Test the trained model analyzer (uncomment after implementing TODOs)\n",
    "# analyzer = TrainedModelAnalyzer(pretrained_model, config)\n",
    "# analyzer.test_model_consistency(test_data_2d[:5])\n",
    "# analyzer.visualize_noise_predictions(test_data_2d[:5])\n",
    "\n",
    "# Test noise prediction quality\n",
    "# quality_results = analyzer.analyze_noise_prediction_quality(test_data_2d[:10], [5, 15, 25, 35, 45])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70857b0",
   "metadata": {},
   "source": [
    "### Task 2.2: Forward Process Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardProcessImpl:\n",
    "    \"\"\"\n",
    "    Implement the forward process for creating training data and understanding sampling.\n",
    "    This helps us understand what the reverse process needs to undo.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: SamplingConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def add_noise(self, x_start: torch.Tensor, t: torch.Tensor, noise: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the forward noising process\n",
    "        \n",
    "        Apply q(x_t | x_0) = N(√ᾱ_t x_0, (1-ᾱ_t) I) to add noise to clean data.\n",
    "        \n",
    "        Args:\n",
    "            x_start: Clean data\n",
    "            t: Timestep (can be tensor for batch processing)\n",
    "            noise: Optional pre-sampled noise (if None, sample new)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (noisy_data, noise_used)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Handle timestep indexing (ensure t is properly shaped)\n",
    "        # Step 2: Extract α̅_t from config.alphas_cumprod\n",
    "        # Step 3: Sample noise if not provided\n",
    "        # Step 4: Apply the forward process formula\n",
    "        # Step 5: Return both noisy data and noise used\n",
    "        pass\n",
    "    \n",
    "    def demonstrate_forward_trajectory(self, x_start: torch.Tensor, timesteps: List[int]):\n",
    "        \"\"\"\n",
    "        Show how data gets progressively noisier through the forward process\n",
    "        \"\"\"\n",
    "        print(\"=== Forward Process Trajectory ===\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, len(timesteps), figsize=(15, 8))\n",
    "        \n",
    "        noise = torch.randn_like(x_start)  # Use same noise for consistency\n",
    "        \n",
    "        for i, t in enumerate(timesteps):\n",
    "            try:\n",
    "                x_noisy, _ = self.add_noise(x_start, torch.tensor(t), noise)\n",
    "                \n",
    "                # Plot noisy data\n",
    "                axes[0, i].scatter(x_noisy[:, 0].cpu(), x_noisy[:, 1].cpu(), alpha=0.6, s=20, c='red')\n",
    "                axes[0, i].scatter(x_start[:, 0].cpu(), x_start[:, 1].cpu(), alpha=0.3, s=10, c='blue')\n",
    "                axes[0, i].set_title(f't={t}')\n",
    "                axes[0, i].grid(True, alpha=0.3)\n",
    "                axes[0, i].set_xlim(-6, 6)\n",
    "                axes[0, i].set_ylim(-6, 6)\n",
    "                \n",
    "                # Plot noise level\n",
    "                alpha_cumprod = self.config.alphas_cumprod[t]\n",
    "                signal_strength = torch.sqrt(alpha_cumprod).item()\n",
    "                noise_strength = torch.sqrt(1 - alpha_cumprod).item()\n",
    "                \n",
    "                axes[1, i].bar(['Signal', 'Noise'], [signal_strength, noise_strength], \n",
    "                              color=['blue', 'red'], alpha=0.7)\n",
    "                axes[1, i].set_title(f'Signal: {signal_strength:.2f}\\nNoise: {noise_strength:.2f}')\n",
    "                axes[1, i].set_ylim(0, 1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[0, i].text(0.5, 0.5, 'Implement\\nadd_noise\\nfirst', ha='center', va='center')\n",
    "                axes[1, i].text(0.5, 0.5, 'Implement\\nTODOs', ha='center', va='center')\n",
    "        \n",
    "        axes[0, 0].set_ylabel('Data Trajectory')\n",
    "        axes[1, 0].set_ylabel('Signal/Noise Ratio')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test forward process implementation (uncomment after implementing TODOs)\n",
    "# forward_process = ForwardProcessImpl(config)\n",
    "# forward_process.demonstrate_forward_trajectory(test_data_2d[:20], [0, 10, 20, 30, 40, 49])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b490bc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: DDPM Stochastic Sampling Implementation (25 minutes)\n",
    "\n",
    "### Task 3.1: Implement the Complete DDPM Sampler\n",
    "\n",
    "**Your Mission**: Build the original DDPM sampling algorithm with proper stochastic sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMSampler:\n",
    "    \"\"\"\n",
    "    Implement the complete DDPM sampling algorithm.\n",
    "    This is the original stochastic approach to diffusion sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SamplingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "    def predict_noise(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Use the trained model to predict noise\"\"\"\n",
    "        return self.model(x_t, t)\n",
    "    \n",
    "    def compute_posterior_mean(self, x_t: torch.Tensor, t: int, predicted_noise: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement posterior mean computation for DDPM\n",
    "        \n",
    "        Compute μ_θ(x_t, t) = (1/√α_t) * (x_t - (1-α_t)/√(1-ᾱ_t) * ε_θ(x_t, t))\n",
    "        \n",
    "        This is the mean of the reverse process distribution.\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            t: Current timestep\n",
    "            predicted_noise: Noise predicted by the model\n",
    "            \n",
    "        Returns:\n",
    "            Posterior mean μ_θ(x_t, t)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract α_t and ᾱ_t from noise schedule\n",
    "        # Step 2: Compute coefficient for x_t term: 1/√α_t\n",
    "        # Step 3: Compute coefficient for noise term: (1-α_t)/√(1-ᾱ_t)\n",
    "        # Step 4: Apply the posterior mean formula\n",
    "        # Step 5: Return computed mean\n",
    "        pass\n",
    "    \n",
    "    def compute_posterior_variance(self, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement posterior variance computation\n",
    "        \n",
    "        Compute σ̃²_t = β_t * (1-ᾱ_{t-1})/(1-ᾱ_t)\n",
    "        \n",
    "        This is the fixed variance of the reverse process.\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Posterior variance σ̃²_t\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Handle edge case for t=0\n",
    "        # Step 2: Extract β_t, ᾱ_t, and ᾱ_{t-1} from config\n",
    "        # Step 3: Apply the posterior variance formula\n",
    "        # Step 4: Return computed variance\n",
    "        pass\n",
    "    \n",
    "    def ddpm_step(self, x_t: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement single DDPM sampling step\n",
    "        \n",
    "        Execute one step of the DDPM reverse process:\n",
    "        1. Predict noise using the model\n",
    "        2. Compute posterior mean\n",
    "        3. Compute posterior variance  \n",
    "        4. Sample from the posterior distribution\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current state at timestep t\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            x_{t-1}: Next state in the reverse process\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Predict noise using the model\n",
    "        # Step 2: Compute posterior mean\n",
    "        # Step 3: Compute posterior variance\n",
    "        # Step 4: Sample from N(mean, variance) if t > 0, else return mean\n",
    "        # Step 5: Return the next state\n",
    "        pass\n",
    "    \n",
    "    def sample(self, shape: Tuple[int, ...], return_trajectory: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement complete DDPM sampling\n",
    "        \n",
    "        Generate samples by running the reverse process from pure noise to data.\n",
    "        \n",
    "        Args:\n",
    "            shape: Shape of samples to generate\n",
    "            return_trajectory: Whether to return intermediate states\n",
    "            \n",
    "        Returns:\n",
    "            Generated samples (and trajectory if requested)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Initialize x_T from pure noise N(0, I)\n",
    "        # Step 2: For t = T-1, T-2, ..., 0: apply ddpm_step\n",
    "        # Step 3: Optionally store trajectory\n",
    "        # Step 4: Return final samples (and trajectory if requested)\n",
    "        pass\n",
    "    \n",
    "    def sample_with_progress(self, shape: Tuple[int, ...], show_every: int = 10) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample with visualization of the denoising process\n",
    "        \"\"\"\n",
    "        print(\"=== DDPM Sampling with Progress ===\\n\")\n",
    "        \n",
    "        # Initialize\n",
    "        x = torch.randn(shape).to(device)\n",
    "        trajectory = [x.clone()]\n",
    "        \n",
    "        # Reverse process\n",
    "        for t in reversed(range(self.config.T)):\n",
    "            try:\n",
    "                x = self.ddpm_step(x, t)\n",
    "                if t % show_every == 0 or t == 0:\n",
    "                    trajectory.append(x.clone())\n",
    "                    print(f\"Step {self.config.T - t}: t={t}, sample mean={x.mean().item():.3f}, std={x.std().item():.3f}\")\n",
    "            except:\n",
    "                print(f\"Implement ddpm_step to see progress at t={t}\")\n",
    "                break\n",
    "        \n",
    "        # Visualize trajectory\n",
    "        if len(trajectory) > 1:\n",
    "            self.visualize_sampling_trajectory(trajectory, \"DDPM Sampling Progress\")\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def visualize_sampling_trajectory(self, trajectory: List[torch.Tensor], title: str):\n",
    "        \"\"\"Visualize the sampling trajectory\"\"\"\n",
    "        n_steps = len(trajectory)\n",
    "        cols = min(6, n_steps)\n",
    "        rows = (n_steps + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 3*rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, x in enumerate(trajectory):\n",
    "            row, col = i // cols, i % cols\n",
    "            if i < len(trajectory):\n",
    "                axes[row, col].scatter(x[:, 0].cpu(), x[:, 1].cpu(), alpha=0.6, s=20)\n",
    "                step_num = i * 10 if i < len(trajectory)-1 else \"Final\"\n",
    "                axes[row, col].set_title(f'Step {step_num}')\n",
    "                axes[row, col].grid(True, alpha=0.3)\n",
    "                axes[row, col].set_xlim(-6, 6)\n",
    "                axes[row, col].set_ylim(-6, 6)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(trajectory), rows * cols):\n",
    "            row, col = i // cols, i % cols\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test DDPM sampler (uncomment after implementing TODOs)\n",
    "# ddpm_sampler = DDPMSampler(pretrained_model, config)\n",
    "\n",
    "# # Test individual components\n",
    "# test_x = torch.randn(5, 2).to(device)\n",
    "# test_t = 20\n",
    "# predicted_noise = ddpm_sampler.predict_noise(test_x, torch.tensor(test_t))\n",
    "# print(f\"Predicted noise shape: {predicted_noise.shape}\")\n",
    "\n",
    "# # Test posterior computations\n",
    "# posterior_mean = ddpm_sampler.compute_posterior_mean(test_x, test_t, predicted_noise)\n",
    "# posterior_var = ddpm_sampler.compute_posterior_variance(test_t)\n",
    "# print(f\"Posterior mean shape: {posterior_mean.shape if posterior_mean is not None else 'Not implemented'}\")\n",
    "# print(f\"Posterior variance: {posterior_var.item() if posterior_var is not None else 'Not implemented'}\")\n",
    "\n",
    "# # Test complete sampling\n",
    "# samples = ddmp_sampler.sample_with_progress((20, 2), show_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c4c31",
   "metadata": {},
   "source": [
    "### Task 3.2: DDPM Analysis and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze DDPM sampling behavior and validate implementation correctness.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sampler: DDPMSampler):\n",
    "        self.sampler = sampler\n",
    "        \n",
    "    def validate_sampling_implementation(self):\n",
    "        \"\"\"\n",
    "        Validate that DDPM implementation is working correctly\n",
    "        \"\"\"\n",
    "        print(\"=== DDPM Implementation Validation ===\\n\")\n",
    "        \n",
    "        # Test 1: Check that sampling produces reasonable outputs\n",
    "        try:\n",
    "            samples = self.sampler.sample((10, 2))\n",
    "            if samples is not None:\n",
    "                print(f\"✓ Sample generation successful\")\n",
    "                print(f\"  Sample shape: {samples.shape}\")\n",
    "                print(f\"  Sample mean: {samples.mean().item():.3f}\")\n",
    "                print(f\"  Sample std: {samples.std().item():.3f}\")\n",
    "            else:\n",
    "                print(\"❌ Implement DDPM sample() method\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in sampling: {e}\")\n",
    "        \n",
    "        # Test 2: Check posterior variance behavior\n",
    "        variances = []\n",
    "        for t in [0, 10, 20, 30, 40]:\n",
    "            try:\n",
    "                var = self.sampler.compute_posterior_variance(t)\n",
    "                if var is not None:\n",
    "                    variances.append((t, var.item()))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if variances:\n",
    "            print(f\"\\n✓ Posterior variance computed successfully\")\n",
    "            for t, var in variances:\n",
    "                print(f\"  t={t}: σ̃² = {var:.6f}\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Implement compute_posterior_variance method\")\n",
    "        \n",
    "        # Test 3: Check that final step is deterministic\n",
    "        try:\n",
    "            test_x = torch.randn(3, 2).to(device)\n",
    "            step1 = self.sampler.ddpm_step(test_x, 0)  # Final step\n",
    "            step2 = self.sampler.ddmp_step(test_x, 0)  # Should be identical\n",
    "            \n",
    "            if step1 is not None and step2 is not None:\n",
    "                diff = torch.norm(step1 - step2).item()\n",
    "                print(f\"\\n✓ Final step determinism check: diff = {diff:.8f}\")\n",
    "                print(\"✓ Final step is deterministic\" if diff < 1e-6 else \"❌ Final step should be deterministic\")\n",
    "        except:\n",
    "            print(f\"\\n❌ Implement ddpm_step method\")\n",
    "    \n",
    "    def analyze_stochasticity(self, n_runs: int = 5):\n",
    "        \"\"\"\n",
    "        Analyze the stochastic behavior of DDPM sampling\n",
    "        \"\"\"\n",
    "        print(\"=== DDPM Stochasticity Analysis ===\\n\")\n",
    "        \n",
    "        samples_list = []\n",
    "        \n",
    "        for run in range(n_runs):\n",
    "            try:\n",
    "                samples = self.sampler.sample((20, 2))\n",
    "                if samples is not None:\n",
    "                    samples_list.append(samples)\n",
    "                else:\n",
    "                    print(\"❌ Implement sample() method first\")\n",
    "                    return\n",
    "            except:\n",
    "                print(\"❌ Error in sampling - implement missing methods\")\n",
    "                return\n",
    "        \n",
    "        # Analyze diversity across runs\n",
    "        print(f\"Generated {n_runs} independent sample sets\")\n",
    "        \n",
    "        # Compute pairwise differences between runs\n",
    "        for i in range(n_runs-1):\n",
    "            diff = torch.norm(samples_list[i] - samples_list[i+1]).item()\n",
    "            print(f\"Difference run {i+1} vs {i+2}: {diff:.3f}\")\n",
    "        \n",
    "        # Visualize different runs\n",
    "        fig, axes = plt.subplots(1, min(n_runs, 5), figsize=(15, 3))\n",
    "        if n_runs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, samples in enumerate(samples_list[:5]):\n",
    "            axes[i].scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.6, s=20)\n",
    "            axes[i].set_title(f'Run {i+1}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].set_xlim(-6, 6)\n",
    "            axes[i].set_ylim(-6, 6)\n",
    "        \n",
    "        plt.suptitle('DDPM Sample Diversity Across Runs')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ DDPM produces diverse samples due to stochastic sampling\")\n",
    "\n",
    "# Test DDPM analyzer (uncomment after implementing DDPM methods)\n",
    "# ddpm_analyzer = DDPMAnalyzer(ddpm_sampler)\n",
    "# ddpm_analyzer.validate_sampling_implementation()\n",
    "# ddpm_analyzer.analyze_stochasticity(n_runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8818611",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: DDIM Deterministic Sampling Implementation (25 minutes)\n",
    "\n",
    "### Task 4.1: Implement the DDIM Sampler\n",
    "\n",
    "**Your Mission**: Build the DDIM sampling algorithm with step skipping and controllable stochasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSampler:\n",
    "    \"\"\"\n",
    "    Implement the DDIM sampling algorithm.\n",
    "    The key innovation: deterministic sampling with step skipping capability.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SamplingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "    def predict_x0_from_eps(self, x_t: torch.Tensor, eps: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement x0 prediction from noise\n",
    "        \n",
    "        Given x_t and predicted noise, recover the estimated clean image:\n",
    "        x̂_0 = (x_t - √(1-ᾱ_t) * ε) / √ᾱ_t\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            eps: Predicted noise\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Predicted clean data x̂_0\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract ᾱ_t from config\n",
    "        # Step 2: Apply the inversion formula\n",
    "        # Step 3: Return predicted x_0\n",
    "        pass\n",
    "    \n",
    "    def predict_eps_from_x0(self, x_t: torch.Tensor, x0: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement noise prediction from x0\n",
    "        \n",
    "        Given x_t and x_0, recover the noise:\n",
    "        ε = (x_t - √ᾱ_t * x_0) / √(1-ᾱ_t)\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            x0: Clean data estimate\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Predicted noise ε\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract ᾱ_t from config\n",
    "        # Step 2: Apply the noise extraction formula\n",
    "        # Step 3: Return predicted noise\n",
    "        pass\n",
    "    \n",
    "    def ddim_step(self, x_t: torch.Tensor, t: int, s: int, eta: float = 0.0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement single DDIM sampling step\n",
    "        \n",
    "        The core DDIM update: deterministic reconstruction of the trajectory.\n",
    "        \n",
    "        When eta=0 (deterministic):\n",
    "        x_s = √ᾱ_s * x̂_0 + √(1-ᾱ_s) * ε̂\n",
    "        \n",
    "        When eta>0 (stochastic):\n",
    "        Add controlled randomness with variance σ_t^2 = eta^2 * β̃_t\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current state at timestep t\n",
    "            t: Current timestep  \n",
    "            s: Target timestep (s < t)\n",
    "            eta: Stochasticity parameter (0=deterministic, 1=like DDPM)\n",
    "            \n",
    "        Returns:\n",
    "            x_s: Next state in the reverse process\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Predict noise using the model\n",
    "        # Step 2: Predict x_0 from the noise\n",
    "        # Step 3: Compute the deterministic part: √ᾱ_s * x̂_0 + √(1-ᾱ_s) * ε̂\n",
    "        # Step 4: Add stochastic part if eta > 0\n",
    "        # Step 5: Return the next state\n",
    "        pass\n",
    "    \n",
    "    def sample(self, shape: Tuple[int, ...], timesteps: Optional[List[int]] = None, \n",
    "               eta: float = 0.0, return_trajectory: bool = False) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement complete DDIM sampling with step skipping\n",
    "        \n",
    "        Generate samples using DDIM with arbitrary timestep scheduling.\n",
    "        \n",
    "        Args:\n",
    "            shape: Shape of samples to generate\n",
    "            timesteps: Custom timestep schedule (if None, use all timesteps)\n",
    "            eta: Stochasticity parameter\n",
    "            return_trajectory: Whether to return intermediate states\n",
    "            \n",
    "        Returns:\n",
    "            Generated samples (and trajectory if requested)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Set up timestep schedule (default to uniform if not provided)\n",
    "        # Step 2: Initialize x_T from pure noise\n",
    "        # Step 3: For each consecutive pair (t, s) in timesteps: apply ddim_step\n",
    "        # Step 4: Optionally store trajectory\n",
    "        # Step 5: Return final samples (and trajectory if requested)\n",
    "        pass\n",
    "    \n",
    "    def create_timestep_schedule(self, num_steps: int, schedule_type: str = \"uniform\") -> List[int]:\n",
    "        \"\"\"\n",
    "        Create different timestep schedules for step skipping\n",
    "        \n",
    "        Args:\n",
    "            num_steps: Number of sampling steps to use\n",
    "            schedule_type: Type of schedule (\"uniform\", \"quadratic\", \"custom\")\n",
    "            \n",
    "        Returns:\n",
    "            List of timesteps to use for sampling\n",
    "        \"\"\"\n",
    "        if schedule_type == \"uniform\":\n",
    "            # Uniform spacing\n",
    "            step_size = self.config.T // num_steps\n",
    "            timesteps = list(range(self.config.T - 1, -1, -step_size))\n",
    "            timesteps.append(0)  # Ensure we end at 0\n",
    "            return timesteps[:num_steps + 1]\n",
    "        \n",
    "        elif schedule_type == \"quadratic\":\n",
    "            # More steps at high noise levels\n",
    "            timesteps = []\n",
    "            for i in range(num_steps):\n",
    "                # Quadratic spacing\n",
    "                t = int(self.config.T * (1 - (i / num_steps) ** 2))\n",
    "                timesteps.append(max(0, t))\n",
    "            return sorted(set(timesteps), reverse=True)\n",
    "        \n",
    "        else:\n",
    "            # Default to uniform\n",
    "            return self.create_timestep_schedule(num_steps, \"uniform\")\n",
    "    \n",
    "    def sample_with_schedule_analysis(self, shape: Tuple[int, ...], step_counts: List[int], eta: float = 0.0):\n",
    "        \"\"\"\n",
    "        Analyze sampling with different step counts\n",
    "        \"\"\"\n",
    "        print(\"=== DDIM Schedule Analysis ===\\n\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for num_steps in step_counts:\n",
    "            print(f\"Testing with {num_steps} steps...\")\n",
    "            timesteps = self.create_timestep_schedule(num_steps)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                samples = self.sample(shape, timesteps, eta)\n",
    "                sampling_time = time.time() - start_time\n",
    "                \n",
    "                if samples is not None:\n",
    "                    results[num_steps] = {\n",
    "                        'samples': samples,\n",
    "                        'time': sampling_time,\n",
    "                        'timesteps': timesteps\n",
    "                    }\n",
    "                    print(f\"  ✓ {num_steps} steps completed in {sampling_time:.3f}s\")\n",
    "                else:\n",
    "                    print(f\"  ❌ Implement DDIM sample() method\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error with {num_steps} steps: {e}\")\n",
    "                break\n",
    "        \n",
    "        if results:\n",
    "            self.visualize_schedule_comparison(results)\n",
    "            self.analyze_speed_quality_tradeoff(results)\n",
    "    \n",
    "    def visualize_schedule_comparison(self, results: Dict):\n",
    "        \"\"\"Visualize samples from different step schedules\"\"\"\n",
    "        n_schedules = len(results)\n",
    "        fig, axes = plt.subplots(1, n_schedules, figsize=(4*n_schedules, 4))\n",
    "        \n",
    "        if n_schedules == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (num_steps, result) in enumerate(results.items()):\n",
    "            samples = result['samples']\n",
    "            axes[i].scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.6, s=20)\n",
    "            axes[i].set_title(f'{num_steps} steps\\n{result[\"time\"]:.2f}s')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].set_xlim(-6, 6)\n",
    "            axes[i].set_ylim(-6, 6)\n",
    "        \n",
    "        plt.suptitle('DDIM Sampling: Different Step Counts')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_speed_quality_tradeoff(self, results: Dict):\n",
    "        \"\"\"Analyze the speed vs quality tradeoff\"\"\"\n",
    "        step_counts = list(results.keys())\n",
    "        times = [results[k]['time'] for k in step_counts]\n",
    "        \n",
    "        # Simple quality metric: how close to a circle (for our flower pattern)\n",
    "        qualities = []\n",
    "        for num_steps in step_counts:\n",
    "            samples = results[num_steps]['samples']\n",
    "            # Measure distance from origin (crude quality metric)\n",
    "            distances = torch.norm(samples, dim=1)\n",
    "            quality = -distances.std().item()  # Lower std = more circular = higher quality\n",
    "            qualities.append(quality)\n",
    "        \n",
    "        # Plot tradeoff\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Speed vs steps\n",
    "        ax1.plot(step_counts, times, 'bo-', linewidth=2, markersize=8)\n",
    "        ax1.set_xlabel('Number of Steps')\n",
    "        ax1.set_ylabel('Sampling Time (s)')\n",
    "        ax1.set_title('Sampling Speed vs Steps')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Quality vs steps  \n",
    "        ax2.plot(step_counts, qualities, 'ro-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Number of Steps')\n",
    "        ax2.set_ylabel('Quality Metric')\n",
    "        ax2.set_title('Sample Quality vs Steps')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Speed improvement from {max(step_counts)} to {min(step_counts)} steps: \"\n",
    "              f\"{max(times)/min(times):.1f}x faster\")\n",
    "\n",
    "# Test DDIM sampler (uncomment after implementing TODOs)\n",
    "# ddim_sampler = DDIMSampler(pretrained_model, config)\n",
    "\n",
    "# # Test timestep scheduling\n",
    "# uniform_schedule = ddim_sampler.create_timestep_schedule(20, \"uniform\")\n",
    "# quadratic_schedule = ddim_sampler.create_timestep_schedule(20, \"quadratic\") \n",
    "# print(f\"Uniform schedule (20 steps): {uniform_schedule[:10]}...\")\n",
    "# print(f\"Quadratic schedule (20 steps): {quadratic_schedule[:10]}...\")\n",
    "\n",
    "# # Test schedule analysis\n",
    "# ddim_sampler.sample_with_schedule_analysis((30, 2), step_counts=[50, 25, 10], eta=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ad158",
   "metadata": {},
   "source": [
    "### Task 4.2: Controllable Stochasticity with η Parameter\n",
    "\n",
    "**Your Mission**: Implement the η parameter to control the deterministic vs stochastic behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticityController:\n",
    "    \"\"\"\n",
    "    Implement and analyze the η parameter that controls DDIM stochasticity.\n",
    "    This is a key innovation that allows trading off speed vs diversity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ddim_sampler: DDIMSampler):\n",
    "        self.ddim_sampler = ddim_sampler\n",
    "        \n",
    "    def compute_stochastic_variance(self, t: int, s: int, eta: float) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement stochastic variance computation for DDIM\n",
    "        \n",
    "        Compute σ_t^2 = η^2 * β̃_{t→s} where β̃_{t→s} is the \"effective\" beta\n",
    "        for the jump from t to s.\n",
    "        \n",
    "        Formula: β̃_{t→s} = (1-ᾱ_s)/(1-ᾱ_t) * (1 - ᾱ_t/ᾱ_s)\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            s: Target timestep  \n",
    "            eta: Stochasticity parameter\n",
    "            \n",
    "        Returns:\n",
    "            Variance for stochastic sampling\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract ᾱ_t and ᾱ_s from config\n",
    "        # Step 2: Compute effective beta β̃_{t→s}\n",
    "        # Step 3: Apply eta scaling: σ^2 = η^2 * β̃\n",
    "        # Step 4: Return computed variance\n",
    "        pass\n",
    "    \n",
    "    def analyze_eta_effects(self, shape: Tuple[int, ...], eta_values: List[float], num_steps: int = 20):\n",
    "        \"\"\"\n",
    "        Analyze how different η values affect sampling behavior\n",
    "        \"\"\"\n",
    "        print(\"=== η Parameter Analysis ===\\n\")\n",
    "        \n",
    "        timesteps = self.ddim_sampler.create_timestep_schedule(num_steps)\n",
    "        results = {}\n",
    "        \n",
    "        for eta in eta_values:\n",
    "            print(f\"Testing η = {eta}\")\n",
    "            \n",
    "            try:\n",
    "                # Generate multiple samples to assess diversity\n",
    "                samples_list = []\n",
    "                for run in range(3):\n",
    "                    samples = self.ddim_sampler.sample(shape, timesteps, eta)\n",
    "                    if samples is not None:\n",
    "                        samples_list.append(samples)\n",
    "                    else:\n",
    "                        print(f\"  ❌ Implement DDIM sample() method\")\n",
    "                        return\n",
    "                \n",
    "                results[eta] = samples_list\n",
    "                print(f\"  ✓ Generated {len(samples_list)} sample sets\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error with η={eta}: {e}\")\n",
    "        \n",
    "        if results:\n",
    "            self.visualize_eta_comparison(results)\n",
    "            self.analyze_diversity_vs_eta(results)\n",
    "    \n",
    "    def visualize_eta_comparison(self, results: Dict):\n",
    "        \"\"\"Visualize how η affects sample appearance\"\"\"\n",
    "        eta_values = list(results.keys())\n",
    "        n_etas = len(eta_values)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_etas, 3, figsize=(12, 3*n_etas))\n",
    "        if n_etas == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, eta in enumerate(eta_values):\n",
    "            samples_list = results[eta]\n",
    "            \n",
    "            for j, samples in enumerate(samples_list):\n",
    "                axes[i, j].scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.6, s=20)\n",
    "                if j == 0:\n",
    "                    axes[i, j].set_ylabel(f'η = {eta}')\n",
    "                axes[i, j].set_title(f'Run {j+1}')\n",
    "                axes[i, j].grid(True, alpha=0.3)\n",
    "                axes[i, j].set_xlim(-6, 6)\n",
    "                axes[i, j].set_ylim(-6, 6)\n",
    "        \n",
    "        plt.suptitle('DDIM Stochasticity: Effect of η Parameter')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_diversity_vs_eta(self, results: Dict):\n",
    "        \"\"\"Quantify sample diversity for different η values\"\"\"\n",
    "        eta_values = list(results.keys())\n",
    "        diversities = []\n",
    "        \n",
    "        for eta in eta_values:\n",
    "            samples_list = results[eta]\n",
    "            \n",
    "            # Compute pairwise differences between runs\n",
    "            total_diff = 0\n",
    "            num_pairs = 0\n",
    "            \n",
    "            for i in range(len(samples_list)):\n",
    "                for j in range(i+1, len(samples_list)):\n",
    "                    diff = torch.norm(samples_list[i] - samples_list[j]).item()\n",
    "                    total_diff += diff\n",
    "                    num_pairs += 1\n",
    "            \n",
    "            avg_diversity = total_diff / num_pairs if num_pairs > 0 else 0\n",
    "            diversities.append(avg_diversity)\n",
    "            print(f\"η = {eta}: Average diversity = {avg_diversity:.3f}\")\n",
    "        \n",
    "        # Plot diversity vs eta\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(eta_values, diversities, 'go-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('η (Stochasticity Parameter)')\n",
    "        plt.ylabel('Sample Diversity')\n",
    "        plt.title('Sample Diversity vs η Parameter')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKey insights:\")\n",
    "        print(\"• η = 0: Deterministic sampling (identical samples)\")\n",
    "        print(\"• η > 0: Stochastic sampling (diverse samples)\")\n",
    "        print(\"• η = 1: Similar to DDPM stochasticity\")\n",
    "\n",
    "# Test stochasticity controller (uncomment after implementing TODOs)\n",
    "# stochasticity_controller = StochasticityController(ddim_sampler)\n",
    "# stochasticity_controller.analyze_eta_effects((25, 2), eta_values=[0.0, 0.1, 0.5, 1.0], num_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648ad675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: DDPM vs DDIM Comparison (15 minutes)\n",
    "\n",
    "### Task 5.1: Comprehensive Comparison\n",
    "\n",
    "**Your Mission**: Compare DDPM and DDIM across multiple dimensions: speed, quality, and diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e7733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplerComparison:\n",
    "    \"\"\"\n",
    "    Comprehensive comparison between DDPM and DDIM sampling approaches.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ddpm_sampler: DDPMSampler, ddim_sampler: DDIMSampler):\n",
    "        self.ddpm_sampler = ddpm_sampler\n",
    "        self.ddim_sampler = ddim_sampler\n",
    "        \n",
    "    def speed_comparison(self, shape: Tuple[int, ...], ddim_steps: List[int]):\n",
    "        \"\"\"\n",
    "        Compare sampling speeds between DDPM and DDIM\n",
    "        \"\"\"\n",
    "        print(\"=== Speed Comparison: DDPM vs DDIM ===\\n\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # Test DDPM (full steps)\n",
    "        print(\"Testing DDPM (full schedule)...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            ddpm_samples = self.ddpm_sampler.sample(shape)\n",
    "            ddpm_time = time.time() - start_time\n",
    "            if ddmp_samples is not None:\n",
    "                results['DDPM'] = {'samples': ddpm_samples, 'time': ddpm_time, 'steps': self.ddpm_sampler.config.T}\n",
    "                print(f\"  ✓ DDPM: {ddpm_time:.3f}s with {self.ddpm_sampler.config.T} steps\")\n",
    "            else:\n",
    "                print(\"  ❌ Implement DDPM sample() method\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ DDPM error: {e}\")\n",
    "        \n",
    "        # Test DDIM with different step counts\n",
    "        for num_steps in ddim_steps:\n",
    "            print(f\"Testing DDIM ({num_steps} steps)...\")\n",
    "            timesteps = self.ddim_sampler.create_timestep_schedule(num_steps)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                ddim_samples = self.ddim_sampler.sample(shape, timesteps, eta=0.0)\n",
    "                ddim_time = time.time() - start_time\n",
    "                if ddim_samples is not None:\n",
    "                    results[f'DDIM_{num_steps}'] = {'samples': ddim_samples, 'time': ddim_time, 'steps': num_steps}\n",
    "                    speedup = ddpm_time / ddim_time if 'DDPM' in results else 1.0\n",
    "                    print(f\"  ✓ DDIM: {ddim_time:.3f}s with {num_steps} steps (Speedup: {speedup:.1f}x)\")\n",
    "                else:\n",
    "                    print(\"  ❌ Implement DDIM sample() method\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ DDIM error: {e}\")\n",
    "        \n",
    "        if results:\n",
    "            self.visualize_speed_comparison(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def quality_comparison(self, shape: Tuple[int, ...], reference_data: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compare sample quality between different methods\n",
    "        \"\"\"\n",
    "        print(\"=== Quality Comparison ===\\n\")\n",
    "        \n",
    "        methods = {\n",
    "            'DDPM': lambda: self.ddpm_sampler.sample(shape),\n",
    "            'DDIM_50': lambda: self.ddim_sampler.sample(shape, \n",
    "                                 self.ddim_sampler.create_timestep_schedule(50), eta=0.0),\n",
    "            'DDIM_20': lambda: self.ddim_sampler.sample(shape,\n",
    "                                 self.ddim_sampler.create_timestep_schedule(20), eta=0.0),\n",
    "            'DDIM_10': lambda: self.ddim_sampler.sample(shape,\n",
    "                                 self.ddim_sampler.create_timestep_schedule(10), eta=0.0)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method_name, sample_fn in methods.items():\n",
    "            try:\n",
    "                samples = sample_fn()\n",
    "                if samples is not None:\n",
    "                    # Simple quality metric: how close to reference distribution\n",
    "                    quality_score = self.compute_simple_quality_metric(samples, reference_data)\n",
    "                    results[method_name] = {'samples': samples, 'quality': quality_score}\n",
    "                    print(f\"{method_name}: Quality score = {quality_score:.3f}\")\n",
    "                else:\n",
    "                    print(f\"{method_name}: ❌ Implementation needed\")\n",
    "            except Exception as e:\n",
    "                print(f\"{method_name}: ❌ Error: {e}\")\n",
    "        \n",
    "        if results:\n",
    "            self.visualize_quality_comparison(results, reference_data)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compute_simple_quality_metric(self, samples: torch.Tensor, reference: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Simple quality metric based on distribution moments\n",
    "        \"\"\"\n",
    "        # Compare means and standard deviations\n",
    "        sample_mean = samples.mean(dim=0)\n",
    "        sample_std = samples.std(dim=0)\n",
    "        ref_mean = reference.mean(dim=0)\n",
    "        ref_std = reference.std(dim=0)\n",
    "        \n",
    "        mean_diff = torch.norm(sample_mean - ref_mean).item()\n",
    "        std_diff = torch.norm(sample_std - ref_std).item()\n",
    "        \n",
    "        # Lower is better (closer to reference)\n",
    "        quality_score = -(mean_diff + std_diff)\n",
    "        return quality_score\n",
    "    \n",
    "    def diversity_analysis(self, shape: Tuple[int, ...], n_runs: int = 5):\n",
    "        \"\"\"\n",
    "        Analyze sample diversity for DDPM vs DDIM\n",
    "        \"\"\"\n",
    "        print(\"=== Diversity Analysis ===\\n\")\n",
    "        \n",
    "        methods = {\n",
    "            'DDPM': lambda: self.ddpm_sampler.sample(shape),\n",
    "            'DDIM_det': lambda: self.ddim_sampler.sample(shape, \n",
    "                                  self.ddim_sampler.create_timestep_schedule(20), eta=0.0),\n",
    "            'DDIM_stoch': lambda: self.ddim_sampler.sample(shape,\n",
    "                                    self.ddim_sampler.create_timestep_schedule(20), eta=0.5)\n",
    "        }\n",
    "        \n",
    "        for method_name, sample_fn in methods.items():\n",
    "            print(f\"Testing {method_name} diversity...\")\n",
    "            \n",
    "            samples_list = []\n",
    "            for run in range(n_runs):\n",
    "                try:\n",
    "                    samples = sample_fn()\n",
    "                    if samples is not None:\n",
    "                        samples_list.append(samples)\n",
    "                    else:\n",
    "                        print(f\"  ❌ Implementation needed\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"  ❌ Error: {e}\")\n",
    "                    break\n",
    "            \n",
    "            if len(samples_list) == n_runs:\n",
    "                # Compute diversity\n",
    "                total_diff = 0\n",
    "                num_pairs = 0\n",
    "                for i in range(n_runs):\n",
    "                    for j in range(i+1, n_runs):\n",
    "                        diff = torch.norm(samples_list[i] - samples_list[j]).item()\n",
    "                        total_diff += diff\n",
    "                        num_pairs += 1\n",
    "                \n",
    "                avg_diversity = total_diff / num_pairs\n",
    "                print(f\"  Average diversity: {avg_diversity:.3f}\")\n",
    "                \n",
    "                # Visualize first few runs\n",
    "                self.visualize_diversity_runs(samples_list[:3], method_name)\n",
    "    \n",
    "    def visualize_speed_comparison(self, results: Dict):\n",
    "        \"\"\"Visualize speed comparison results\"\"\"\n",
    "        methods = list(results.keys())\n",
    "        times = [results[m]['time'] for m in methods]\n",
    "        steps = [results[m]['steps'] for m in methods]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Time comparison\n",
    "        bars1 = ax1.bar(methods, times, color=['blue', 'red', 'orange', 'green'][:len(methods)])\n",
    "        ax1.set_ylabel('Sampling Time (s)')\n",
    "        ax1.set_title('Sampling Speed Comparison')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add time labels on bars\n",
    "        for bar, time in zip(bars1, times):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{time:.3f}s', ha='center', va='bottom')\n",
    "        \n",
    "        # Steps comparison\n",
    "        bars2 = ax2.bar(methods, steps, color=['blue', 'red', 'orange', 'green'][:len(methods)])\n",
    "        ax2.set_ylabel('Number of Steps')\n",
    "        ax2.set_title('Sampling Steps Comparison')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add steps labels on bars\n",
    "        for bar, step in zip(bars2, steps):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                    f'{step}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_quality_comparison(self, results: Dict, reference_data: torch.Tensor):\n",
    "        \"\"\"Visualize quality comparison results\"\"\"\n",
    "        n_methods = len(results)\n",
    "        fig, axes = plt.subplots(1, n_methods + 1, figsize=(4*(n_methods+1), 4))\n",
    "        \n",
    "        # Plot reference data\n",
    "        axes[0].scatter(reference_data[:, 0].cpu(), reference_data[:, 1].cpu(), \n",
    "                       alpha=0.6, s=20, color='black')\n",
    "        axes[0].set_title('Reference Data')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_xlim(-6, 6)\n",
    "        axes[0].set_ylim(-6, 6)\n",
    "        \n",
    "        # Plot generated samples\n",
    "        for i, (method, result) in enumerate(results.items()):\n",
    "            samples = result['samples']\n",
    "            quality = result['quality']\n",
    "            \n",
    "            axes[i+1].scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.6, s=20)\n",
    "            axes[i+1].set_title(f'{method}\\nQuality: {quality:.3f}')\n",
    "            axes[i+1].grid(True, alpha=0.3)\n",
    "            axes[i+1].set_xlim(-6, 6)\n",
    "            axes[i+1].set_ylim(-6, 6)\n",
    "        \n",
    "        plt.suptitle('Sample Quality Comparison')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_diversity_runs(self, samples_list: List[torch.Tensor], method_name: str):\n",
    "        \"\"\"Visualize multiple runs for diversity analysis\"\"\"\n",
    "        n_runs = len(samples_list)\n",
    "        fig, axes = plt.subplots(1, n_runs, figsize=(4*n_runs, 4))\n",
    "        \n",
    "        if n_runs == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, samples in enumerate(samples_list):\n",
    "            axes[i].scatter(samples[:, 0].cpu(), samples[:, 1].cpu(), alpha=0.6, s=20)\n",
    "            axes[i].set_title(f'Run {i+1}')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].set_xlim(-6, 6)\n",
    "            axes[i].set_ylim(-6, 6)\n",
    "        \n",
    "        plt.suptitle(f'{method_name}: Sample Diversity')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test comprehensive comparison (uncomment after implementing all samplers)\n",
    "# comparison = SamplerComparison(ddpm_sampler, ddim_sampler)\n",
    "\n",
    "# # Speed comparison\n",
    "# speed_results = comparison.speed_comparison((30, 2), ddim_steps=[50, 25, 10])\n",
    "\n",
    "# # Quality comparison  \n",
    "# quality_results = comparison.quality_comparison((30, 2), test_data_2d)\n",
    "\n",
    "# # Diversity analysis\n",
    "# comparison.diversity_analysis((25, 2), n_runs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda95cb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Advanced Sampling Optimizations (10 minutes)\n",
    "\n",
    "### Task 6.1: Implement Practical Optimizations\n",
    "\n",
    "**Your Mission**: Implement real-world optimizations for faster and more efficient sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2e7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingOptimizer:\n",
    "    \"\"\"\n",
    "    Implement practical optimizations for diffusion sampling.\n",
    "    These techniques are essential for real-world deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SamplingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "    def cached_noise_schedule_sampling(self, ddim_sampler: DDIMSampler, \n",
    "                                     shape: Tuple[int, ...], num_steps: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement optimized sampling with pre-computed coefficients\n",
    "        \n",
    "        Pre-compute all noise schedule coefficients to avoid repeated calculations.\n",
    "        This can provide 10-20% speedup in practice.\n",
    "        \n",
    "        Args:\n",
    "            ddim_sampler: DDIM sampler to optimize\n",
    "            shape: Shape of samples to generate\n",
    "            num_steps: Number of sampling steps\n",
    "            \n",
    "        Returns:\n",
    "            Generated samples with optimized computation\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Pre-compute all α, ᾱ coefficients for the timestep schedule\n",
    "        # Step 2: Create optimized sampling loop using cached values\n",
    "        # Step 3: Avoid repeated tensor operations and indexing\n",
    "        # Step 4: Return samples with improved efficiency\n",
    "        pass\n",
    "    \n",
    "    def mixed_precision_sampling(self, sampler, shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Implement mixed precision sampling for speed/memory optimization\n",
    "        \"\"\"\n",
    "        print(\"=== Mixed Precision Sampling ===\\n\")\n",
    "        \n",
    "        # Use autocast for automatic mixed precision\n",
    "        with torch.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', enabled=True):\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                samples = sampler.sample(shape)\n",
    "                mixed_precision_time = time.time() - start_time\n",
    "                print(f\"Mixed precision sampling time: {mixed_precision_time:.3f}s\")\n",
    "                return samples\n",
    "            except Exception as e:\n",
    "                print(f\"Mixed precision error: {e}\")\n",
    "                return None\n",
    "    \n",
    "    def benchmark_optimizations(self, ddim_sampler: DDIMSampler, shape: Tuple[int, ...]):\n",
    "        \"\"\"\n",
    "        Benchmark different optimization techniques\n",
    "        \"\"\"\n",
    "        print(\"=== Optimization Benchmarks ===\\n\")\n",
    "        \n",
    "        num_steps = 20\n",
    "        results = {}\n",
    "        \n",
    "        # Baseline DDIM\n",
    "        print(\"1. Baseline DDIM...\")\n",
    "        timesteps = ddim_sampler.create_timestep_schedule(num_steps)\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            baseline_samples = ddim_sampler.sample(shape, timesteps, eta=0.0)\n",
    "            baseline_time = time.time() - start_time\n",
    "            results['Baseline'] = {'time': baseline_time, 'samples': baseline_samples}\n",
    "            print(f\"   Time: {baseline_time:.3f}s\")\n",
    "        except:\n",
    "            print(\"   ❌ Implement DDIM sample() first\")\n",
    "            return\n",
    "        \n",
    "        # Cached coefficients\n",
    "        print(\"2. Cached coefficients...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            cached_samples = self.cached_noise_schedule_sampling(ddim_sampler, shape, num_steps)\n",
    "            cached_time = time.time() - start_time\n",
    "            if cached_samples is not None:\n",
    "                results['Cached'] = {'time': cached_time, 'samples': cached_samples}\n",
    "                speedup = baseline_time / cached_time\n",
    "                print(f\"   Time: {cached_time:.3f}s (Speedup: {speedup:.2f}x)\")\n",
    "            else:\n",
    "                print(\"   ❌ Implement cached sampling\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error: {e}\")\n",
    "        \n",
    "        # Mixed precision\n",
    "        print(\"3. Mixed precision...\")\n",
    "        mixed_samples = self.mixed_precision_sampling(\n",
    "            lambda shape: ddim_sampler.sample(shape, timesteps, eta=0.0), shape)\n",
    "        \n",
    "        # Memory usage analysis\n",
    "        print(\"4. Memory analysis...\")\n",
    "        self.analyze_memory_usage(ddim_sampler, shape, num_steps)\n",
    "        \n",
    "        if len(results) > 1:\n",
    "            self.visualize_optimization_results(results)\n",
    "    \n",
    "    def analyze_memory_usage(self, ddim_sampler: DDIMSampler, shape: Tuple[int, ...], num_steps: int):\n",
    "        \"\"\"\n",
    "        Analyze memory usage during sampling\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            initial_memory = torch.cuda.memory_allocated()\n",
    "            \n",
    "            timesteps = ddim_sampler.create_timestep_schedule(num_steps)\n",
    "            \n",
    "            try:\n",
    "                samples = ddim_sampler.sample(shape, timesteps, eta=0.0)\n",
    "                peak_memory = torch.cuda.max_memory_allocated()\n",
    "                \n",
    "                memory_usage = (peak_memory - initial_memory) / 1024**2  # MB\n",
    "                print(f\"   Peak memory usage: {memory_usage:.1f} MB\")\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "            except:\n",
    "                print(\"   ❌ Cannot analyze memory - implement DDIM first\")\n",
    "        else:\n",
    "            print(\"   CUDA not available for memory analysis\")\n",
    "    \n",
    "    def visualize_optimization_results(self, results: Dict):\n",
    "        \"\"\"Visualize optimization benchmark results\"\"\"\n",
    "        methods = list(results.keys())\n",
    "        times = [results[m]['time'] for m in methods]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(methods, times, color=['blue', 'orange', 'green'][:len(methods)])\n",
    "        plt.ylabel('Sampling Time (s)')\n",
    "        plt.title('Sampling Optimization Benchmarks')\n",
    "        \n",
    "        # Add speedup labels\n",
    "        baseline_time = times[0] if 'Baseline' in methods else times[0]\n",
    "        for bar, time in zip(bars, times):\n",
    "            speedup = baseline_time / time\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{time:.3f}s\\n({speedup:.2f}x)', ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test sampling optimizer (uncomment after implementing samplers)\n",
    "# optimizer = SamplingOptimizer(pretrained_model, config)\n",
    "# optimizer.benchmark_optimizations(ddim_sampler, (25, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50fbf9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Real-World Deployment Considerations (10 minutes)\n",
    "\n",
    "### Task 7.1: Production-Ready Sampling\n",
    "\n",
    "**Your Mission**: Implement considerations for deploying diffusion models in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionSampler:\n",
    "    \"\"\"\n",
    "    Production-ready sampling implementation with practical considerations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, config: SamplingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.model.eval()  # Ensure model is in eval mode\n",
    "        \n",
    "    def robust_sampling_with_fallback(self, shape: Tuple[int, ...], \n",
    "                                    preferred_steps: int = 20, \n",
    "                                    fallback_steps: int = 50,\n",
    "                                    max_retries: int = 3) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement robust sampling with error handling and fallback\n",
    "        \n",
    "        Production systems need to handle failures gracefully.\n",
    "        \n",
    "        Args:\n",
    "            shape: Shape of samples to generate\n",
    "            preferred_steps: Preferred number of steps (fast)\n",
    "            fallback_steps: Fallback number of steps (slower but more reliable)\n",
    "            max_retries: Maximum number of retry attempts\n",
    "            \n",
    "        Returns:\n",
    "            Generated samples with fallback handling\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Try preferred (fast) sampling first\n",
    "        # Step 2: If failed, try fallback (slower) sampling\n",
    "        # Step 3: Implement retry logic with exponential backoff\n",
    "        # Step 4: Return samples or raise informative error after max retries\n",
    "        pass\n",
    "    \n",
    "    def batch_sampling_with_memory_management(self, total_samples: int, \n",
    "                                            batch_size: int = 16,\n",
    "                                            num_steps: int = 20) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement memory-efficient batch sampling\n",
    "        \n",
    "        Generate large numbers of samples without running out of memory.\n",
    "        \n",
    "        Args:\n",
    "            total_samples: Total number of samples to generate\n",
    "            batch_size: Number of samples per batch\n",
    "            num_steps: Number of sampling steps\n",
    "            \n",
    "        Returns:\n",
    "            All generated samples concatenated\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Calculate number of batches needed\n",
    "        # Step 2: For each batch: generate samples and clear memory\n",
    "        # Step 3: Concatenate results efficiently\n",
    "        # Step 4: Monitor memory usage and adjust batch size if needed\n",
    "        pass\n",
    "    \n",
    "    def adaptive_quality_sampling(self, shape: Tuple[int, ...], \n",
    "                                 target_quality: float = 0.8,\n",
    "                                 max_steps: int = 50) -> Dict:\n",
    "        \"\"\"\n",
    "        Implement adaptive sampling that adjusts steps based on quality\n",
    "        \"\"\"\n",
    "        print(\"=== Adaptive Quality Sampling ===\\n\")\n",
    "        \n",
    "        # Try different step counts and measure quality\n",
    "        step_counts = [10, 15, 20, 30, 40, 50]\n",
    "        results = []\n",
    "        \n",
    "        for steps in step_counts:\n",
    "            if steps > max_steps:\n",
    "                break\n",
    "                \n",
    "            print(f\"Testing {steps} steps...\")\n",
    "            try:\n",
    "                # This would use your DDIM implementation\n",
    "                # timesteps = create_timestep_schedule(steps)\n",
    "                # samples = ddim_sampler.sample(shape, timesteps, eta=0.0)\n",
    "                \n",
    "                # Placeholder for quality assessment\n",
    "                # In practice, you'd use metrics like FID, IS, or perceptual similarity\n",
    "                estimated_quality = min(0.9, 0.3 + steps * 0.015)  # Mock quality function\n",
    "                \n",
    "                results.append({\n",
    "                    'steps': steps,\n",
    "                    'quality': estimated_quality,\n",
    "                    'time': steps * 0.02  # Mock timing\n",
    "                })\n",
    "                \n",
    "                print(f\"  Quality: {estimated_quality:.3f}\")\n",
    "                \n",
    "                if estimated_quality >= target_quality:\n",
    "                    print(f\"✓ Target quality {target_quality} reached with {steps} steps\")\n",
    "                    break\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ❌ Error with {steps} steps: {e}\")\n",
    "        \n",
    "        self.visualize_adaptive_results(results, target_quality)\n",
    "        return results\n",
    "    \n",
    "    def sampling_health_check(self) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Perform health checks on the sampling system\n",
    "        \"\"\"\n",
    "        print(\"=== Sampling System Health Check ===\\n\")\n",
    "        \n",
    "        health_status = {}\n",
    "        \n",
    "        # Check 1: Model is in eval mode\n",
    "        health_status['model_eval_mode'] = not self.model.training\n",
    "        print(f\"Model in eval mode: {'✓' if health_status['model_eval_mode'] else '❌'}\")\n",
    "        \n",
    "        # Check 2: Device availability\n",
    "        health_status['device_available'] = torch.cuda.is_available() if 'cuda' in str(device) else True\n",
    "        print(f\"Device available: {'✓' if health_status['device_available'] else '❌'}\")\n",
    "        \n",
    "        # Check 3: Memory availability\n",
    "        if torch.cuda.is_available():\n",
    "            available_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "            health_status['sufficient_memory'] = available_memory > 100 * 1024**2  # 100MB threshold\n",
    "            print(f\"Sufficient memory: {'✓' if health_status['sufficient_memory'] else '❌'} ({available_memory/1024**2:.0f}MB available)\")\n",
    "        else:\n",
    "            health_status['sufficient_memory'] = True\n",
    "            print(\"Memory check: ✓ (CPU mode)\")\n",
    "        \n",
    "        # Check 4: Model prediction test\n",
    "        try:\n",
    "            test_input = torch.randn(1, 2).to(device)\n",
    "            test_t = torch.tensor(10).to(device)\n",
    "            with torch.no_grad():\n",
    "                test_output = self.model(test_input, test_t)\n",
    "            health_status['model_functional'] = test_output.shape == test_input.shape\n",
    "            print(f\"Model functional: {'✓' if health_status['model_functional'] else '❌'}\")\n",
    "        except Exception as e:\n",
    "            health_status['model_functional'] = False\n",
    "            print(f\"Model functional: ❌ ({e})\")\n",
    "        \n",
    "        # Check 5: Noise schedule validity\n",
    "        health_status['noise_schedule_valid'] = (\n",
    "            len(self.config.betas) == self.config.T and\n",
    "            torch.all(self.config.betas > 0) and\n",
    "            torch.all(self.config.betas < 1)\n",
    "        )\n",
    "        print(f\"Noise schedule valid: {'✓' if health_status['noise_schedule_valid'] else '❌'}\")\n",
    "        \n",
    "        overall_health = all(health_status.values())\n",
    "        print(f\"\\nOverall system health: {'✓ HEALTHY' if overall_health else '❌ ISSUES DETECTED'}\")\n",
    "        \n",
    "        return health_status\n",
    "    \n",
    "    def visualize_adaptive_results(self, results: List[Dict], target_quality: float):\n",
    "        \"\"\"Visualize adaptive quality results\"\"\"\n",
    "        if not results:\n",
    "            return\n",
    "            \n",
    "        steps = [r['steps'] for r in results]\n",
    "        qualities = [r['quality'] for r in results]\n",
    "        times = [r['time'] for r in results]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Quality vs steps\n",
    "        ax1.plot(steps, qualities, 'bo-', linewidth=2, markersize=8)\n",
    "        ax1.axhline(y=target_quality, color='red', linestyle='--', alpha=0.7, label=f'Target: {target_quality}')\n",
    "        ax1.set_xlabel('Number of Steps')\n",
    "        ax1.set_ylabel('Quality Score')\n",
    "        ax1.set_title('Adaptive Quality Control')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Time vs quality\n",
    "        ax2.plot(times, qualities, 'ro-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=target_quality, color='red', linestyle='--', alpha=0.7, label=f'Target: {target_quality}')\n",
    "        ax2.set_xlabel('Sampling Time (s)')\n",
    "        ax2.set_ylabel('Quality Score')\n",
    "        ax2.set_title('Time vs Quality Trade-off')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test production sampler\n",
    "production_sampler = ProductionSampler(pretrained_model, config)\n",
    "\n",
    "# Health check\n",
    "health_status = production_sampler.sampling_health_check()\n",
    "\n",
    "# Adaptive quality demonstration\n",
    "adaptive_results = production_sampler.adaptive_quality_sampling((20, 2), target_quality=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2ad81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Integration and Final Validation (5 minutes)\n",
    "\n",
    "### Task 8.1: Complete System Integration\n",
    "\n",
    "**Your Mission**: Integrate all components and validate the complete sampling system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff046f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_sampling_validation():\n",
    "    \"\"\"\n",
    "    Final validation of the complete sampling implementation\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive Sampling System Validation ===\\n\")\n",
    "    \n",
    "    validation_results = {\n",
    "        'ddpm_implemented': False,\n",
    "        'ddim_implemented': False,\n",
    "        'optimization_working': False,\n",
    "        'production_ready': False\n",
    "    }\n",
    "    \n",
    "    # Test 1: DDPM Implementation\n",
    "    print(\"1. Validating DDPM Implementation...\")\n",
    "    try:\n",
    "        ddpm_sampler = DDPMSampler(pretrained_model, config)\n",
    "        ddpm_samples = ddpm_sampler.sample((10, 2))\n",
    "        if ddmp_samples is not None:\n",
    "            validation_results['ddpm_implemented'] = True\n",
    "            print(\"   ✓ DDPM sampling functional\")\n",
    "        else:\n",
    "            print(\"   ❌ DDPM sample() returns None\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ DDPM error: {e}\")\n",
    "    \n",
    "    # Test 2: DDIM Implementation\n",
    "    print(\"2. Validating DDIM Implementation...\")\n",
    "    try:\n",
    "        ddim_sampler = DDIMSampler(pretrained_model, config)\n",
    "        timesteps = ddim_sampler.create_timestep_schedule(20)\n",
    "        ddim_samples = ddim_sampler.sample((10, 2), timesteps, eta=0.0)\n",
    "        if ddim_samples is not None:\n",
    "            validation_results['ddim_implemented'] = True\n",
    "            print(\"   ✓ DDIM sampling functional\")\n",
    "        else:\n",
    "            print(\"   ❌ DDIM sample() returns None\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ DDIM error: {e}\")\n",
    "    \n",
    "    # Test 3: Optimization Features\n",
    "    print(\"3. Validating Optimization Features...\")\n",
    "    try:\n",
    "        optimizer = SamplingOptimizer(pretrained_model, config)\n",
    "        # Test cached sampling if DDIM works\n",
    "        if validation_results['ddim_implemented']:\n",
    "            cached_samples = optimizer.cached_noise_schedule_sampling(ddim_sampler, (5, 2), 10)\n",
    "            if cached_samples is not None:\n",
    "                validation_results['optimization_working'] = True\n",
    "                print(\"   ✓ Optimizations functional\")\n",
    "            else:\n",
    "                print(\"   ❌ Cached sampling not implemented\")\n",
    "        else:\n",
    "            print(\"   ❌ Cannot test optimizations without DDIM\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Optimization error: {e}\")\n",
    "    \n",
    "    # Test 4: Production Readiness\n",
    "    print(\"4. Validating Production Features...\")\n",
    "    try:\n",
    "        production_sampler = ProductionSampler(pretrained_model, config)\n",
    "        health_status = production_sampler.sampling_health_check()\n",
    "        if all(health_status.values()):\n",
    "            validation_results['production_ready'] = True\n",
    "            print(\"   ✓ Production features functional\")\n",
    "        else:\n",
    "            print(\"   ❌ Production health check failed\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Production error: {e}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL VALIDATION RESULTS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for component, status in validation_results.items():\n",
    "        status_str = \"✓ PASS\" if status else \"❌ FAIL\"\n",
    "        print(f\"{component.replace('_', ' ').title()}: {status_str}\")\n",
    "    \n",
    "    overall_pass = sum(validation_results.values()) >= 2  # At least 2 components working\n",
    "    print(f\"\\nOverall System Status: {'✓ FUNCTIONAL' if overall_pass else '❌ NEEDS WORK'}\")\n",
    "    \n",
    "    if overall_pass:\n",
    "        print(\"\\n🎉 Congratulations! Your sampling system is working!\")\n",
    "        print(\"You've successfully implemented the core of modern diffusion sampling.\")\n",
    "    else:\n",
    "        print(\"\\n🔧 Keep working on the TODO implementations.\")\n",
    "        print(\"Focus on DDPM and DDIM sample() methods first.\")\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def demonstrate_complete_sampling_pipeline():\n",
    "    \"\"\"\n",
    "    Demonstrate the complete sampling pipeline from theory to practice\n",
    "    \"\"\"\n",
    "    print(\"=== Complete Sampling Pipeline Demonstration ===\\n\")\n",
    "    \n",
    "    pipeline_stages = [\n",
    "        \"1. 🧠 Trained Model: Noise predictor ε_θ(x_t, t)\",\n",
    "        \"2. 📐 Mathematical Framework: ELBO and reverse process theory\", \n",
    "        \"3. 🎲 DDPM Sampling: Stochastic reverse process (high quality)\",\n",
    "        \"4. ⚡ DDIM Sampling: Deterministic acceleration (fast generation)\",\n",
    "        \"5. 🎛️  Controllable Stochasticity: η parameter for speed/diversity tradeoff\",\n",
    "        \"6. 🚀 Optimizations: Caching, mixed precision, memory management\",\n",
    "        \"7. 🏭 Production Deployment: Error handling, health checks, batching\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Your journey through diffusion sampling:\")\n",
    "    for stage in pipeline_stages:\n",
    "        print(f\"  {stage}\")\n",
    "    \n",
    "    print(f\"\\n💡 Key insights achieved:\")\n",
    "    print(f\"   • DDPM: Faithful to theory, slow but high quality\")\n",
    "    print(f\"   • DDIM: Clever mathematical insight enables 10-50x speedup\")\n",
    "    print(f\"   • η parameter: Smooth interpolation between deterministic and stochastic\")\n",
    "    print(f\"   • Production: Real systems need robustness and optimization\")\n",
    "    \n",
    "    print(f\"\\n🌟 What this enables:\")\n",
    "    print(f\"   • Real-time creative applications\") \n",
    "    print(f\"   • High-resolution image generation\")\n",
    "    print(f\"   • Interactive AI art tools\")\n",
    "    print(f\"   • Large-scale content creation\")\n",
    "    \n",
    "    # Create pipeline visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "    \n",
    "    stages = [\"Theory\", \"DDPM\", \"DDIM\", \"Optimization\", \"Production\"]\n",
    "    stage_colors = ['lightblue', 'blue', 'red', 'orange', 'green']\n",
    "    \n",
    "    # Draw pipeline flow\n",
    "    y_pos = 0.5\n",
    "    stage_width = 0.15\n",
    "    \n",
    "    for i, (stage, color) in enumerate(zip(stages, stage_colors)):\n",
    "        x_pos = 0.1 + i * 0.2\n",
    "        \n",
    "        # Stage box\n",
    "        ax.add_patch(plt.Rectangle((x_pos - stage_width/2, y_pos - 0.1), \n",
    "                                  stage_width, 0.2, \n",
    "                                  facecolor=color, alpha=0.7, edgecolor='black'))\n",
    "        ax.text(x_pos, y_pos, stage, ha='center', va='center', \n",
    "               fontsize=10, weight='bold')\n",
    "        \n",
    "        # Arrow to next stage\n",
    "        if i < len(stages) - 1:\n",
    "            ax.arrow(x_pos + stage_width/2, y_pos, \n",
    "                    0.2 - stage_width, 0, \n",
    "                    head_width=0.03, head_length=0.02, \n",
    "                    fc='gray', ec='gray')\n",
    "    \n",
    "    # Add benefits annotations\n",
    "    benefits = [\n",
    "        (0.1, 0.3, \"Mathematical\\nFoundation\"),\n",
    "        (0.3, 0.7, \"High Quality\\nGeneration\"),\n",
    "        (0.5, 0.3, \"Fast Sampling\\n10-50x speedup\"),\n",
    "        (0.7, 0.7, \"Memory Efficient\\nCached Operations\"),\n",
    "        (0.9, 0.3, \"Robust\\nDeployment\")\n",
    "    ]\n",
    "    \n",
    "    for x, y, text in benefits:\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=9,\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Diffusion Sampling: From Theory to Production', fontsize=16, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run final validation and demonstration\n",
    "validation_results = comprehensive_sampling_validation()\n",
    "demonstrate_complete_sampling_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6716068",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "### Core Sampling Functions (Students Implement):\n",
    "\n",
    "**✅ Essential TODOs:**\n",
    "- [ ] `analyze_noise_prediction_quality()` - Validate model behavior\n",
    "- [ ] `add_noise()` - Forward process implementation\n",
    "- [ ] `compute_posterior_mean()` - DDPM posterior mean calculation\n",
    "- [ ] `compute_posterior_variance()` - DDPM posterior variance calculation\n",
    "- [ ] `ddpm_step()` - Single DDPM sampling step\n",
    "- [ ] `sample()` (DDPM) - Complete DDPM sampling algorithm\n",
    "- [ ] `predict_x0_from_eps()` - Clean image prediction from noise\n",
    "- [ ] `predict_eps_from_x0()` - Noise prediction from clean image\n",
    "- [ ] `ddim_step()` - Single DDIM sampling step with η control\n",
    "- [ ] `sample()` (DDIM) - Complete DDIM sampling with step skipping\n",
    "- [ ] `compute_stochastic_variance()` - η parameter variance computation\n",
    "- [ ] `cached_noise_schedule_sampling()` - Performance optimization\n",
    "- [ ] `robust_sampling_with_fallback()` - Production error handling\n",
    "- [ ] `batch_sampling_with_memory_management()` - Memory-efficient sampling\n",
    "\n",
    "**✅ Provided Starter Code:**\n",
    "- [ ] All visualization and analysis functions\n",
    "- [ ] Model architecture and configuration setup\n",
    "- [ ] Comparison and benchmarking frameworks\n",
    "- [ ] Health checking and validation systems\n",
    "- [ ] Complete testing and demonstration pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "Submit your completed Jupyter notebook (.ipynb file) with:\n",
    "\n",
    "**✅ DDPM Implementation:**\n",
    "- Complete stochastic sampling algorithm with proper noise injection\n",
    "- Posterior mean and variance calculations\n",
    "- Validation of sampling consistency and quality\n",
    "\n",
    "**✅ DDIM Implementation:**\n",
    "- Deterministic sampling with step skipping capability\n",
    "- Clean image prediction and noise schedule reconstruction\n",
    "- η parameter implementation for controllable stochasticity\n",
    "\n",
    "**✅ Performance Analysis:**\n",
    "- Speed comparisons between DDPM and DDIM\n",
    "- Quality vs speed trade-off analysis\n",
    "- Step reduction studies and optimal scheduling\n",
    "\n",
    "**✅ Optimization Techniques:**\n",
    "- Cached coefficient computation for efficiency\n",
    "- Memory management for large-scale sampling\n",
    "- Production-ready error handling and fallback mechanisms\n",
    "\n",
    "**✅ Comprehensive Evaluation:**\n",
    "- Validation of all sampling components\n",
    "- Comparison of different sampling strategies\n",
    "- Analysis of practical deployment considerations\n",
    "\n",
    "**✅ Documentation and Insights:**\n",
    "- Clear explanations of implementation choices\n",
    "- Discussion of trade-offs and practical considerations\n",
    "- Connection between mathematical theory and implementation details\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Key Mathematical Formulas\n",
    "\n",
    "### For Implementation Reference:\n",
    "\n",
    "**DDPM Posterior Mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c768e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ_θ(x_t, t) = (1/√α_t) * (x_t - (1-α_t)/√(1-ᾱ_t) * ε_θ(x_t, t))\n",
    "alpha_t = config.alphas[t]\n",
    "alpha_cumprod_t = config.alphas_cumprod[t]\n",
    "coeff_1 = 1.0 / torch.sqrt(alpha_t)\n",
    "coeff_2 = (1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)\n",
    "posterior_mean = coeff_1 * (x_t - coeff_2 * predicted_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9949263",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**DDPM Posterior Variance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae429aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̃²_t = β_t * (1-ᾱ_{t-1})/(1-ᾱ_t)\n",
    "if t == 0:\n",
    "    posterior_variance = 0\n",
    "else:\n",
    "    posterior_variance = (config.betas[t] * (1 - config.alphas_cumprod[t-1]) / \n",
    "                         (1 - config.alphas_cumprod[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46a98e1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**DDIM Clean Image Prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef18383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x̂_0 = (x_t - √(1-ᾱ_t) * ε) / √ᾱ_t\n",
    "alpha_cumprod_t = config.alphas_cumprod[t]\n",
    "predicted_x0 = (x_t - torch.sqrt(1 - alpha_cumprod_t) * eps) / torch.sqrt(alpha_cumprod_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cb007b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**DDIM Deterministic Update:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_s = √ᾱ_s * x̂_0 + √(1-ᾱ_s) * ε̂\n",
    "alpha_cumprod_s = config.alphas_cumprod[s]\n",
    "x_s = (torch.sqrt(alpha_cumprod_s) * predicted_x0 + \n",
    "       torch.sqrt(1 - alpha_cumprod_s) * predicted_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831182f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**η-Controlled Stochastic Variance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae9a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ_t^2 = η^2 * β̃_{t→s} where β̃_{t→s} = (1-ᾱ_s)/(1-ᾱ_t) * (1 - ᾱ_t/ᾱ_s)\n",
    "alpha_cumprod_t = config.alphas_cumprod[t]\n",
    "alpha_cumprod_s = config.alphas_cumprod[s]\n",
    "beta_tilde = ((1 - alpha_cumprod_s) / (1 - alpha_cumprod_t) * \n",
    "              (1 - alpha_cumprod_t / alpha_cumprod_s))\n",
    "stochastic_variance = eta**2 * beta_tilde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a497f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Implementation Issues & Solutions\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "**Sampling Divergence:**\n",
    "- Check that model is in `.eval()` mode during sampling\n",
    "- Ensure noise schedule coefficients are computed correctly\n",
    "- Verify timestep indexing (0-based vs 1-based)\n",
    "- Add gradient checkpointing if running out of memory\n",
    "\n",
    "**DDIM vs DDPM Consistency:**\n",
    "- When η=1, DDIM should behave similarly to DDPM\n",
    "- Test with same random seeds for reproducibility\n",
    "- Verify that timestep schedules are correctly implemented\n",
    "- Check that final step (t=0) doesn't add noise\n",
    "\n",
    "**Performance Issues:**\n",
    "- Pre-compute noise schedule coefficients outside sampling loop\n",
    "- Use in-place operations where possible\n",
    "- Implement gradient checkpointing for memory efficiency\n",
    "- Consider mixed precision for speed without quality loss\n",
    "\n",
    "**Quality Degradation:**\n",
    "- Too few DDIM steps can cause artifacts\n",
    "- Check that noise prediction model is properly trained\n",
    "- Verify that timestep embeddings are correctly normalized\n",
    "- Ensure numerical stability in coefficient computations\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
