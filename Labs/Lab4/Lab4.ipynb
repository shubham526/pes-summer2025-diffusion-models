{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528c80ba",
   "metadata": {},
   "source": [
    "# Lab 4: ELBO for Diffusion Models - Learning to Reverse Chaos\n",
    "**Course: Diffusion Models: Theory and Applications**  \n",
    "**Duration: 90 minutes**  \n",
    "**Team Size: 2 students (same teams from Labs 1-3)**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, students will be able to:\n",
    "1. **Implement** the complete ELBO derivation for diffusion models from first principles\n",
    "2. **Build** the three-forces decomposition: reconstruction, prior matching, and denoising\n",
    "3. **Create** the tractable reverse distribution using Bayes' rule\n",
    "4. **Construct** the noise prediction reparameterization\n",
    "5. **Connect** complex ELBO theory to simple practical training algorithms\n",
    "6. **Demonstrate** how mathematical elegance enables state-of-the-art generation\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Setup and Diffusion Framework\n",
    "\n",
    "### Part 1: Team Setup & Sequential Latent Mission (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609662a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffusion ELBO implementation setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "from typing import Tuple, Dict, List\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducible mathematics\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Diffusion mathematics on: {device}\")\n",
    "\n",
    "# Create test data for diffusion experiments\n",
    "def create_diffusion_test_data(n_samples: int = 300) -> torch.Tensor:\n",
    "    \"\"\"Create 2D data for testing diffusion ELBO implementations\"\"\"\n",
    "    # Create a more complex distribution - mixture of Gaussians\n",
    "    t = torch.linspace(0, 2*math.pi, n_samples//3)\n",
    "    \n",
    "    # Three clusters in a triangle formation\n",
    "    cluster1 = torch.stack([2*torch.cos(t) + 0.3*torch.randn(n_samples//3), \n",
    "                           2*torch.sin(t) + 0.3*torch.randn(n_samples//3)], dim=1)\n",
    "    cluster2 = torch.stack([2*torch.cos(t + 2*math.pi/3) + 0.3*torch.randn(n_samples//3),\n",
    "                           2*torch.sin(t + 2*math.pi/3) + 0.3*torch.randn(n_samples//3)], dim=1)\n",
    "    cluster3 = torch.stack([2*torch.cos(t + 4*math.pi/3) + 0.3*torch.randn(n_samples//3),\n",
    "                           2*torch.sin(t + 4*math.pi/3) + 0.3*torch.randn(n_samples//3)], dim=1)\n",
    "    \n",
    "    data = torch.cat([cluster1, cluster2, cluster3], dim=0)\n",
    "    return data.to(device)\n",
    "\n",
    "# Generate test data\n",
    "test_data = create_diffusion_test_data(300)\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# Visualize test data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(test_data[:, 0].cpu(), test_data[:, 1].cpu(), alpha=0.7, s=30, c='blue')\n",
    "plt.title('Test Data: Three-Cluster Distribution')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Define noise schedule for diffusion process\n",
    "def create_noise_schedule(T: int = 100, beta_start: float = 1e-4, beta_end: float = 2e-2) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Create linear noise schedule for diffusion process\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing β_t, α_t, and ᾱ_t sequences\n",
    "    \"\"\"\n",
    "    # Linear schedule\n",
    "    betas = torch.linspace(beta_start, beta_end, T).to(device)\n",
    "    alphas = 1.0 - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    \n",
    "    return {\n",
    "        'betas': betas,\n",
    "        'alphas': alphas, \n",
    "        'alphas_cumprod': alphas_cumprod,\n",
    "        'T': T\n",
    "    }\n",
    "\n",
    "# Create noise schedule\n",
    "noise_schedule = create_noise_schedule(T=50)  # Smaller T for faster computation\n",
    "T = noise_schedule['T']\n",
    "print(f\"Using T = {T} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694ea2e2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: The Intractable Sequential Likelihood (15 minutes)\n",
    "\n",
    "### Task 2.1: Experience the Marginal Likelihood Crisis\n",
    "\n",
    "**Your Mission**: Implement the intractable marginal likelihood for sequential latents and see why direct optimization fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a44916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialLikelihoodDemo:\n",
    "    \"\"\"\n",
    "    Demonstrate why direct likelihood computation fails for sequential generative models.\n",
    "    You'll implement the mathematics to see the exponential complexity explosion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dim: int = 2, T: int = 50):\n",
    "        self.data_dim = data_dim\n",
    "        self.T = T\n",
    "        self.noise_schedule = create_noise_schedule(T)\n",
    "        \n",
    "        # Simple reverse process model (will be learned)\n",
    "        # For demo purposes, we'll use a simple linear model\n",
    "        self.reverse_model = nn.ModuleList([\n",
    "            nn.Linear(data_dim, data_dim) for _ in range(T)\n",
    "        ]).to(device)\n",
    "        \n",
    "    def forward_step(self, x_prev: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement single forward diffusion step\n",
    "        \n",
    "        Apply q(x_t | x_{t-1}) = N(x_t; √α_t x_{t-1}, β_t I)\n",
    "        \n",
    "        Args:\n",
    "            x_prev: Previous state (batch_size, data_dim)\n",
    "            t: Timestep (0-indexed)\n",
    "            \n",
    "        Returns:\n",
    "            x_t: Next state after adding noise\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get α_t and β_t from noise schedule\n",
    "        # Step 2: Compute mean: √α_t * x_{t-1}\n",
    "        # Step 3: Sample noise with variance β_t\n",
    "        # Step 4: Return x_t = mean + noise\n",
    "        pass\n",
    "    \n",
    "    def forward_trajectory(self, x0: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement complete forward diffusion trajectory\n",
    "        \n",
    "        Generate x_1, x_2, ..., x_T from x_0 using the forward process\n",
    "        \n",
    "        Args:\n",
    "            x0: Clean data (batch_size, data_dim)\n",
    "            \n",
    "        Returns:\n",
    "            trajectory: List of states [x_0, x_1, ..., x_T]\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Initialize trajectory with x_0\n",
    "        # Step 2: For each timestep t, apply forward_step\n",
    "        # Step 3: Store each intermediate state\n",
    "        # Step 4: Return complete trajectory\n",
    "        pass\n",
    "    \n",
    "    def direct_jump_forward(self, x0: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement direct jump to timestep t\n",
    "        \n",
    "        Use the analytical form: q(x_t | x_0) = N(x_t; √ᾱ_t x_0, (1-ᾱ_t) I)\n",
    "        \n",
    "        This is much more efficient than sequential forward steps!\n",
    "        \n",
    "        Args:\n",
    "            x0: Clean data (batch_size, data_dim)\n",
    "            t: Target timestep\n",
    "            \n",
    "        Returns:\n",
    "            x_t: State at timestep t\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get ᾱ_t from noise schedule\n",
    "        # Step 2: Compute mean: √ᾱ_t * x_0\n",
    "        # Step 3: Compute variance: (1 - ᾱ_t)\n",
    "        # Step 4: Sample from N(mean, variance * I)\n",
    "        pass\n",
    "    \n",
    "    def marginal_likelihood_approximation(self, x0: torch.Tensor, n_trajectories: int = 100) -> float:\n",
    "        \"\"\"\n",
    "        Attempt Monte Carlo approximation of p(x_0)\n",
    "        \n",
    "        This will demonstrate the computational intractability.\n",
    "        \n",
    "        p(x_0) = ∫ p(x_{0:T}) dx_{1:T}\n",
    "               ≈ (1/K) Σ p(x_{0:T}^{(k)}) where x_{1:T}^{(k)} ~ q(x_{1:T}|x_0)\n",
    "        \"\"\"\n",
    "        print(f\"Attempting marginal likelihood with {n_trajectories} trajectories...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        log_probs = []\n",
    "        for k in range(n_trajectories):\n",
    "            # Generate forward trajectory (after students implement forward_trajectory)\n",
    "            try:\n",
    "                trajectory = self.forward_trajectory(x0[:1])  # Single sample\n",
    "                \n",
    "                # Compute joint probability p(x_{0:T}) = p(x_T) * ∏ p(x_{t-1}|x_t)\n",
    "                # For demo, we'll use simple Gaussian approximations\n",
    "                log_prob = 0.0\n",
    "                \n",
    "                # Prior term: p(x_T) ≈ N(0, I) since x_T should be noise\n",
    "                x_T = trajectory[-1]\n",
    "                log_prob += -0.5 * (x_T**2).sum()\n",
    "                \n",
    "                # Reverse terms (using our simple model)\n",
    "                for t in range(self.T, 0, -1):\n",
    "                    x_t = trajectory[t]\n",
    "                    x_t_minus_1 = trajectory[t-1]\n",
    "                    \n",
    "                    # Simple reverse model prediction\n",
    "                    predicted_mean = self.reverse_model[t-1](x_t)\n",
    "                    log_prob += -0.5 * ((x_t_minus_1 - predicted_mean)**2).sum()\n",
    "                \n",
    "                log_probs.append(log_prob.item())\n",
    "            except:\n",
    "                print(\"Forward trajectory not implemented yet\")\n",
    "                return 0.0\n",
    "        \n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # Results analysis\n",
    "        log_probs = np.array(log_probs)\n",
    "        print(f\"Computation time: {computation_time:.3f}s\")\n",
    "        print(f\"Log probability estimates: mean={log_probs.mean():.3f}, std={log_probs.std():.3f}\")\n",
    "        print(f\"Probability range: [{np.exp(log_probs.min()):.2e}, {np.exp(log_probs.max()):.2e}]\")\n",
    "        \n",
    "        print(\"\\nWhy this approach fails:\")\n",
    "        print(\"❌ Exponential growth with sequence length T\")\n",
    "        print(\"❌ High variance in estimates\")\n",
    "        print(\"❌ Requires learned reverse model (circular dependency)\")\n",
    "        print(\"❌ Completely impractical for realistic T (1000+ steps)\")\n",
    "        \n",
    "        return log_probs.mean()\n",
    "    \n",
    "    def demonstrate_complexity_explosion(self):\n",
    "        \"\"\"Show how complexity grows with T\"\"\"\n",
    "        print(\"=== Demonstrating Complexity Explosion ===\\n\")\n",
    "        \n",
    "        complexity_analysis = []\n",
    "        for T_test in [5, 10, 20, 50]:\n",
    "            print(f\"Sequence length T = {T_test}:\")\n",
    "            print(f\"  Number of random variables: {T_test + 1}\")\n",
    "            print(f\"  Marginal integration dimensions: {T_test * self.data_dim}\")\n",
    "            print(f\"  Forward process evaluations: {T_test}\")\n",
    "            print(f\"  Reverse process evaluations: {T_test}\")\n",
    "            \n",
    "            # Estimate computational cost (hypothetical)\n",
    "            forward_cost = T_test\n",
    "            reverse_cost = T_test\n",
    "            integration_cost = (T_test * self.data_dim) ** 2  # Simplified estimate\n",
    "            total_cost = forward_cost + reverse_cost + integration_cost\n",
    "            \n",
    "            complexity_analysis.append((T_test, total_cost))\n",
    "            print(f\"  Estimated computational cost: {total_cost}\")\n",
    "            print()\n",
    "        \n",
    "        # Visualize complexity growth\n",
    "        T_values, costs = zip(*complexity_analysis)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(T_values, costs, 'ro-', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Sequence Length T')\n",
    "        plt.ylabel('Computational Cost (arbitrary units)')\n",
    "        plt.title('Computational Complexity vs Sequence Length')\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"💡 Solution: ELBO provides a tractable alternative!\")\n",
    "\n",
    "# Test sequential likelihood demo (uncomment after implementing TODOs)\n",
    "# demo = SequentialLikelihoodDemo(data_dim=2, T=10)\n",
    "# demo.demonstrate_complexity_explosion()\n",
    "# demo.marginal_likelihood_approximation(test_data[:1], n_trajectories=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d9b1f",
   "metadata": {},
   "source": [
    "### Task 2.2: The Forward Process Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_forward_process():\n",
    "    \"\"\"\n",
    "    Show the forward diffusion process in action\n",
    "    \"\"\"\n",
    "    print(\"=== Forward Diffusion Process ===\\n\")\n",
    "    \n",
    "    # Take a single data point\n",
    "    x0 = test_data[0:1]  # Single sample\n",
    "    print(f\"Original data point: {x0.squeeze().cpu().numpy()}\")\n",
    "    \n",
    "    # Show sequential vs direct jump\n",
    "    timesteps_to_show = [0, 5, 10, 20, 30, 49]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, len(timesteps_to_show), figsize=(15, 6))\n",
    "    \n",
    "    demo = SequentialLikelihoodDemo(data_dim=2, T=50)\n",
    "    \n",
    "    # Sequential forward process\n",
    "    print(\"\\nSequential forward process:\")\n",
    "    try:\n",
    "        trajectory = demo.forward_trajectory(x0)\n",
    "        for i, t in enumerate(timesteps_to_show):\n",
    "            if trajectory is not None:\n",
    "                x_t = trajectory[t].cpu().numpy().squeeze()\n",
    "                axes[0, i].scatter(x_t[0], x_t[1], c='blue', s=100)\n",
    "                axes[0, i].set_title(f't={t}')\n",
    "                axes[0, i].set_xlim(-4, 4)\n",
    "                axes[0, i].set_ylim(-4, 4)\n",
    "                axes[0, i].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylabel('Sequential\\nForward')\n",
    "    except:\n",
    "        print(\"Implement forward_trajectory first\")\n",
    "    \n",
    "    # Direct jump forward process\n",
    "    print(\"Direct jump forward process:\")\n",
    "    try:\n",
    "        for i, t in enumerate(timesteps_to_show):\n",
    "            x_t = demo.direct_jump_forward(x0, t).cpu().numpy().squeeze()\n",
    "            axes[1, i].scatter(x_t[0], x_t[1], c='red', s=100)\n",
    "            axes[1, i].set_title(f't={t}')\n",
    "            axes[1, i].set_xlim(-4, 4)\n",
    "            axes[1, i].set_ylim(-4, 4)\n",
    "            axes[1, i].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_ylabel('Direct Jump\\nForward')\n",
    "    except:\n",
    "        print(\"Implement direct_jump_forward first\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insight: Direct jump is much more efficient!\")\n",
    "    print(\"This efficiency will be crucial for ELBO computation.\")\n",
    "\n",
    "# Run forward process demonstration\n",
    "demonstrate_forward_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282e115",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: ELBO Derivation for Diffusion Models (25 minutes)\n",
    "\n",
    "### Task 3.1: Implement the ELBO Decomposition\n",
    "\n",
    "**Your Mission**: Derive the three-forces decomposition of the diffusion ELBO step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionELBODerivation:\n",
    "    \"\"\"\n",
    "    Implement the complete ELBO derivation for diffusion models.\n",
    "    You'll walk through each algebraic step to transform the intractable likelihood\n",
    "    into three interpretable terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noise_schedule: Dict[str, torch.Tensor]):\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.T = noise_schedule['T']\n",
    "        self.betas = noise_schedule['betas']\n",
    "        self.alphas = noise_schedule['alphas']\n",
    "        self.alphas_cumprod = noise_schedule['alphas_cumprod']\n",
    "    \n",
    "    def log_probability_factorizations(self, x_trajectory: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the Markovian factorizations\n",
    "        \n",
    "        Compute both:\n",
    "        1. Generative model: p_θ(x_{0:T}) = p(x_T) ∏ p_θ(x_{t-1}|x_t)\n",
    "        2. Forward process: q(x_{1:T}|x_0) = ∏ q(x_t|x_{t-1})\n",
    "        \n",
    "        This is the foundation for ELBO derivation.\n",
    "        \n",
    "        Args:\n",
    "            x_trajectory: List of states [x_0, x_1, ..., x_T]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with log probability components\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # For forward process:\n",
    "        # Step 1: Compute log q(x_t|x_{t-1}) for each t\n",
    "        # Step 2: Sum all forward steps\n",
    "        \n",
    "        # For generative model (simplified for demonstration):\n",
    "        # Step 1: Compute log p(x_T) assuming N(0, I)\n",
    "        # Step 2: Compute log p_θ(x_{t-1}|x_t) for each t (simplified)\n",
    "        # Step 3: Combine prior and reverse terms\n",
    "        \n",
    "        # Return both factorizations for ELBO computation\n",
    "        pass\n",
    "    \n",
    "    def implement_elbo_step1_separation(self, x_trajectory: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement ELBO Step 1 - Strategic Term Separation\n",
    "        \n",
    "        Starting from: ELBO = E_q[log p_θ(x_{0:T}) - log q(x_{1:T}|x_0)]\n",
    "        \n",
    "        Separate into:\n",
    "        1. log p(x_T) - prior term\n",
    "        2. log p_θ(x_0|x_1) - reconstruction term  \n",
    "        3. Σ log p_θ(x_{t-1}|x_t) - reverse denoising terms\n",
    "        4. Σ log q(x_t|x_{t-1}) - forward denoising terms\n",
    "        \n",
    "        Args:\n",
    "            x_trajectory: Complete trajectory [x_0, ..., x_T]\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with separated terms\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract boundary terms (t=0 and t=T)\n",
    "        # Step 2: Identify bulk terms (t=2 to T-1)\n",
    "        # Step 3: Group terms for strategic pairing\n",
    "        # Step 4: Return organized components\n",
    "        pass\n",
    "    \n",
    "    def implement_elbo_step2_reindexing(self, separated_terms: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement ELBO Step 2 - Strategic Reindexing\n",
    "        \n",
    "        Align the indices of forward and reverse sums for proper comparison.\n",
    "        Transform forward sum to match reverse sum indexing.\n",
    "        \n",
    "        Args:\n",
    "            separated_terms: Output from step 1\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with aligned indices\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Split forward sum: separate last term log q(x_T|x_{T-1})\n",
    "        # Step 2: Reindex remaining forward terms to align with reverse terms\n",
    "        # Step 3: Verify both sums now run from t=2 to T\n",
    "        # Step 4: Return aligned terms for comparison\n",
    "        pass\n",
    "    \n",
    "    def implement_elbo_step3_bayes_rule(self, aligned_terms: Dict[str, torch.Tensor], \n",
    "                                       x_trajectory: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement ELBO Step 3 - Bayes Rule Transformation\n",
    "        \n",
    "        Transform mismatched comparisons into proper reverse process comparisons.\n",
    "        \n",
    "        Key insight: We need to compare p_θ(x_{t-1}|x_t) with q(x_{t-1}|x_t, x_0)\n",
    "        not with q(x_{t-1}|x_{t-2}).\n",
    "        \n",
    "        Use: q(x_{t-1}|x_t, x_0) = q(x_t|x_{t-1})q(x_{t-1}|x_0) / q(x_t|x_0)\n",
    "        \n",
    "        Args:\n",
    "            aligned_terms: Output from step 2\n",
    "            x_trajectory: Complete trajectory for Bayes rule application\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with proper reverse comparisons\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Apply Bayes rule to create q(x_{t-1}|x_t, x_0)\n",
    "        # Step 2: Substitute Bayes rule result into aligned terms\n",
    "        # Step 3: Rearrange using logarithm properties\n",
    "        # Step 4: Group terms to create KL divergences\n",
    "        pass\n",
    "    \n",
    "    def implement_elbo_step4_final_form(self, bayes_terms: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement ELBO Step 4 - Final KL Divergence Form\n",
    "        \n",
    "        Transform grouped terms into the final three-forces decomposition:\n",
    "        1. Reconstruction: E_q[log p_θ(x_0|x_1)]\n",
    "        2. Prior matching: KL(q(x_T|x_0) || p(x_T))\n",
    "        3. Denoising: Σ KL(q(x_{t-1}|x_t, x_0) || p_θ(x_{t-1}|x_t))\n",
    "        \n",
    "        Args:\n",
    "            bayes_terms: Output from step 3\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with final ELBO decomposition\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Identify difference-of-logs patterns\n",
    "        # Step 2: Convert to KL divergence form\n",
    "        # Step 3: Organize into three interpretable terms\n",
    "        # Step 4: Return final ELBO decomposition\n",
    "        pass\n",
    "    \n",
    "    def complete_elbo_derivation(self, x_trajectory: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Execute the complete ELBO derivation pipeline\n",
    "        \n",
    "        Walk through all four steps to transform intractable likelihood\n",
    "        into tractable three-forces decomposition.\n",
    "        \"\"\"\n",
    "        print(\"=== Complete ELBO Derivation ===\\n\")\n",
    "        \n",
    "        # Step 1: Strategic separation\n",
    "        print(\"Step 1: Strategic term separation...\")\n",
    "        separated = self.implement_elbo_step1_separation(x_trajectory)\n",
    "        \n",
    "        # Step 2: Index alignment  \n",
    "        print(\"Step 2: Strategic reindexing...\")\n",
    "        aligned = self.implement_elbo_step2_reindexing(separated)\n",
    "        \n",
    "        # Step 3: Bayes rule transformation\n",
    "        print(\"Step 3: Bayes rule transformation...\")\n",
    "        bayes_transformed = self.implement_elbo_step3_bayes_rule(aligned, x_trajectory)\n",
    "        \n",
    "        # Step 4: Final KL form\n",
    "        print(\"Step 4: Final KL divergence form...\")\n",
    "        final_elbo = self.implement_elbo_step4_final_form(bayes_transformed)\n",
    "        \n",
    "        print(\"✓ ELBO derivation complete!\")\n",
    "        return final_elbo\n",
    "    \n",
    "    def visualize_derivation_steps(self):\n",
    "        \"\"\"\n",
    "        Create visualization showing the derivation progression\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "        \n",
    "        steps = [\n",
    "            \"Intractable\\nLikelihood\",\n",
    "            \"Strategic\\nSeparation\", \n",
    "            \"Index\\nAlignment\",\n",
    "            \"Bayes Rule\\nTransform\",\n",
    "            \"Final KL\\nForm\"\n",
    "        ]\n",
    "        \n",
    "        descriptions = [\n",
    "            \"log p(x₀) = ?\\nMarginal integral\",\n",
    "            \"Boundary terms\\nvs bulk terms\",\n",
    "            \"Align forward\\nand reverse sums\",\n",
    "            \"Create proper\\nreverse comparisons\", \n",
    "            \"Three forces:\\nReconstruction,\\nPrior, Denoising\"\n",
    "        ]\n",
    "        \n",
    "        colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "        \n",
    "        # Draw progression\n",
    "        y_pos = 0.7\n",
    "        x_positions = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        for i, (step, desc, color, x) in enumerate(zip(steps, descriptions, colors, x_positions)):\n",
    "            # Step box\n",
    "            ax.text(x, y_pos, step, ha='center', va='center', fontsize=11, weight='bold',\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=color, alpha=0.8))\n",
    "            \n",
    "            # Description\n",
    "            ax.text(x, y_pos - 0.25, desc, ha='center', va='center', fontsize=9,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.8))\n",
    "            \n",
    "            # Arrow to next step\n",
    "            if i < len(steps) - 1:\n",
    "                ax.arrow(x + 0.05, y_pos, 0.1, 0, head_width=0.02, head_length=0.02,\n",
    "                        fc='black', ec='black')\n",
    "        \n",
    "        # Add mathematical expressions\n",
    "        math_expressions = [\n",
    "            \"∫ p(x₀:T) dx₁:T\",\n",
    "            \"log p(xT) +\\nlog pθ(x₀|x₁) +\\nΣ log pθ(x_{t-1}|xt)\",\n",
    "            \"Aligned sums\\nt=2 to T\",\n",
    "            \"Compare\\npθ(x_{t-1}|xt) vs\\nq(x_{t-1}|xt,x₀)\",\n",
    "            \"E[log pθ(x₀|x₁)] -\\nKL(q(xT|x₀)||p(xT)) -\\nΣ KL(...)\"\n",
    "        ]\n",
    "        \n",
    "        for i, (expr, x) in enumerate(zip(math_expressions, x_positions)):\n",
    "            ax.text(x, 0.2, expr, ha='center', va='center', fontsize=8,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.2\", facecolor='lightblue', alpha=0.6))\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('ELBO Derivation: From Intractable to Tractable', fontsize=14, weight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test ELBO derivation (uncomment after implementing TODOs)\n",
    "# elbo_derivation = DiffusionELBODerivation(noise_schedule)\n",
    "# elbo_derivation.visualize_derivation_steps()\n",
    "\n",
    "# # Test with sample trajectory\n",
    "# demo = SequentialLikelihoodDemo(data_dim=2, T=50)\n",
    "# try:\n",
    "#     sample_trajectory = demo.forward_trajectory(test_data[0:1])\n",
    "#     if sample_trajectory is not None:\n",
    "#         elbo_result = elbo_derivation.complete_elbo_derivation(sample_trajectory)\n",
    "#         print(\"ELBO derivation successful!\")\n",
    "# except:\n",
    "#     print(\"Implement trajectory methods first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17219e53",
   "metadata": {},
   "source": [
    "### Task 3.2: The Tractable Reverse Distribution\n",
    "\n",
    "**Your Mission**: Implement the key insight that makes diffusion training possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed44e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TractableReverseDistribution:\n",
    "    \"\"\"\n",
    "    Implement the tractable reverse distribution q(x_{t-1}|x_t, x_0).\n",
    "    This is the mathematical breakthrough that enables practical diffusion training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noise_schedule: Dict[str, torch.Tensor]):\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.betas = noise_schedule['betas']\n",
    "        self.alphas = noise_schedule['alphas']\n",
    "        self.alphas_cumprod = noise_schedule['alphas_cumprod']\n",
    "    \n",
    "    def bayes_rule_transformation(self, x_t: torch.Tensor, x_0: torch.Tensor, t: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement Bayes rule for reverse distribution\n",
    "        \n",
    "        Compute q(x_{t-1}|x_t, x_0) using:\n",
    "        q(x_{t-1}|x_t, x_0) = q(x_t|x_{t-1}) * q(x_{t-1}|x_0) / q(x_t|x_0)\n",
    "        \n",
    "        All terms are Gaussian, so the result is also Gaussian!\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            x_0: Original clean data\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with mean and variance of q(x_{t-1}|x_t, x_0)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Extract noise schedule parameters for timestep t\n",
    "        # Step 2: Apply the analytical Gaussian formula (provided in lecture)\n",
    "        # Step 3: Compute optimal mean using the interpolation formula\n",
    "        # Step 4: Compute optimal variance (fixed, no learning required!)\n",
    "        # Step 5: Return both mean and variance\n",
    "        pass\n",
    "    \n",
    "    def optimal_reverse_mean(self, x_t: torch.Tensor, x_0: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement the optimal reverse mean\n",
    "        \n",
    "        Use the beautiful interpolation formula:\n",
    "        μ̃_t(x_t, x_0) = (√ᾱ_{t-1} β_t)/(1-ᾱ_t) * x_0 + (√α_t (1-ᾱ_{t-1}))/(1-ᾱ_t) * x_t\n",
    "        \n",
    "        This tells us exactly what the optimal denoising step should predict!\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            x_0: Original clean data  \n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Optimal mean for the reverse step\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get ᾱ_t, ᾱ_{t-1}, α_t, β_t from noise schedule\n",
    "        # Step 2: Compute coefficient for x_0 term\n",
    "        # Step 3: Compute coefficient for x_t term\n",
    "        # Step 4: Verify coefficients sum to 1 (weighted average)\n",
    "        # Step 5: Return weighted combination\n",
    "        pass\n",
    "    \n",
    "    def optimal_reverse_variance(self, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement the optimal reverse variance\n",
    "        \n",
    "        Use the formula: σ̃²_t = (1-ᾱ_{t-1})/(1-ᾱ_t) * β_t\n",
    "        \n",
    "        This is fixed and requires no learning!\n",
    "        \n",
    "        Args:\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Optimal variance for the reverse step\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get ᾱ_t, ᾱ_{t-1}, β_t from noise schedule\n",
    "        # Step 2: Apply the variance formula\n",
    "        # Step 3: Return fixed variance (no parameters to learn!)\n",
    "        pass\n",
    "    \n",
    "    def demonstrate_interpolation_weights(self):\n",
    "        \"\"\"\n",
    "        Show how the interpolation weights change across timesteps\n",
    "        \"\"\"\n",
    "        print(\"=== Optimal Mean Interpolation Analysis ===\\n\")\n",
    "        \n",
    "        timesteps = torch.arange(1, self.noise_schedule['T'])\n",
    "        \n",
    "        # Compute weights for each timestep\n",
    "        weight_x0_list = []\n",
    "        weight_xt_list = []\n",
    "        \n",
    "        for t in timesteps:\n",
    "            # Extract parameters\n",
    "            alpha_t = self.alphas[t]\n",
    "            alpha_cumprod_t = self.alphas_cumprod[t]\n",
    "            alpha_cumprod_t_minus_1 = self.alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0)\n",
    "            beta_t = self.betas[t]\n",
    "            \n",
    "            # Compute weights\n",
    "            weight_x0 = (torch.sqrt(alpha_cumprod_t_minus_1) * beta_t) / (1 - alpha_cumprod_t)\n",
    "            weight_xt = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_minus_1)) / (1 - alpha_cumprod_t)\n",
    "            \n",
    "            weight_x0_list.append(weight_x0.item())\n",
    "            weight_xt_list.append(weight_xt.item())\n",
    "        \n",
    "        # Visualize weight evolution\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Individual weights\n",
    "        ax1.plot(timesteps, weight_x0_list, 'b-', linewidth=2, label='Weight for x₀ (clean)')\n",
    "        ax1.plot(timesteps, weight_xt_list, 'r-', linewidth=2, label='Weight for xₜ (noisy)')\n",
    "        ax1.set_xlabel('Timestep t')\n",
    "        ax1.set_ylabel('Weight')\n",
    "        ax1.set_title('Interpolation Weights')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Weight sum (should be 1)\n",
    "        weight_sums = [w0 + wt for w0, wt in zip(weight_x0_list, weight_xt_list)]\n",
    "        ax2.plot(timesteps, weight_sums, 'g-', linewidth=2)\n",
    "        ax2.axhline(y=1.0, color='k', linestyle='--', alpha=0.7, label='Expected sum = 1')\n",
    "        ax2.set_xlabel('Timestep t')\n",
    "        ax2.set_ylabel('Sum of Weights')\n",
    "        ax2.set_title('Weight Sum Verification')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Relative dominance\n",
    "        weight_ratios = [w0 / wt if wt > 0 else 0 for w0, wt in zip(weight_x0_list, weight_xt_list)]\n",
    "        ax3.plot(timesteps, weight_ratios, 'purple', linewidth=2)\n",
    "        ax3.axhline(y=1.0, color='k', linestyle='--', alpha=0.7, label='Equal weights')\n",
    "        ax3.set_xlabel('Timestep t')\n",
    "        ax3.set_ylabel('Weight Ratio (x₀/xₜ)')\n",
    "        ax3.set_title('Clean vs Noisy Dominance')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        ax3.set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Key insights:\")\n",
    "        print(\"• Early timesteps: Trust noisy observation more (small corruption)\")\n",
    "        print(\"• Late timesteps: Trust clean target more (heavy corruption)\")\n",
    "        print(\"• Weights always sum to 1 (perfect interpolation)\")\n",
    "        print(\"• Adaptive balancing based on noise level\")\n",
    "    \n",
    "    def visualize_reverse_distribution(self, x_0: torch.Tensor, timesteps_to_show: List[int] = [5, 15, 25, 35, 45]):\n",
    "        \"\"\"\n",
    "        Visualize the tractable reverse distribution at different timesteps\n",
    "        \"\"\"\n",
    "        print(\"=== Tractable Reverse Distribution Visualization ===\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, len(timesteps_to_show), figsize=(15, 8))\n",
    "        \n",
    "        # Create forward trajectory\n",
    "        demo = SequentialLikelihoodDemo(data_dim=2, T=50)\n",
    "        \n",
    "        for i, t in enumerate(timesteps_to_show):\n",
    "            try:\n",
    "                # Get x_t using direct jump\n",
    "                x_t = demo.direct_jump_forward(x_0, t)\n",
    "                \n",
    "                # Compute optimal reverse distribution\n",
    "                optimal_mean = self.optimal_reverse_mean(x_t, x_0, t)\n",
    "                optimal_var = self.optimal_reverse_variance(t)\n",
    "                \n",
    "                # Plot current state x_t\n",
    "                axes[0, i].scatter(x_t[0, 0].cpu(), x_t[0, 1].cpu(), c='red', s=100, label=f'xₜ (t={t})')\n",
    "                axes[0, i].scatter(x_0[0, 0].cpu(), x_0[0, 1].cpu(), c='blue', s=100, label='x₀ (clean)')\n",
    "                \n",
    "                if optimal_mean is not None:\n",
    "                    axes[0, i].scatter(optimal_mean[0, 0].cpu(), optimal_mean[0, 1].cpu(), \n",
    "                                     c='green', s=100, label='Optimal μ̃')\n",
    "                \n",
    "                axes[0, i].set_title(f'Timestep {t}')\n",
    "                axes[0, i].legend(fontsize=8)\n",
    "                axes[0, i].grid(True, alpha=0.3)\n",
    "                axes[0, i].set_xlim(-4, 4)\n",
    "                axes[0, i].set_ylim(-4, 4)\n",
    "                \n",
    "                # Plot variance evolution\n",
    "                if optimal_var is not None:\n",
    "                    axes[1, i].bar(0, optimal_var.item(), color='orange', alpha=0.7)\n",
    "                    axes[1, i].set_title(f'Variance: {optimal_var.item():.4f}')\n",
    "                    axes[1, i].set_ylim(0, 0.1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                axes[0, i].text(0.5, 0.5, 'Implement\\nTODOs first', ha='center', va='center')\n",
    "                axes[1, i].text(0.5, 0.5, 'Implement\\nTODOs first', ha='center', va='center')\n",
    "        \n",
    "        axes[0, 0].set_ylabel('Spatial Distribution')\n",
    "        axes[1, 0].set_ylabel('Optimal Variance')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Test tractable reverse distribution (uncomment after implementing TODOs)\n",
    "# reverse_dist = TractableReverseDistribution(noise_schedule)\n",
    "# reverse_dist.demonstrate_interpolation_weights()\n",
    "# reverse_dist.visualize_reverse_distribution(test_data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659018c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: The Noise Prediction Reparameterization (20 minutes)\n",
    "\n",
    "### Task 4.1: Implement the Noise Prediction Breakthrough\n",
    "\n",
    "**Your Mission**: Implement the reparameterization that transforms diffusion training from image prediction to noise prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea20565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisePredictionReparameterization:\n",
    "    \"\"\"\n",
    "    Implement the noise prediction reparameterization.\n",
    "    This transforms complex denoising into simple noise prediction!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noise_schedule: Dict[str, torch.Tensor]):\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.betas = noise_schedule['betas']\n",
    "        self.alphas = noise_schedule['alphas']\n",
    "        self.alphas_cumprod = noise_schedule['alphas_cumprod']\n",
    "    \n",
    "    def forward_process_with_noise(self, x_0: torch.Tensor, t: int, epsilon: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement forward process with explicit noise tracking\n",
    "        \n",
    "        Use: x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε, where ε ~ N(0,I)\n",
    "        \n",
    "        This explicit form will enable noise prediction training.\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean data\n",
    "            t: Timestep\n",
    "            epsilon: Noise vector (if None, sample new noise)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (x_t, epsilon_used)\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Sample noise ε ~ N(0,I) if not provided\n",
    "        # Step 2: Get ᾱ_t from noise schedule\n",
    "        # Step 3: Compute mean coefficient: √ᾱ_t\n",
    "        # Step 4: Compute noise coefficient: √(1-ᾱ_t)\n",
    "        # Step 5: Apply the formula: x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε\n",
    "        # Step 6: Return both x_t and the noise used\n",
    "        pass\n",
    "    \n",
    "    def solve_for_x0(self, x_t: torch.Tensor, epsilon: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement solving for x_0 given x_t and noise\n",
    "        \n",
    "        From x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε\n",
    "        Solve: x_0 = (x_t - √(1-ᾱ_t) * ε) / √ᾱ_t\n",
    "        \n",
    "        This shows how knowing the noise allows us to recover clean data!\n",
    "        \n",
    "        Args:\n",
    "            x_t: Noisy observation\n",
    "            epsilon: Noise vector\n",
    "            t: Timestep\n",
    "            \n",
    "        Returns:\n",
    "            Recovered x_0\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get ᾱ_t from noise schedule\n",
    "        # Step 2: Apply the inversion formula\n",
    "        # Step 3: Return recovered x_0\n",
    "        pass\n",
    "    \n",
    "    def reparameterize_optimal_mean(self, x_t: torch.Tensor, epsilon: torch.Tensor, t: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement noise-parameterized optimal mean\n",
    "        \n",
    "        Transform the optimal mean formula to use noise instead of x_0:\n",
    "        μ̃_t(x_t, ε) = (1/√α_t) * (x_t - (1-α_t)/√(1-ᾱ_t) * ε)\n",
    "        \n",
    "        This is the key insight: optimal denoising = noise prediction + simple arithmetic!\n",
    "        \n",
    "        Args:\n",
    "            x_t: Current noisy state\n",
    "            epsilon: True noise that was added\n",
    "            t: Current timestep\n",
    "            \n",
    "        Returns:\n",
    "            Optimal mean in terms of noise\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get α_t and ᾱ_t from noise schedule\n",
    "        # Step 2: Compute the noise coefficient: (1-α_t)/√(1-ᾱ_t)\n",
    "        # Step 3: Compute the scaling factor: 1/√α_t\n",
    "        # Step 4: Apply the reparameterized formula\n",
    "        # Step 5: Return noise-parameterized optimal mean\n",
    "        pass\n",
    "    \n",
    "    def demonstrate_noise_prediction_equivalence(self, x_0: torch.Tensor, t: int):\n",
    "        \"\"\"\n",
    "        Demonstrate that noise prediction is equivalent to optimal denoising\n",
    "        \"\"\"\n",
    "        print(f\"=== Noise Prediction Equivalence Demo (t={t}) ===\\n\")\n",
    "        \n",
    "        # Generate noisy sample with known noise\n",
    "        try:\n",
    "            x_t, true_epsilon = self.forward_process_with_noise(x_0, t)\n",
    "            \n",
    "            print(f\"Original x_0: {x_0.squeeze().cpu().numpy()}\")\n",
    "            print(f\"Noisy x_t: {x_t.squeeze().cpu().numpy()}\")\n",
    "            print(f\"True noise ε: {true_epsilon.squeeze().cpu().numpy()}\")\n",
    "            \n",
    "            # Method 1: Direct optimal mean (using x_0)\n",
    "            reverse_dist = TractableReverseDistribution(self.noise_schedule)\n",
    "            optimal_mean_direct = reverse_dist.optimal_reverse_mean(x_t, x_0, t)\n",
    "            \n",
    "            # Method 2: Noise-parameterized optimal mean\n",
    "            optimal_mean_noise = self.reparameterize_optimal_mean(x_t, true_epsilon, t)\n",
    "            \n",
    "            if optimal_mean_direct is not None and optimal_mean_noise is not None:\n",
    "                print(f\"Optimal mean (direct): {optimal_mean_direct.squeeze().cpu().numpy()}\")\n",
    "                print(f\"Optimal mean (noise): {optimal_mean_noise.squeeze().cpu().numpy()}\")\n",
    "                \n",
    "                # Check equivalence\n",
    "                difference = torch.abs(optimal_mean_direct - optimal_mean_noise).max()\n",
    "                print(f\"Maximum difference: {difference.item():.8f}\")\n",
    "                print(f\"Equivalent: {torch.allclose(optimal_mean_direct, optimal_mean_noise, atol=1e-6)}\")\n",
    "            \n",
    "            # Test noise recovery\n",
    "            recovered_x0 = self.solve_for_x0(x_t, true_epsilon, t)\n",
    "            if recovered_x0 is not None:\n",
    "                recovery_error = torch.abs(x_0 - recovered_x0).max()\n",
    "                print(f\"x_0 recovery error: {recovery_error.item():.8f}\")\n",
    "            \n",
    "        except:\n",
    "            print(\"Implement the TODO methods first\")\n",
    "    \n",
    "    def visualize_noise_prediction_training(self):\n",
    "        \"\"\"\n",
    "        Visualize how noise prediction training works\n",
    "        \"\"\"\n",
    "        print(\"=== Noise Prediction Training Visualization ===\\n\")\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        \n",
    "        # Take a sample point\n",
    "        x_0 = test_data[0:1]\n",
    "        timesteps = [10, 20, 30, 40]\n",
    "        \n",
    "        for i, t in enumerate(timesteps):\n",
    "            try:\n",
    "                # Generate training sample\n",
    "                x_t, true_epsilon = self.forward_process_with_noise(x_0, t)\n",
    "                \n",
    "                # Show the training data\n",
    "                axes[0, i].scatter(x_0[0, 0].cpu(), x_0[0, 1].cpu(), c='blue', s=100, label='x₀ (clean)')\n",
    "                axes[0, i].scatter(x_t[0, 0].cpu(), x_t[0, 1].cpu(), c='red', s=100, label='xₜ (noisy)')\n",
    "                \n",
    "                # Show the noise vector\n",
    "                axes[0, i].arrow(x_t[0, 0].cpu(), x_t[0, 1].cpu(), \n",
    "                               true_epsilon[0, 0].cpu(), true_epsilon[0, 1].cpu(),\n",
    "                               head_width=0.1, head_length=0.1, fc='green', ec='green', alpha=0.7)\n",
    "                axes[0, i].text(x_t[0, 0].cpu() + true_epsilon[0, 0].cpu()/2, \n",
    "                               x_t[0, 1].cpu() + true_epsilon[0, 1].cpu()/2, \n",
    "                               'ε', fontsize=14, color='green', weight='bold')\n",
    "                \n",
    "                axes[0, i].set_title(f't={t}')\n",
    "                axes[0, i].legend(fontsize=8)\n",
    "                axes[0, i].grid(True, alpha=0.3)\n",
    "                axes[0, i].set_xlim(-4, 4)\n",
    "                axes[0, i].set_ylim(-4, 4)\n",
    "                \n",
    "                # Show noise magnitude over time\n",
    "                noise_magnitude = torch.norm(true_epsilon).item()\n",
    "                axes[1, i].bar(0, noise_magnitude, color='green', alpha=0.7)\n",
    "                axes[1, i].set_title(f'||ε||: {noise_magnitude:.3f}')\n",
    "                axes[1, i].set_ylim(0, 3)\n",
    "                \n",
    "            except:\n",
    "                axes[0, i].text(0.5, 0.5, 'Implement\\nTODOs first', ha='center', va='center')\n",
    "                axes[1, i].text(0.5, 0.5, 'Implement\\nTODOs first', ha='center', va='center')\n",
    "        \n",
    "        axes[0, 0].set_ylabel('Training Sample')\n",
    "        axes[1, 0].set_ylabel('Noise Magnitude')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Training insight:\")\n",
    "        print(\"• Network learns: ε_θ(x_t, t) ≈ ε\")\n",
    "        print(\"• Loss: ||ε - ε_θ(x_t, t)||²\")\n",
    "        print(\"• Simple MSE on noise prediction!\")\n",
    "\n",
    "# Test noise prediction reparameterization (uncomment after implementing TODOs)\n",
    "# noise_reparam = NoisePredictionReparameterization(noise_schedule)\n",
    "\n",
    "# # Test equivalence\n",
    "# for t in [5, 15, 25, 35]:\n",
    "#     noise_reparam.demonstrate_noise_prediction_equivalence(test_data[0:1], t)\n",
    "#     print()\n",
    "\n",
    "# noise_reparam.visualize_noise_prediction_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074f96d",
   "metadata": {},
   "source": [
    "### Task 4.2: Implement the Simple Training Algorithm\n",
    "\n",
    "**Your Mission**: Transform the complex ELBO into simple noise prediction training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b280287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDiffusionTraining:\n",
    "    \"\"\"\n",
    "    Implement the simplified diffusion training algorithm.\n",
    "    Show how complex ELBO theory reduces to elegant practice.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noise_schedule: Dict[str, torch.Tensor], data_dim: int = 2):\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.T = noise_schedule['T']\n",
    "        self.data_dim = data_dim\n",
    "        \n",
    "        # Simple noise prediction network\n",
    "        self.noise_network = nn.Sequential(\n",
    "            nn.Linear(data_dim + 1, 64),  # +1 for timestep embedding\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, data_dim)  # Predict noise\n",
    "        ).to(device)\n",
    "        \n",
    "        self.noise_reparam = NoisePredictionReparameterization(noise_schedule)\n",
    "    \n",
    "    def timestep_embedding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simple timestep embedding (just normalized timestep)\n",
    "        \"\"\"\n",
    "        return (t.float() / self.T).unsqueeze(-1)\n",
    "    \n",
    "    def predict_noise(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Predict noise using the network\n",
    "        \n",
    "        Args:\n",
    "            x_t: Noisy input\n",
    "            t: Timestep\n",
    "            \n",
    "        Returns:\n",
    "            Predicted noise ε_θ(x_t, t)\n",
    "        \"\"\"\n",
    "        # Embed timestep\n",
    "        t_embed = self.timestep_embedding(t)\n",
    "        \n",
    "        # Concatenate input and timestep\n",
    "        network_input = torch.cat([x_t, t_embed.expand(x_t.shape[0], -1)], dim=-1)\n",
    "        \n",
    "        # Predict noise\n",
    "        epsilon_pred = self.noise_network(network_input)\n",
    "        return epsilon_pred\n",
    "    \n",
    "    def simple_diffusion_loss(self, x_0: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement the simple diffusion training loss\n",
    "        \n",
    "        Algorithm:\n",
    "        1. Sample random timestep t\n",
    "        2. Sample noise ε ~ N(0,I)\n",
    "        3. Compute x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε\n",
    "        4. Predict ε_θ(x_t, t)\n",
    "        5. Compute loss ||ε - ε_θ(x_t, t)||²\n",
    "        \n",
    "        This is the elegant simplification of the complex ELBO!\n",
    "        \n",
    "        Args:\n",
    "            x_0: Batch of clean data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with loss components\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Sample random timesteps for each sample in batch\n",
    "        # Step 2: Sample noise vectors ε ~ N(0,I)\n",
    "        # Step 3: Create noisy samples x_t using forward_process_with_noise\n",
    "        # Step 4: Predict noise using the network\n",
    "        # Step 5: Compute MSE loss between true and predicted noise\n",
    "        # Step 6: Return loss and auxiliary information\n",
    "        pass\n",
    "    \n",
    "    def train_simple_diffusion(self, data: torch.Tensor, epochs: int = 100, lr: float = 1e-3):\n",
    "        \"\"\"\n",
    "        Train diffusion model using simple noise prediction\n",
    "        \"\"\"\n",
    "        print(\"=== Training Simple Diffusion Model ===\\n\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.noise_network.parameters(), lr=lr)\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training step\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute loss (after students implement simple_diffusion_loss)\n",
    "            try:\n",
    "                loss_dict = self.simple_diffusion_loss(data)\n",
    "                if loss_dict is not None:\n",
    "                    loss = loss_dict['loss']\n",
    "                    \n",
    "                    # Backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    losses.append(loss.item())\n",
    "                    \n",
    "                    if epoch % 20 == 0:\n",
    "                        print(f\"Epoch {epoch}: Loss = {loss.item():.6f}\")\n",
    "                else:\n",
    "                    print(\"Implement simple_diffusion_loss first\")\n",
    "                    break\n",
    "            except:\n",
    "                print(\"Implement simple_diffusion_loss first\")\n",
    "                break\n",
    "        \n",
    "        # Plot training curve\n",
    "        if losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(losses, 'b-', linewidth=2)\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Noise Prediction Loss')\n",
    "            plt.title('Simple Diffusion Training')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "            print(\"Training complete!\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def test_noise_prediction_accuracy(self, test_data: torch.Tensor, n_samples: int = 10):\n",
    "        \"\"\"\n",
    "        Test how well the trained network predicts noise\n",
    "        \"\"\"\n",
    "        print(\"=== Testing Noise Prediction Accuracy ===\\n\")\n",
    "        \n",
    "        self.noise_network.eval()\n",
    "        total_error = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(n_samples):\n",
    "                # Sample random test case\n",
    "                x_0 = test_data[i:i+1]\n",
    "                t = torch.randint(1, self.T, (1,)).to(device)\n",
    "                \n",
    "                # Generate noisy sample with known noise\n",
    "                try:\n",
    "                    x_t, true_epsilon = self.noise_reparam.forward_process_with_noise(x_0, t.item())\n",
    "                    \n",
    "                    # Predict noise\n",
    "                    pred_epsilon = self.predict_noise(x_t, t)\n",
    "                    \n",
    "                    # Compute error\n",
    "                    error = torch.mse_loss(pred_epsilon, true_epsilon)\n",
    "                    total_error += error.item()\n",
    "                    \n",
    "                    if i < 3:  # Show first few examples\n",
    "                        print(f\"Sample {i+1}: t={t.item()}\")\n",
    "                        print(f\"  True noise: {true_epsilon.squeeze().cpu().numpy()}\")\n",
    "                        print(f\"  Pred noise: {pred_epsilon.squeeze().cpu().numpy()}\")\n",
    "                        print(f\"  MSE error: {error.item():.6f}\")\n",
    "                        print()\n",
    "                except:\n",
    "                    print(\"Implement forward_process_with_noise first\")\n",
    "                    return\n",
    "        \n",
    "        avg_error = total_error / n_samples\n",
    "        print(f\"Average noise prediction error: {avg_error:.6f}\")\n",
    "        \n",
    "        self.noise_network.train()\n",
    "    \n",
    "    def demonstrate_elbo_to_practice_connection(self):\n",
    "        \"\"\"\n",
    "        Show the connection between ELBO theory and practical training\n",
    "        \"\"\"\n",
    "        print(\"=== ELBO Theory → Practice Connection ===\\n\")\n",
    "        \n",
    "        connections = [\n",
    "            (\"Complex ELBO\", \"Simple Practice\"),\n",
    "            (\"E[log p_θ(x_0|x_1)]\", \"Reconstruction handled automatically\"),\n",
    "            (\"KL(q(x_T|x_0) || p(x_T))\", \"≈ 0 by design (no learning needed)\"),\n",
    "            (\"Σ KL(q(x_{t-1}|x_t,x_0) || p_θ(x_{t-1}|x_t))\", \"||ε - ε_θ(x_t,t)||² (noise prediction)\"),\n",
    "            (\"Intractable optimization\", \"Simple MSE training\"),\n",
    "            (\"Complex mathematics\", \"Elegant algorithm\")\n",
    "        ]\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "        \n",
    "        y_positions = [0.85, 0.7, 0.55, 0.4, 0.25, 0.1]\n",
    "        \n",
    "        for i, (theory, practice) in enumerate(connections):\n",
    "            y = y_positions[i]\n",
    "            \n",
    "            # Theory side\n",
    "            ax.text(0.15, y, theory, ha='center', va='center', fontsize=11,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightblue', alpha=0.8))\n",
    "            \n",
    "            # Arrow\n",
    "            ax.arrow(0.4, y, 0.2, 0, head_width=0.02, head_length=0.03,\n",
    "                    fc='black', ec='black')\n",
    "            \n",
    "            # Practice side\n",
    "            ax.text(0.85, y, practice, ha='center', va='center', fontsize=11,\n",
    "                   bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='lightgreen', alpha=0.8))\n",
    "        \n",
    "        ax.text(0.15, 0.95, 'ELBO Theory', ha='center', fontsize=14, weight='bold', color='blue')\n",
    "        ax.text(0.85, 0.95, 'Practical Training', ha='center', fontsize=14, weight='bold', color='green')\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('The Mathematical Bridge: From Theory to Practice', fontsize=16, weight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"The profound achievement:\")\n",
    "        print(\"🎯 Complex ELBO mathematics → Simple MSE training\")\n",
    "        print(\"🚀 Intractable optimization → Practical algorithm\")\n",
    "        print(\"💎 Theoretical elegance → State-of-the-art results\")\n",
    "\n",
    "# Test simple diffusion training (uncomment after implementing TODOs)\n",
    "# simple_trainer = SimpleDiffusionTraining(noise_schedule, data_dim=2)\n",
    "# simple_trainer.demonstrate_elbo_to_practice_connection()\n",
    "\n",
    "# # Train on subset of data\n",
    "# losses = simple_trainer.train_simple_diffusion(test_data[:50], epochs=50)\n",
    "\n",
    "# # Test noise prediction\n",
    "# simple_trainer.test_noise_prediction_accuracy(test_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b5ccb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: The Three Forces Analysis (15 minutes)\n",
    "\n",
    "### Task 5.1: Implement the Three Forces Decomposition\n",
    "\n",
    "**Your Mission**: Analyze the three forces that shape diffusion learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83224bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeForcesAnalysis:\n",
    "    \"\"\"\n",
    "    Implement analysis of the three forces in diffusion ELBO:\n",
    "    1. Reconstruction: E[log p_θ(x_0|x_1)]\n",
    "    2. Prior Matching: KL(q(x_T|x_0) || p(x_T))\n",
    "    3. Denoising: Σ KL(q(x_{t-1}|x_t,x_0) || p_θ(x_{t-1}|x_t))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, noise_schedule: Dict[str, torch.Tensor]):\n",
    "        self.noise_schedule = noise_schedule\n",
    "        self.T = noise_schedule['T']\n",
    "        self.reverse_dist = TractableReverseDistribution(noise_schedule)\n",
    "        self.noise_reparam = NoisePredictionReparameterization(noise_schedule)\n",
    "    \n",
    "    def compute_reconstruction_term(self, x_0: torch.Tensor, x_1: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement reconstruction term E[log p_θ(x_0|x_1)]\n",
    "        \n",
    "        This measures how well we can recover original data from slight noise.\n",
    "        Model: p_θ(x_0|x_1) = N(x_0; μ_θ(x_1), σ²I)\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean data\n",
    "            x_1: Slightly noisy data\n",
    "            \n",
    "        Returns:\n",
    "            Reconstruction log-likelihood\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: For simplicity, assume the optimal mean (would be learned in practice)\n",
    "        # Step 2: Use reverse distribution to get optimal mean\n",
    "        # Step 3: Compute Gaussian log-likelihood\n",
    "        # Step 4: Return reconstruction term\n",
    "        pass\n",
    "    \n",
    "    def compute_prior_matching_term(self, x_0: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        TODO: Implement prior matching term KL(q(x_T|x_0) || p(x_T))\n",
    "        \n",
    "        This measures how well the forward endpoint matches pure noise.\n",
    "        Should be ≈ 0 for well-designed noise schedules!\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean data\n",
    "            \n",
    "        Returns:\n",
    "            KL divergence to prior\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # Step 1: Get x_T distribution parameters: q(x_T|x_0) = N(√ᾱ_T x_0, (1-ᾱ_T)I)\n",
    "        # Step 2: Prior is p(x_T) = N(0, I)\n",
    "        # Step 3: Compute KL divergence between these Gaussians\n",
    "        # Step 4: Should be very small when ᾱ_T ≈ 0\n",
    "        pass\n",
    "    \n",
    "    def compute_denoising_terms(self, x_0: torch.Tensor, timesteps: List[int]) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        TODO: Implement denoising terms KL(q(x_{t-1}|x_t,x_0) || p_θ(x_{t-1}|x_t))\n",
    "        \n",
    "        These are the heart of diffusion learning - matching optimal and learned reverse steps.\n",
    "        \n",
    "        Args:\n",
    "            x_0: Clean data\n",
    "            timesteps: List of timesteps to analyze\n",
    "            \n",
    "        Returns:\n",
    "            List of KL divergences for each timestep\n",
    "        \"\"\"\n",
    "        # TODO: Your implementation here\n",
    "        # For each timestep t:\n",
    "        # Step 1: Generate x_t using direct jump\n",
    "        # Step 2: Compute optimal reverse distribution q(x_{t-1}|x_t,x_0)\n",
    "        # Step 3: For demonstration, use a simple learned model p_θ(x_{t-1}|x_t)\n",
    "        # Step 4: Compute KL divergence between optimal and learned\n",
    "        # Step 5: Return list of KL values\n",
    "        pass\n",
    "    \n",
    "    def analyze_three_forces(self, x_0: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Complete analysis of the three forces\n",
    "        \"\"\"\n",
    "        print(\"=== Three Forces Analysis ===\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Force 1: Reconstruction\n",
    "            x_1, _ = self.noise_reparam.forward_process_with_noise(x_0, 1)\n",
    "            recon_term = self.compute_reconstruction_term(x_0, x_1)\n",
    "            \n",
    "            # Force 2: Prior matching\n",
    "            prior_term = self.compute_prior_matching_term(x_0)\n",
    "            \n",
    "            # Force 3: Denoising (sample a few timesteps)\n",
    "            sample_timesteps = [5, 15, 25, 35, 45]\n",
    "            denoising_terms = self.compute_denoising_terms(x_0, sample_timesteps)\n",
    "            \n",
    "            if all(term is not None for term in [recon_term, prior_term] + denoising_terms):\n",
    "                print(\"Force 1 - Reconstruction:\")\n",
    "                print(f\"  E[log p_θ(x_0|x_1)] = {recon_term.mean().item():.6f}\")\n",
    "                print(\"  What it does: Ensures perfect recovery from slight noise\")\n",
    "                print(\"  Intuition: 'Given a slightly grainy photo, restore it perfectly'\")\n",
    "                print()\n",
    "                \n",
    "                print(\"Force 2 - Prior Matching:\")\n",
    "                print(f\"  KL(q(x_T|x_0) || p(x_T)) = {prior_term.mean().item():.6f}\")\n",
    "                print(\"  What it does: Ensures forward endpoint matches pure noise\")\n",
    "                print(\"  Beautiful insight: Should be ≈ 0 by design!\")\n",
    "                print()\n",
    "                \n",
    "                print(\"Force 3 - Denoising:\")\n",
    "                for i, (t, kl_val) in enumerate(zip(sample_timesteps, denoising_terms)):\n",
    "                    print(f\"  t={t}: KL = {kl_val.mean().item():.6f}\")\n",
    "                print(\"  What it does: Learns optimal reverse steps\")\n",
    "                print(\"  This is where the actual learning happens!\")\n",
    "                \n",
    "                # Visualize force magnitudes\n",
    "                self.visualize_three_forces(recon_term, prior_term, denoising_terms, sample_timesteps)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Implement the TODO methods first\")\n",
    "    \n",
    "    def visualize_three_forces(self, recon_term, prior_term, denoising_terms, timesteps):\n",
    "        \"\"\"\n",
    "        Visualize the relative magnitudes of the three forces\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Plot 1: Force magnitudes\n",
    "        forces = ['Reconstruction', 'Prior Matching', 'Avg Denoising']\n",
    "        magnitudes = [\n",
    "            -recon_term.mean().item(),  # Negative because it's a likelihood term\n",
    "            prior_term.mean().item(),\n",
    "            sum(term.mean().item() for term in denoising_terms) / len(denoising_terms)\n",
    "        ]\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        bars = ax1.bar(forces, magnitudes, color=colors, alpha=0.7)\n",
    "        ax1.set_ylabel('Magnitude')\n",
    "        ax1.set_title('Three Forces Magnitude Comparison')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, mag in zip(bars, magnitudes):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{mag:.4f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Plot 2: Denoising terms across timesteps\n",
    "        denoising_mags = [term.mean().item() for term in denoising_terms]\n",
    "        ax2.plot(timesteps, denoising_mags, 'go-', linewidth=2, markersize=8)\n",
    "        ax2.set_xlabel('Timestep t')\n",
    "        ax2.set_ylabel('KL Divergence')\n",
    "        ax2.set_title('Denoising Force Across Timesteps')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def demonstrate_force_balance(self):\n",
    "        \"\"\"\n",
    "        Show how the three forces balance in the complete ELBO\n",
    "        \"\"\"\n",
    "        print(\"=== Three Forces Balance ===\\n\")\n",
    "        \n",
    "        # Create conceptual visualization\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Draw the three forces as a triangle\n",
    "        force_positions = {\n",
    "            'Reconstruction': (0.5, 0.8),\n",
    "            'Prior Matching': (0.2, 0.3),\n",
    "            'Denoising': (0.8, 0.3)\n",
    "        }\n",
    "        \n",
    "        force_colors = {\n",
    "            'Reconstruction': 'blue',\n",
    "            'Prior Matching': 'red', \n",
    "            'Denoising': 'green'\n",
    "        }\n",
    "        \n",
    "        force_descriptions = {\n",
    "            'Reconstruction': 'E[log pθ(x₀|x₁)]\\n• Perfect recovery\\n• Final output quality\\n• Single timestep',\n",
    "            'Prior Matching': 'KL(q(xT|x₀) || p(xT))\\n• Endpoint = noise\\n• Free by design\\n• No learning needed',\n",
    "            'Denoising': 'Σ KL(q(x_{t-1}|xt,x₀) || pθ(x_{t-1}|xt))\\n• Heart of learning\\n• T-1 terms\\n• Most computation'\n",
    "        }\n",
    "        \n",
    "        # Draw force nodes\n",
    "        for force, (x, y) in force_positions.items():\n",
    "            color = force_colors[force]\n",
    "            description = force_descriptions[force]\n",
    "            \n",
    "            # Force circle\n",
    "            circle = plt.Circle((x, y), 0.08, color=color, alpha=0.7)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Force label\n",
    "            ax.text(x, y, force.split()[0], ha='center', va='center', \n",
    "                   fontsize=10, weight='bold', color='white')\n",
    "            \n",
    "            # Description box\n",
    "            if force == 'Reconstruction':\n",
    "                desc_pos = (x, y + 0.15)\n",
    "            elif force == 'Prior Matching':\n",
    "                desc_pos = (x - 0.15, y - 0.15)\n",
    "            else:  # Denoising\n",
    "                desc_pos = (x + 0.15, y - 0.15)\n",
    "            \n",
    "            ax.text(desc_pos[0], desc_pos[1], description, ha='center', va='center',\n",
    "                   fontsize=9, bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=color, alpha=0.3))\n",
    "        \n",
    "        # Draw connections showing balance\n",
    "        connections = [\n",
    "            (force_positions['Reconstruction'], force_positions['Prior Matching']),\n",
    "            (force_positions['Prior Matching'], force_positions['Denoising']),\n",
    "            (force_positions['Denoising'], force_positions['Reconstruction'])\n",
    "        ]\n",
    "        \n",
    "        for (x1, y1), (x2, y2) in connections:\n",
    "            ax.plot([x1, x2], [y1, y2], 'k--', alpha=0.5, linewidth=2)\n",
    "        \n",
    "        # Central balance point\n",
    "        center_x = sum(pos[0] for pos in force_positions.values()) / 3\n",
    "        center_y = sum(pos[1] for pos in force_positions.values()) / 3\n",
    "        ax.plot(center_x, center_y, 'ko', markersize=10)\n",
    "        ax.text(center_x, center_y - 0.1, 'ELBO\\nBalance', ha='center', va='center',\n",
    "               fontsize=12, weight='bold')\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_title('The Three Forces of Diffusion Learning', fontsize=16, weight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Key insights:\")\n",
    "        print(\"🎯 Reconstruction: Quality control for final output\")\n",
    "        print(\"🎁 Prior Matching: Free by construction (smart design)\")\n",
    "        print(\"💪 Denoising: Where the learning happens (T-1 terms)\")\n",
    "        print(\"⚖️  Balance: All three forces shape the final model\")\n",
    "\n",
    "# Test three forces analysis (uncomment after implementing TODOs)\n",
    "# three_forces = ThreeForcesAnalysis(noise_schedule)\n",
    "# three_forces.demonstrate_force_balance()\n",
    "# three_forces.analyze_three_forces(test_data[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358f116",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Mathematical Validation and Connection (10 minutes)\n",
    "\n",
    "### Task 6.1: Validate Complete Implementation\n",
    "\n",
    "**Your Mission**: Test that all mathematical components work together correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04182fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_diffusion_validation():\n",
    "    \"\"\"\n",
    "    Comprehensive validation of all diffusion ELBO components\n",
    "    \"\"\"\n",
    "    print(\"=== Comprehensive Diffusion Validation ===\\n\")\n",
    "    \n",
    "    # Test 1: Forward process consistency\n",
    "    print(\"Test 1: Forward Process Consistency\")\n",
    "    demo = SequentialLikelihoodDemo(data_dim=2, T=50)\n",
    "    x_0 = test_data[0:1]\n",
    "    \n",
    "    try:\n",
    "        # Sequential vs direct jump should be equivalent in distribution\n",
    "        trajectory = demo.forward_trajectory(x_0)\n",
    "        direct_x_T = demo.direct_jump_forward(x_0, 49)\n",
    "        \n",
    "        if trajectory is not None and direct_x_T is not None:\n",
    "            seq_x_T = trajectory[-1]\n",
    "            print(f\"Sequential x_T: {seq_x_T.squeeze().cpu().numpy()}\")\n",
    "            print(f\"Direct jump x_T: {direct_x_T.squeeze().cpu().numpy()}\")\n",
    "            print(\"✓ Forward process implementations consistent\")\n",
    "        else:\n",
    "            print(\"❌ Implement forward process methods\")\n",
    "    except:\n",
    "        print(\"❌ Forward process implementation needed\")\n",
    "    \n",
    "    # Test 2: Noise prediction equivalence\n",
    "    print(f\"\\nTest 2: Noise Prediction Equivalence\")\n",
    "    noise_reparam = NoisePredictionReparameterization(noise_schedule)\n",
    "    reverse_dist = TractableReverseDistribution(noise_schedule)\n",
    "    \n",
    "    try:\n",
    "        t = 20\n",
    "        x_t, epsilon = noise_reparam.forward_process_with_noise(x_0, t)\n",
    "        \n",
    "        # Two ways to compute optimal mean\n",
    "        mean_direct = reverse_dist.optimal_reverse_mean(x_t, x_0, t)\n",
    "        mean_noise = noise_reparam.reparameterize_optimal_mean(x_t, epsilon, t)\n",
    "        \n",
    "        if mean_direct is not None and mean_noise is not None:\n",
    "            diff = torch.abs(mean_direct - mean_noise).max()\n",
    "            print(f\"Mean difference: {diff.item():.8f}\")\n",
    "            print(\"✓ Noise prediction reparameterization correct\" if diff < 1e-6 else \"❌ Reparameterization error\")\n",
    "        else:\n",
    "            print(\"❌ Implement optimal mean methods\")\n",
    "    except:\n",
    "        print(\"❌ Noise prediction implementation needed\")\n",
    "    \n",
    "    # Test 3: Three forces implementation\n",
    "    print(f\"\\nTest 3: Three Forces Components\")\n",
    "    three_forces = ThreeForcesAnalysis(noise_schedule)\n",
    "    \n",
    "    try:\n",
    "        # Test that prior matching is small\n",
    "        prior_kl = three_forces.compute_prior_matching_term(x_0)\n",
    "        if prior_kl is not None:\n",
    "            print(f\"Prior matching KL: {prior_kl.mean().item():.6f}\")\n",
    "            print(\"✓ Prior matching small\" if prior_kl.mean() < 0.1 else \"❌ Prior matching too large\")\n",
    "        else:\n",
    "            print(\"❌ Implement prior matching term\")\n",
    "    except:\n",
    "        print(\"❌ Three forces implementation needed\")\n",
    "    \n",
    "    # Test 4: Training algorithm\n",
    "    print(f\"\\nTest 4: Training Algorithm\")\n",
    "    simple_trainer = SimpleDiffusionTraining(noise_schedule, data_dim=2)\n",
    "    \n",
    "    try:\n",
    "        # Test loss computation\n",
    "        loss_dict = simple_trainer.simple_diffusion_loss(test_data[:5])\n",
    "        if loss_dict is not None:\n",
    "            print(f\"Training loss: {loss_dict['loss'].item():.6f}\")\n",
    "            print(\"✓ Training algorithm implemented\")\n",
    "        else:\n",
    "            print(\"❌ Implement training loss\")\n",
    "    except:\n",
    "        print(\"❌ Training algorithm implementation needed\")\n",
    "    \n",
    "    print(f\"\\n🎓 Validation Summary:\")\n",
    "    print(\"• Forward process: Essential for noise scheduling\")\n",
    "    print(\"• Noise prediction: Key insight for practical training\")\n",
    "    print(\"• Three forces: Mathematical foundation of ELBO\")\n",
    "    print(\"• Training: Bridge from theory to practice\")\n",
    "\n",
    "def demonstrate_elbo_power():\n",
    "    \"\"\"\n",
    "    Demonstrate the power of the ELBO framework\n",
    "    \"\"\"\n",
    "    print(\"=== The Power of ELBO Framework ===\\n\")\n",
    "    \n",
    "    achievements = [\n",
    "        \"🚫 Intractable Likelihood Problem\",\n",
    "        \"🧮 Complex Sequential Dependencies\", \n",
    "        \"📐 Variational Inference Solution\",\n",
    "        \"⚡ Tractable Lower Bound\",\n",
    "        \"🔧 Three Interpretable Forces\",\n",
    "        \"🎯 Simple Noise Prediction\",\n",
    "        \"🚀 State-of-the-Art Results\"\n",
    "    ]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Create a flow diagram\n",
    "    y_positions = [0.9, 0.8, 0.65, 0.5, 0.35, 0.2, 0.05]\n",
    "    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green', 'blue', 'purple']\n",
    "    \n",
    "    for i, (achievement, y, color) in enumerate(zip(achievements, y_positions, colors)):\n",
    "        # Achievement box\n",
    "        box_style = \"round,pad=0.5\" if i != 2 else \"round,pad=0.5\"  # Highlight ELBO\n",
    "        ax.text(0.5, y, achievement, ha='center', va='center', fontsize=12, weight='bold',\n",
    "               bbox=dict(boxstyle=box_style, facecolor=color, alpha=0.8))\n",
    "        \n",
    "        # Arrow to next achievement\n",
    "        if i < len(achievements) - 1:\n",
    "            ax.arrow(0.5, y - 0.05, 0, -0.05, head_width=0.02, head_length=0.01,\n",
    "                    fc='gray', ec='gray', alpha=0.7)\n",
    "    \n",
    "    # Add side annotations\n",
    "    annotations = [\n",
    "        (0.15, 0.9, \"Mathematical\\nChallenge\"),\n",
    "        (0.85, 0.65, \"ELBO\\nBreakthrough\"),\n",
    "        (0.15, 0.35, \"Practical\\nImplementation\"),\n",
    "        (0.85, 0.05, \"Real-World\\nSuccess\")\n",
    "    ]\n",
    "    \n",
    "    for x, y, text in annotations:\n",
    "        ax.text(x, y, text, ha='center', va='center', fontsize=10,\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='lightblue', alpha=0.5))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('ELBO: From Mathematical Challenge to Practical Success', fontsize=14, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run comprehensive validation\n",
    "comprehensive_diffusion_validation()\n",
    "demonstrate_elbo_power()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82becd7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Connection to Modern Diffusion Models (5 minutes)\n",
    "\n",
    "### Task 7.1: Bridge to State-of-the-Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_modern_diffusion():\n",
    "    \"\"\"\n",
    "    Connect lab implementations to modern diffusion models\n",
    "    \"\"\"\n",
    "    print(\"=== Connection to Modern Diffusion Models ===\\n\")\n",
    "    \n",
    "    lab_to_sota = [\n",
    "        (\"Our Implementation\", \"State-of-the-Art Models\"),\n",
    "        (\"Simple 2D data\", \"High-resolution images (1024×1024)\"),\n",
    "        (\"Linear noise schedule\", \"Cosine/learned schedules\"),\n",
    "        (\"Basic MLP network\", \"U-Net architectures with attention\"),\n",
    "        (\"T=50 timesteps\", \"T=1000+ timesteps\"),\n",
    "        (\"MSE noise prediction\", \"Advanced loss functions\"),\n",
    "        (\"No conditioning\", \"Text/class conditioning\"),\n",
    "        (\"Direct sampling\", \"Advanced samplers (DDIM, DPM)\")\n",
    "    ]\n",
    "    \n",
    "    print(\"From Lab to Real World:\")\n",
    "    for lab, sota in lab_to_sota:\n",
    "        print(f\"  {lab:25s} → {sota}\")\n",
    "    \n",
    "    print(f\"\\nThe mathematical foundation you implemented enables:\")\n",
    "    print(\"  🎨 DALL-E 2, Midjourney, Stable Diffusion\")\n",
    "    print(\"  🎵 Audio generation models\")\n",
    "    print(\"  🧬 Protein structure generation\")\n",
    "    print(\"  📹 Video synthesis\")\n",
    "    print(\"  🔬 Scientific data generation\")\n",
    "    \n",
    "    print(f\"\\nKey insight: The ELBO framework scales!\")\n",
    "    print(\"  • Same three forces at any scale\")\n",
    "    print(\"  • Noise prediction principle universal\")\n",
    "    print(\"  • Mathematical elegance → practical power\")\n",
    "\n",
    "def preview_advanced_topics():\n",
    "    \"\"\"\n",
    "    Preview advanced diffusion topics\n",
    "    \"\"\"\n",
    "    print(\"=== Preview: Advanced Diffusion Topics ===\\n\")\n",
    "    \n",
    "    advanced_topics = {\n",
    "        \"Faster Sampling\": [\n",
    "            \"DDIM (deterministic sampling)\",\n",
    "            \"DPM-Solver (ODE-based)\",\n",
    "            \"Progressive distillation\",\n",
    "            \"Consistency models\"\n",
    "        ],\n",
    "        \"Better Training\": [\n",
    "            \"Classifier-free guidance\",\n",
    "            \"Score matching interpretation\",\n",
    "            \"Improved noise schedules\",\n",
    "            \"Loss function innovations\"\n",
    "        ],\n",
    "        \"Conditional Generation\": [\n",
    "            \"Text-to-image synthesis\",\n",
    "            \"Inpainting and editing\",\n",
    "            \"Style transfer\",\n",
    "            \"Controllable generation\"\n",
    "        ],\n",
    "        \"Architecture Advances\": [\n",
    "            \"U-Net with attention\",\n",
    "            \"Transformer-based diffusion\",\n",
    "            \"Latent diffusion models\",\n",
    "            \"Cascaded diffusion\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightyellow']\n",
    "    \n",
    "    for i, (topic, items) in enumerate(advanced_topics.items()):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Create topic visualization\n",
    "        ax.text(0.5, 0.9, topic, ha='center', va='center', fontsize=14, weight='bold',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=colors[i]))\n",
    "        \n",
    "        for j, item in enumerate(items):\n",
    "            y_pos = 0.7 - j * 0.15\n",
    "            ax.text(0.5, y_pos, f\"• {item}\", ha='center', va='center', fontsize=10)\n",
    "        \n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Advanced Diffusion Model Topics', fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Your mathematical foundation enables all of these advances!\")\n",
    "\n",
    "# Connect to modern developments\n",
    "connect_to_modern_diffusion()\n",
    "preview_advanced_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd06ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Reflection and Mathematical Mastery (5 minutes)\n",
    "\n",
    "### Task 8.1: Diffusion Mathematics Summary\n",
    "\n",
    "**Discussion Questions** (Work with your partner):\n",
    "\n",
    "1. **ELBO Derivation Insights**:\n",
    "   - Which step in the ELBO derivation was most challenging to understand?\n",
    "   - How does the three-forces decomposition clarify the learning process?\n",
    "\n",
    "2. **Noise Prediction Breakthrough**:\n",
    "   - Why is noise prediction easier than image prediction for neural networks?\n",
    "   - How does this reparameterization connect to the optimal reverse distribution?\n",
    "\n",
    "3. **Practical Impact**:\n",
    "   - How does complex mathematical theory enable simple practical algorithms?\n",
    "   - What would diffusion models be like without the ELBO framework?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9cf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diffusion_mathematics():\n",
    "    \"\"\"\n",
    "    Reflect on the diffusion mathematics mastery achieved\n",
    "    \"\"\"\n",
    "    print(\"=== Your Diffusion Mathematics Journey ===\\n\")\n",
    "    \n",
    "    concepts_mastered = [\n",
    "        \"🔄 Sequential latent variable models\",\n",
    "        \"📐 Diffusion ELBO derivation (four algebraic steps)\",\n",
    "        \"⚖️  Three forces: Reconstruction, Prior Matching, Denoising\",\n",
    "        \"🎯 Tractable reverse distribution via Bayes rule\",\n",
    "        \"⚡ Noise prediction reparameterization\",\n",
    "        \"🚀 Simple training algorithm from complex theory\",\n",
    "        \"🧮 Mathematical validation and testing\",\n",
    "        \"🌉 Bridge from theory to state-of-the-art practice\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Mathematical concepts you implemented:\")\n",
    "    for concept in concepts_mastered:\n",
    "        print(f\"  {concept}\")\n",
    "    \n",
    "    print(f\"\\n🎓 You now understand the mathematical heart of:\")\n",
    "    print(f\"   • Diffusion Models (DDPM, DDIM, Score-based)\")\n",
    "    print(f\"   • Modern generative AI (DALL-E, Midjourney, Stable Diffusion)\")\n",
    "    print(f\"   • Variational inference for sequential models\")\n",
    "    print(f\"   • The connection between theory and practice\")\n",
    "    \n",
    "    print(f\"\\n🔬 Key mathematical insights achieved:\")\n",
    "    print(f\"   • Why sequential latents avoid approximation errors\")\n",
    "    print(f\"   • How ELBO transforms intractable to tractable\")\n",
    "    print(f\"   • Why noise prediction is the optimal parameterization\")\n",
    "    print(f\"   • How mathematical elegance enables practical success\")\n",
    "    \n",
    "    # Create mastery visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Mathematical journey progression\n",
    "    journey_steps = [\n",
    "        (\"Intractable\\nLikelihood\", 0.1, 0.8, 'red'),\n",
    "        (\"Sequential\\nELBO\", 0.3, 0.8, 'orange'),\n",
    "        (\"Three Forces\\nDecomposition\", 0.5, 0.8, 'yellow'),\n",
    "        (\"Tractable\\nReverse\", 0.7, 0.8, 'lightgreen'),\n",
    "        (\"Noise\\nPrediction\", 0.9, 0.8, 'green'),\n",
    "        (\"Simple\\nTraining\", 0.3, 0.5, 'lightblue'),\n",
    "        (\"Mathematical\\nValidation\", 0.7, 0.5, 'blue'),\n",
    "        (\"State-of-the-Art\\nConnection\", 0.5, 0.2, 'purple')\n",
    "    ]\n",
    "    \n",
    "    # Draw journey progression\n",
    "    for i, (step, x, y, color) in enumerate(journey_steps):\n",
    "        ax.text(x, y, step, ha='center', va='center', fontsize=11, weight='bold',\n",
    "               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=color, alpha=0.8))\n",
    "        \n",
    "        # Add connections\n",
    "        if i < 5:  # Top row connections\n",
    "            if i < 4:\n",
    "                ax.arrow(x + 0.08, y, 0.12, 0, head_width=0.02, head_length=0.02,\n",
    "                        fc='gray', ec='gray', alpha=0.7)\n",
    "        elif i == 5:  # Down from step 2\n",
    "            ax.arrow(0.3, 0.75, 0, -0.2, head_width=0.02, head_length=0.02,\n",
    "                    fc='gray', ec='gray', alpha=0.7)\n",
    "        elif i == 6:  # Down from step 4  \n",
    "            ax.arrow(0.7, 0.75, 0, -0.2, head_width=0.02, head_length=0.02,\n",
    "                    fc='gray', ec='gray', alpha=0.7)\n",
    "        elif i == 7:  # Final convergence\n",
    "            ax.arrow(0.3, 0.45, 0.15, -0.2, head_width=0.02, head_length=0.02,\n",
    "                    fc='gray', ec='gray', alpha=0.7)\n",
    "            ax.arrow(0.7, 0.45, -0.15, -0.2, head_width=0.02, head_length=0.02,\n",
    "                    fc='gray', ec='gray', alpha=0.7)\n",
    "    \n",
    "    # Add achievement badges\n",
    "    achievements = [\n",
    "        (0.1, 0.6, \"🔧\\nDerivation\\nMastery\"),\n",
    "        (0.9, 0.6, \"⚡\\nImplementation\\nPower\"),\n",
    "        (0.1, 0.3, \"🧮\\nValidation\\nSkills\"),\n",
    "        (0.9, 0.3, \"🌉\\nTheory-Practice\\nBridge\")\n",
    "    ]\n",
    "    \n",
    "    for x, y, achievement in achievements:\n",
    "        ax.text(x, y, achievement, ha='center', va='center', fontsize=10,\n",
    "               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='gold', alpha=0.6))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Your Diffusion Mathematics Mastery Journey', fontsize=16, weight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summarize the mathematical journey\n",
    "summarize_diffusion_mathematics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0af632e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## Implementation Checklist\n",
    "\n",
    "### Core Mathematical Functions (Students Implement):\n",
    "\n",
    "**✅ Essential TODOs:**\n",
    "- [ ] `forward_step()` - Single forward diffusion step\n",
    "- [ ] `forward_trajectory()` - Complete forward trajectory\n",
    "- [ ] `direct_jump_forward()` - Efficient analytical forward jump\n",
    "- [ ] `implement_elbo_step1_separation()` - Strategic term separation\n",
    "- [ ] `implement_elbo_step2_reindexing()` - Index alignment\n",
    "- [ ] `implement_elbo_step3_bayes_rule()` - Bayes rule transformation\n",
    "- [ ] `implement_elbo_step4_final_form()` - Final KL divergence form\n",
    "- [ ] `bayes_rule_transformation()` - Tractable reverse distribution\n",
    "- [ ] `optimal_reverse_mean()` - Optimal interpolation formula\n",
    "- [ ] `optimal_reverse_variance()` - Fixed optimal variance\n",
    "- [ ] `forward_process_with_noise()` - Explicit noise tracking\n",
    "- [ ] `solve_for_x0()` - Noise inversion formula\n",
    "- [ ] `reparameterize_optimal_mean()` - Noise-parameterized mean\n",
    "- [ ] `simple_diffusion_loss()` - Simple training algorithm\n",
    "- [ ] `compute_reconstruction_term()` - First ELBO force\n",
    "- [ ] `compute_prior_matching_term()` - Second ELBO force\n",
    "- [ ] `compute_denoising_terms()` - Third ELBO force\n",
    "\n",
    "**✅ Provided Starter Code:**\n",
    "- [ ] All visualization functions with complete plotting code\n",
    "- [ ] Training loops and optimization infrastructure\n",
    "- [ ] Mathematical validation and testing frameworks\n",
    "- [ ] Connection to modern diffusion models\n",
    "- [ ] Noise schedule creation and management\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Requirements\n",
    "\n",
    "### What to Submit\n",
    "\n",
    "Submit your completed Jupyter notebook (.ipynb file) with:\n",
    "\n",
    "**✅ ELBO Derivation:**\n",
    "- Complete four-step algebraic derivation implemented correctly\n",
    "- Strategic separation, reindexing, Bayes rule, and final KL form\n",
    "- Clear mathematical comments explaining each transformation\n",
    "\n",
    "**✅ Tractable Reverse Distribution:**\n",
    "- Optimal mean and variance implementations\n",
    "- Interpolation weight analysis and visualization\n",
    "- Connection to forward process via Bayes rule\n",
    "\n",
    "**✅ Noise Prediction Reparameterization:**\n",
    "- Forward process with explicit noise tracking\n",
    "- Noise inversion and reparameterization formulas\n",
    "- Equivalence demonstrations between parameterizations\n",
    "\n",
    "**✅ Three Forces Analysis:**\n",
    "- Implementation of all three ELBO terms\n",
    "- Force magnitude analysis and interpretation\n",
    "- Connection between forces and learning dynamics\n",
    "\n",
    "**✅ Simple Training Algorithm:**\n",
    "- Complete noise prediction training implementation\n",
    "- Connection between complex ELBO and simple MSE loss\n",
    "- Mathematical validation of all components\n",
    "\n",
    "**✅ Documentation and Insights:**\n",
    "- Clear explanations of mathematical derivations\n",
    "- Discussion of implementation challenges and solutions\n",
    "- Connection between theory and practical state-of-the-art models\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference: Key Mathematical Formulas\n",
    "\n",
    "### For Implementation Reference:\n",
    "\n",
    "**Forward Process Direct Jump:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e276b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q(x_t | x_0) = N(√ᾱ_t * x_0, (1-ᾱ_t) * I)\n",
    "mean = torch.sqrt(alpha_cumprod_t) * x_0\n",
    "variance = 1 - alpha_cumprod_t\n",
    "x_t = mean + torch.sqrt(variance) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd070c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Optimal Reverse Mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d281ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̃_t(x_t, x_0) = weight_x0 * x_0 + weight_xt * x_t\n",
    "weight_x0 = (torch.sqrt(alpha_cumprod_t_minus_1) * beta_t) / (1 - alpha_cumprod_t)\n",
    "weight_xt = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_minus_1)) / (1 - alpha_cumprod_t)\n",
    "optimal_mean = weight_x0 * x_0 + weight_xt * x_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a705c6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Noise Reparameterization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2daeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# μ̃_t(x_t, ε) = (1/√α_t) * (x_t - (1-α_t)/√(1-ᾱ_t) * ε)\n",
    "coeff = (1 - alpha_t) / torch.sqrt(1 - alpha_cumprod_t)\n",
    "optimal_mean = (x_t - coeff * epsilon) / torch.sqrt(alpha_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5ade6c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Simple Training Loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample t ~ Uniform{1, ..., T}\n",
    "# Sample ε ~ N(0, I)  \n",
    "# Compute x_t = √ᾱ_t * x_0 + √(1-ᾱ_t) * ε\n",
    "# Loss = ||ε - ε_θ(x_t, t)||²\n",
    "t = torch.randint(1, T+1, (batch_size,))\n",
    "epsilon = torch.randn_like(x_0)\n",
    "x_t = torch.sqrt(alpha_cumprod[t]) * x_0 + torch.sqrt(1 - alpha_cumprod[t]) * epsilon\n",
    "loss = F.mse_loss(epsilon_pred, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bf21ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Optimal Reverse Variance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3bc30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# σ̃²_t = (1-ᾱ_{t-1})/(1-ᾱ_t) * β_t\n",
    "optimal_variance = ((1 - alpha_cumprod_t_minus_1) / (1 - alpha_cumprod_t)) * beta_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0882ccd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Implementation Issues & Solutions\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "**Index Confusion:**\n",
    "- Forward process: x_0 → x_1 → ... → x_T (T+1 states total)\n",
    "- Reverse process: x_T → x_{T-1} → ... → x_0\n",
    "- Python indexing: alphas[t] corresponds to timestep t (1-indexed mathematically)\n",
    "\n",
    "**Numerical Stability:**\n",
    "- Use `torch.sqrt()` instead of `** 0.5` for gradients\n",
    "- Add small epsilon (1e-8) when computing log probabilities\n",
    "- Clamp alpha values to avoid division by zero: `torch.clamp(alpha, min=1e-8)`\n",
    "\n",
    "**Shape Broadcasting:**\n",
    "- Noise schedule tensors: shape (T,)\n",
    "- Batch operations: ensure proper broadcasting with (batch_size, data_dim)\n",
    "- Use `.view(-1, 1)` or `.unsqueeze(-1)` for proper tensor alignment\n",
    "\n",
    "**Mathematical Sign Conventions:**\n",
    "- ELBO terms: reconstruction (+), KL divergences (-)\n",
    "- Loss for optimization: minimize negative ELBO\n",
    "- Log probabilities vs probabilities: ensure consistent use\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
