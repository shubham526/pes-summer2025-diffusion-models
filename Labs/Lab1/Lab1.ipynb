{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9c7f26f",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "e9c7f26f"
      },
      "source": [
        "# Lab 1: Hands-On Image Generation and Diffusion Fundamentals\n",
        "**Course: Diffusion Models: Theory and Applications**  \n",
        "**Duration: 90 minutes**  \n",
        "**Team Size: 2 students**\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lab, students will be able to:\n",
        "1. **Implement** a naive direct image generator and understand its limitations\n",
        "2. **Design and code** different noise addition schedules for the forward diffusion process\n",
        "3. **Build** a U-Net denoising network with time embeddings from scratch\n",
        "4. **Train** diffusion models using the noise prediction objective\n",
        "5. **Generate** new images using the reverse diffusion sampling process\n",
        "6. **Compare** different generation approaches quantitatively and qualitatively\n",
        "7. **Form** effective teams for the course mini-project\n",
        "\n",
        "---\n",
        "\n",
        "## Prerequisites\n",
        "- Basic PyTorch knowledge (tensors, nn.Module, training loops)\n",
        "- Understanding of convolutional neural networks\n",
        "- Familiarity with image data representation\n",
        "\n",
        "---\n",
        "\n",
        "## Lab Setup and Environment\n",
        "\n",
        "### Part 1: Team Formation & Setup (10 minutes)\n",
        "\n",
        "#### Task 1.1: Find Your Lab Partner & Set Goals\n",
        "- Form teams of 2 students (this will be your mini-project team)\n",
        "- Exchange contact information and GitHub usernames\n",
        "- **Today's Mission**: Build 4 different image generation systems from scratch\n",
        "- **Success Criteria**: Generate recognizable MNIST digits by the end of lab\n",
        "\n",
        "#### Task 1.2: Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "861478f7",
      "metadata": {
        "id": "861478f7",
        "outputId": "5589f96c-e5ee-4c02-d98b-7ef49e275136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 485kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.85MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.10MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x200 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAACNCAYAAACDr+ZrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJKJJREFUeJzt3Xd0VNX2wPEdICYgHQKKSADpj97hh4AChi5IVWkK6gJBZAEiiIKFXqQXUZrwRBYQUEQRBQQ0L4AIigLGQKQ8Sui9398fb3E854Ykk2Qmk3vn+1mLtfbJnjv3ZHamHOace4Isy7IEAAAAAACHyuTvDgAAAAAAkBYMbAEAAAAAjsbAFgAAAADgaAxsAQAAAACOxsAWAAAAAOBoDGwBAAAAAI7GwBYAAAAA4GgMbAEAAAAAjsbAFgAAAADgaI4d2MbFxUlQUJBMnDjRa/e5efNmCQoKks2bN3vtPpFy1Na9qK07UVf3orbuRW3di9q6E3VNXroObBcuXChBQUGyc+fO9Dxtuhk5cqQEBQUl+BcaGurvrvmc22srInLs2DHp2LGj5M6dW3LmzClPP/20HDx40N/d8rlAqK2uSZMmEhQUJH379vV3V3zK7XU9cOCADBgwQOrWrSuhoaESFBQkcXFx/u5WunB7bUVEli1bJlWrVpXQ0FAJCwuTnj17yunTp/3dLZ9ze21XrVolnTp1kuLFi0u2bNmkdOnSMnDgQDl//ry/u+Zzbq9toL4mu72ukZGREhERIYUKFZKQkBApXLiwtG/fXvbu3euX/mTxy1ldbvbs2ZI9e3bVzpw5sx97A2+4fPmyPPHEE3LhwgUZNmyYBAcHy4cffigNGjSQ3bt3S758+fzdRXjBqlWrJCoqyt/dgBdERUXJtGnTpFy5clK2bFnZvXu3v7sEL5k9e7b06dNHGjVqJJMnT5ajR4/K1KlTZefOnRIdHR0Q/5nsVi+//LIUKlRIunTpIkWKFJHffvtNZsyYIevWrZNdu3ZJ1qxZ/d1FpBKvye7022+/SZ48eaR///6SP39+OXHihMyfP19q1qwpUVFRUqlSpXTtDwNbH2jfvr3kz5/f392AF82aNUtiYmJk+/btUqNGDRERadasmZQvX14mTZoko0eP9nMPkVbXr1+XgQMHypAhQ+Sdd97xd3eQRq1bt5bz589Ljhw5ZOLEiXyIcombN2/KsGHDpH79+rJhwwYJCgoSEZG6detKq1atZN68edKvXz8/9xKptWLFCmnYsKHxs2rVqkn37t1l6dKl0qtXL/90DGnGa7I73e/zUq9evaRw4cIye/ZsmTNnTrr2J8Otsb1586a88847Uq1aNcmVK5c8+OCD8vjjj8umTZsSPebDDz+U8PBwyZo1qzRo0OC+X3/v379f2rdvL3nz5pXQ0FCpXr26fPHFF8n25+rVq7J///4UTXGyLEsuXrwolmV5fEwgcHJtV6xYITVq1FCDWhGRMmXKSKNGjWT58uXJHu92Tq7tPePHj5e7d+/KoEGDPD7G7Zxc17x580qOHDmSvV2gcmpt9+7dK+fPn5dOnTqpQa2ISMuWLSV79uyybNmyZM/ldk6trYgkGNSKiLRt21ZERPbt25fs8W7n5Nrympw4J9f1fgoUKCDZsmXzyxKCDDewvXjxonz88cfSsGFDGTdunIwcOVLi4+MlIiLivv+7s3jxYpk2bZq8+uqrMnToUNm7d688+eSTcvLkSXWb33//XWrXri379u2TN998UyZNmiQPPvigtGnTRiIjI5Psz/bt26Vs2bIyY8YMj3+H4sWLS65cuSRHjhzSpUsXoy+BzKm1vXv3rvz6669SvXr1BLmaNWtKbGysXLp0ybMHwaWcWtt7Dh8+LGPHjpVx48Yx1U3j9LoicU6t7Y0bN0RE7vs8zZo1q/zyyy9y9+5dDx4B93JqbRNz4sQJERFmwon7aov/cUNdz58/L/Hx8fLbb79Jr1695OLFi9KoUSOPj/caKx0tWLDAEhFrx44did7m9u3b1o0bN4yfnTt3zipYsKD14osvqp8dOnTIEhEra9as1tGjR9XPo6OjLRGxBgwYoH7WqFEjq0KFCtb169fVz+7evWvVrVvXKlmypPrZpk2bLBGxNm3alOBnI0aMSPb3mzJlitW3b19r6dKl1ooVK6z+/ftbWbJksUqWLGlduHAh2eOdzM21jY+Pt0TEeu+99xLkZs6caYmItX///iTvw8ncXNt72rdvb9WtW1e1RcR69dVXPTrWqQKhrvdMmDDBEhHr0KFDKTrOqdxc2/j4eCsoKMjq2bOn8fP9+/dbImKJiHX69Okk78PJ3FzbxPTs2dPKnDmz9eeff6bqeKcIpNoG0mtyoNS1dOnS6jU4e/bs1vDhw607d+54fLy3ZLhvbDNnziwPPPCAiPzvm7KzZ8/K7du3pXr16rJr164Et2/Tpo088sgjql2zZk2pVauWrFu3TkREzp49Kxs3bpSOHTvKpUuX5PTp03L69Gk5c+aMRERESExMjBw7dizR/jRs2FAsy5KRI0cm2/f+/fvL9OnT5bnnnpN27drJlClTZNGiRRITEyOzZs1K4SPhPk6t7bVr10REJCQkJEHu3kVK7t0mUDm1tiIimzZtkpUrV8qUKVNS9ksHACfXFUlzam3z588vHTt2lEWLFsmkSZPk4MGDsnXrVunUqZMEBweLCK/HTq3t/fz73/+WTz75RAYOHCglS5ZM8fFu46ba4h9uqOuCBQvkm2++kVmzZknZsmXl2rVrcufOHY+P95YMN7AVEVm0aJFUrFhRQkNDJV++fBIWFiZfffWVXLhwIcFt7/dCV6pUKXUJ8b/++kssy5K3335bwsLCjH8jRowQEZFTp0757Hd57rnn5KGHHpLvvvvOZ+dwEifW9t6Ut3tT4HTXr183bhPInFjb27dvy2uvvSZdu3Y11k/jH06sKzzj1NrOnTtXmjdvLoMGDZLHHntM6tevLxUqVJBWrVqJiBi7EgQqp9ZWt3XrVunZs6dERETIqFGjvH7/TuWG2iIhp9e1Tp06EhERIb1795b169fLkiVLZOjQoV49hycy3FWRlyxZIj169JA2bdrI4MGDpUCBApI5c2YZM2aMxMbGpvj+7q21GTRokERERNz3NiVKlEhTn5Pz6KOPytmzZ316Didwam3z5s0rISEhcvz48QS5ez8rVKhQms/jZE6t7eLFi+XAgQMyd+7cBPvpXbp0SeLi4tRFEAKRU+uK5Dm5trly5ZI1a9bI4cOHJS4uTsLDwyU8PFzq1q0rYWFhkjt3bq+cx6mcXNt79uzZI61bt5by5cvLihUrJEuWDPdx1S/cUFsk5La65smTR5588klZunSpTJw40WfnuZ8M90qxYsUKKV68uKxatcq44uG9/2Gwi4mJSfCzP//8U4oWLSoi/7uQk4hIcHCwNG7c2PsdToZlWRIXFydVqlRJ93NnNE6tbaZMmaRChQr33Vw7OjpaihcvHvBX+nNqbQ8fPiy3bt2S//u//0uQW7x4sSxevFgiIyOlTZs2PutDRubUuiJ5bqhtkSJFpEiRIiLyvwuX/Pzzz9KuXbt0OXdG5vTaxsbGStOmTaVAgQKybt06voHXOL22uD831vXatWv3/bbZ1zLcVOTMmTOLiBhb5URHR0tUVNR9b7969Wpjnvj27dslOjpamjVrJiL/u+R0w4YNZe7cuff9xi0+Pj7J/qTkktf3u6/Zs2dLfHy8NG3aNNnj3c7JtW3fvr3s2LHDGNweOHBANm7cKB06dEj2eLdzam07d+4skZGRCf6JiDRv3lwiIyOlVq1aSd6Hmzm1rkie22o7dOhQuX37tgwYMCBVx7uJk2t74sQJeeqppyRTpkyyfv16CQsLS/aYQOLk2iJxTq7r/aY0x8XFyffff3/f3UR8zS/f2M6fP1+++eabBD/v37+/tGzZUlatWiVt27aVFi1ayKFDh2TOnDlSrlw5uXz5coJjSpQoIfXq1ZPevXvLjRs3ZMqUKZIvXz5544031G1mzpwp9erVkwoVKshLL70kxYsXl5MnT0pUVJQcPXpU9uzZk2hft2/fLk888YSMGDEi2UXU4eHh0qlTJ6lQoYKEhobKtm3bZNmyZVK5cmV55ZVXPH+AHMytte3Tp4/MmzdPWrRoIYMGDZLg4GCZPHmyFCxYUAYOHOj5A+RgbqxtmTJlpEyZMvfNFStWLCC+qXVjXUVELly4INOnTxcRkR9//FFERGbMmCG5c+eW3LlzS9++fT15eBzNrbUdO3as7N27V2rVqiVZsmSR1atXy7fffisffPBBwKyVd2ttmzZtKgcPHpQ33nhDtm3bJtu2bVO5ggULSpMmTTx4dJzNrbUN9Ndkt9a1QoUK0qhRI6lcubLkyZNHYmJi5JNPPpFbt27J2LFjPX+AvCXdrr9s/XPJ68T+HTlyxLp79641evRoKzw83AoJCbGqVKlirV271urevbsVHh6u7uveJa8nTJhgTZo0yXr00UetkJAQ6/HHH7f27NmT4NyxsbFWt27drIceesgKDg62HnnkEatly5bWihUr1G3SesnrXr16WeXKlbNy5MhhBQcHWyVKlLCGDBliXbx4MS0PmyO4vbaWZVlHjhyx2rdvb+XMmdPKnj271bJlSysmJia1D5ljBEJt7SSAtvtxa13v9el+//S+u5Hba7t27VqrZs2aVo4cOaxs2bJZtWvXtpYvX56Wh8wx3F7bpH63Bg0apOGRy/jcXttAfU12e11HjBhhVa9e3cqTJ4+VJUsWq1ChQlbnzp2tX3/9NS0PW6oFWZb2vTcAAAAAAA6T4dbYAgAAAACQEgxsAQAAAACOxsAWAAAAAOBoDGwBAAAAAI7GwBYAAAAA4GgMbAEAAAAAjsbAFgAAAADgaFk8vWFQUJAv+4EU8PbWw9Q24/BmbalrxsFz1r2orXtRW/fivdadeM66l6e15RtbAAAAAICjMbAFAAAAADgaA1sAAAAAgKMxsAUAAAAAOBoDWwAAAACAozGwBQAAAAA4GgNbAAAAAICjMbAFAAAAADgaA1sAAAAAgKMxsAUAAAAAOBoDWwAAAACAozGwBQAAAAA4GgNbAAAAAICjZfF3BwBfqFatmtHu27evirt162bkFi9erOLp06cbuV27dvmgdwAAAOln6tSpRvu1115T8d69e41cy5Ytjfbff//tu44BXsQ3tgAAAAAAR2NgCwAAAABwNAa2AAAAAABHC7Isy/LohkFBvu6LV2TOnNlo58qVy+Nj9XWY2bJlM3KlS5dW8auvvmrkJk6cqOJnn33WyF2/fl3FY8eONXLvvvuux33TeVgyjzmltkmpXLmy0d64caPRzpkzp0f3c+HCBaOdL1++NPUrpbxZWzfU1VcaNWqk4qVLlxq5Bg0aqPjAgQNeOR/PWe8aPny4iu2vo5ky/fP/tQ0bNjRyP/zwg9f7Qm3di9omL0eOHEY7e/bsKm7RooWRCwsLU/HkyZON3I0bN3zQu8QFyntt0aJFVfzzzz8budy5c6vY/njYa7d+/Xqv980XAuk5W6pUKRUHBwcbufr166t41qxZRu7u3bteOf+aNWtU3LlzZyN38+ZNr5xD52lt+cYWAAAAAOBoDGwBAAAAAI6WYbf7KVKkiNF+4IEHVFy3bl0jV69ePRXrUytERNq1a+eV/hw9elTF06ZNM3Jt27ZV8aVLl4zcnj17VOyLaXCBrGbNmipeuXKlkbNPQdenMNhrpE+ZsE89rl27tortW//4YqpFRqBPYRExH5PIyMj07o5P1KhRQ8U7duzwY0/giR49ehjtIUOGqDipaVXenpYGBCJ9Oqv+3BMRqVOnjtEuX768R/f58MMPG2196xl4T3x8vIq3bNli5Fq3bp3e3UEK/etf/1Kx/X2wQ4cOKtaX4IiIFCpUSMX290hvvS/qfz9z5swxcq+//rqKL1686JXzeYpvbAEAAAAAjsbAFgAAAADgaAxsAQAAAACOlqHW2Opbtti3a0nJtj3eYJ+Trm8vcfnyZSOnbxdy/PhxI3fu3DkVe2vrkECib7tUtWpVI7dkyRIV29frJCUmJsZojx8/XsXLli0zcj/++KOK9b8BEZExY8Z4fE4nsW+RUrJkSRU7dY2tff1JsWLFVBweHm7kMvLl/QOVvUahoaF+6gnuqVWrltHu0qWLivUts0TMdWJ2gwYNMtr//e9/VaxfP0PEfM2Pjo72vLNIVpkyZVSsr48TEXn++edVnDVrViNnf708cuSIiu3XsyhbtqyKO3bsaOT0LUn279/vYa+RnCtXrqj477//9mNPkBr658zmzZv7sSdJ69atm9H+5JNPVKx/jk4PfGMLAAAAAHA0BrYAAAAAAEfLUFORDx8+rOIzZ84YOW9MRbZPXTp//rzRfuKJJ1Rs38rl008/TfP5kXJz585V8bPPPuuV+7RPac6ePbuK7Vsy6dNyK1as6JXzZ3T2KSVRUVF+6on32Keqv/TSSyrWpzeKMA0uo2jcuLGK+/Xrl+jt7PVq2bKlik+ePOn9jgWwTp06qXjq1KlGLn/+/Cq2T0/dvHmz0Q4LC1PxhAkTEj2f/X704zp37px8h2HQP0eNGzfOyOm1zZEjh8f3aV/aExERoeLg4GAjpz9X9b+X+7XhHfoWmJUqVfJfR5AqGzZsUHFSU5FPnTpltPWpwPalWEltkWffTtW+rMQJ+MYWAAAAAOBoDGwBAAAAAI7GwBYAAAAA4GgZao3t2bNnVTx48GAjp6+b+uWXX4zctGnTEr3P3bt3q7hJkyZGTr8Muoi5JUH//v2T7zC8rlq1aka7RYsWKk5qGxb72tgvv/zSaE+cOFHF+nYSIubfk749k4jIk08+6dH53cS+HsMNPv7440Rz9jVi8A/71i4LFixQcVLXWLCv0WRLi7TJkuWfjwXVq1c3cvPmzVOxvhWbiMiWLVtU/P777xu5bdu2Ge2QkBAVL1++3Mg99dRTifZt586dieaQvLZt26q4V69eqbqP2NhYo23/XKVv91OiRIlUnQPeoz9PixQp4vFxNWrUMNr6+mheY9PP7NmzVbx69epEb3fr1i2jfeLEiVSdL2fOnEZ77969Ki5UqFCix9n75s/Xavd9ggUAAAAABBQGtgAAAAAAR8tQU5F19q+1N27cqOJLly4ZOf0S5j179jRy+hRU+9Rju99//13FL7/8ssd9RdpUrlxZxfqlzUXMaRGWZRm5r7/+WsX2rYDslygfPny4iu3TUuPj41W8Z88eI6dfFl2fFi1ibhu0a9cucTJ9K6OCBQv6sSe+kdRUVvvfHPyje/fuRjupaU/69jGLFy/2VZcCUpcuXVSc1BR++/NG3y7m4sWLSZ5Dv21SU4+PHj1qtBctWpTk/SJpHTp08Oh2cXFxRnvHjh0qHjJkiJHTpx7blS1b1vPOwSf0pVcLFy40ciNHjkz0OHtO3x5zxowZXugZPHH79m0VJ/Vc8xZ9uy4RkTx58nh0nP21+saNG17rU0rxjS0AAAAAwNEY2AIAAAAAHI2BLQAAAADA0TLsGlu7pNbsXLhwIdHcSy+9pOLPP//cyOnrJ5F+SpUqZbT1rZ3sayFPnz6t4uPHjxs5fb3V5cuXjdxXX32VZDs1smbNarQHDhyo4ueffz7N9+9PzZs3V7H993Qqfa1wsWLFEr3dsWPH0qM7sMmfP7/RfvHFF422/vqsr+8SEfnggw981q9AY9+aZ9iwYSq2X9dg1qxZKtavWyCS/Lpa3VtvveXR7V577TWjrV8PASmnfx6yX0fk22+/VfFff/1l5E6dOpWq87nxeg1OZn+uJ7XGFoGjc+fOKtZfI0Q8/zz4zjvveLVPacE3tgAAAAAAR2NgCwAAAABwNMdMRU6KPp2iWrVqRk7f9qVx48ZGTp96A98KCQlRsb4Fk4g5Dda+lVO3bt1UvHPnTiPn7ymzRYoU8ev5val06dKJ5vRtsJxE/zuzT4n7888/VWz/m4PvFC1aVMUrV670+Ljp06cb7U2bNnmrSwFJnzamTz0WEbl586aK169fb+T0rV6uXbuW6P2HhoYabfuWPvprZ1BQkJHTp5mvWbMm0XMg5fStX9JjGmqdOnV8fg6kXqZM/3y3xdI897IvlXvzzTeNdokSJVQcHBzs8f3u3r1bxbdu3Upd53yAb2wBAAAAAI7GwBYAAAAA4GgMbAEAAAAAjuaKNbZXrlxRsf1S1bt27VLxvHnzjJx9nZa+hnPmzJlGzr7tAVKmSpUqKtbX1No9/fTTRvuHH37wWZ/gmR07dvi7C0rOnDmNdtOmTVXcpUsXI2df16fTtz2wbyUD39HrVbFixSRv+/3336t46tSpPutTIMidO7fR7tOnj4rt7236uto2bdp4fA59ndbSpUuNnP3aF7oVK1YY7fHjx3t8TqQPfdulBx980OPjKlSokGjup59+MtpRUVEp7xjSRF9Xy2fcjEm/LkXXrl2NnP26QYmpV6+e0U5JrfVt3Oxrc9etW6fipK65kN74xhYAAAAA4GgMbAEAAAAAjuaKqci62NhYo92jRw8VL1iwwMjZv9bX2/bpNosXL1bx8ePH09rNgDN58mQV27d30KcbZ7Spx1wOXyRv3rypOq5SpUoqttdcn0JTuHBhI/fAAw+o2H6Zer0eIub0l+joaCN348YNFWfJYr7U/fzzz0n2Hd5hn8o6duzYRG+7bds2o929e3cVX7hwwav9CjT6c0pEJH/+/IneVp92WqBAASP3wgsvqLh169ZGrnz58irOnj27kbNPfdPbS5YsMXL60iL4TrZs2Yx2uXLlVDxixAgjl9TyIftrclLvk/p2Q/rfkojInTt3Eu8sECD011ERkS+++ELF/thicuvWrSr+6KOP0v38qcE3tgAAAAAAR2NgCwAAAABwNAa2AAAAAABHc90aW7vIyEgVx8TEGDl93aeISKNGjVQ8evRoIxceHq7iUaNGGbljx46luZ9u07JlS6NduXJlFdvXW+lrCDKapC6Hv3v37nTuje/oa1Xtv+ecOXNUPGzYMI/vU9/Oxb7G9vbt2yq+evWqkfvjjz9UPH/+fCOnb8klYq7JPnnypJE7evSoirNmzWrk9u/fn2TfkXr69gQrV670+LiDBw8abXs9kXo3b9402vHx8SoOCwszcocOHVJxSraF0NdP6ltEiIg8/PDDRvv06dMq/vLLLz0+B1ImODjYaOvb7tmfm3qN7Ft36LW1b8ujb+ElknDtrk6/1sEzzzxj5PQtvex/r0Cg0j872T9HeSol6+Dt9M/yzZo1M3Jff/11qvrja3xjCwAAAABwNAa2AAAAAABHY2ALAAAAAHA016+x1e3du9dod+zY0Wi3atVKxfY9b1955RUVlyxZ0sg1adLEW110DfuaRn0fxVOnThm5zz//PF36lJiQkBAVjxw5MtHbbdy40WgPHTrUV11Kd3369FHx33//beTq1q2bqvs8fPiwilevXm3k9u3bp+L//Oc/qbp/u5dfftlo62sH7es34TtDhgxRcUrW8iS1xy3S5vz580Zb31947dq1Rk7ft9q+L/yaNWtUvHDhQiN39uxZFS9btszI2dfY2vPwHv291r7+ddWqVYke9+6776rY/l73448/qti+r7n9tvZ9OHX6a/KYMWOMXFLvF/qe5PAefe1lcq/V9evXV/GMGTN81qdAZx+nNGzYUMVdunQxcuvXr1fx9evXU33Onj17qrhfv36pvp+Mgm9sAQAAAACOxsAWAAAAAOBoQZaH1/NP7WWmnco+9UW/TL2+VYmISEREhIo3b97s036JpGwLBk/4orYdOnQw2p999pmKjxw5YuSKFSvm9fMnRZ96LCIyfPhwFdunF+tbOdmnuurTQLzFm7UNtOesfUq7/jc4YcIEI6dPl00PTnjOppa+lZeIuY1IkSJFEj1On9YqItK+fXuv9iu9uLm2KaFPVdS34RJJOM3x9ddfV/H06dN92q+0cEJt7Vv6vPfeeyoePHhwosfZt+ro2rWriu1T1/UpxOvWrTNyVatWNdr6Vj3jx483cvo05aeffjrRvn333XdGe9y4cSo+d+5coselZAs+3mtF7ty5o+KUPB76Vn4i5hZ9/uaE52xGkytXLhWfOXMm0dvpyzVF0n+7H09ryze2AAAAAABHY2ALAAAAAHA0BrYAAAAAAEcLqO1+7OsC7Gu6atSooWJ9Ta2dfT3Bli1bvNC7wPHFF1+k+zn1dYD2dUedOnVSsX3dX7t27XzaL6SPyMhIf3fBtb799lujnSdPnkRvq2/t1KNHD191CX6gb/FmX1NrXxvFdj9pkzlzZhW///77Rm7QoEEqvnLlipF78803VWyvgb6utnr16kZO396lSpUqRi4mJsZo9+7dW8WbNm0ycjlz5lSxfRu5559/XsWtW7c2chs2bJDE6NfsSO/rdTjdnDlzVKxvaZkc+/VG9DXzcB79OkFuwDe2AAAAAABHY2ALAAAAAHA0101FLl26tNHu27evip955hkj99BDD3l8v/pl0Y8fP27k7NOukPAS6Xq7TZs2Rq5///5eP/+AAQOM9ttvv61i/dLmIiJLly5Vcbdu3bzeF8DN8uXLZ7STej2cNWuWii9fvuyzPiH9+WL7M9yfPhVUn3osInL16lUV26eX6ssGateubeReeOEFFTdr1szI6dPM9e2EREQWLFhgtO3b+ekuXryo4m+++cbI6e1nn33WyD333HOJ3qf9vR6e279/v7+7EJDsW3Q99dRTKt64caORu3btmtfPrz/XRUSmTp3q9XP4E9/YAgAAAAAcjYEtAAAAAMDRGNgCAAAAABzNkWts7Wtj9fUY+ppaEZGiRYum6hw7d+402qNGjVKxP7arcRr79g56216/adOmqXj+/PlG7syZMyq2rwnq2rWriitVqmTkChcubLQPHz6sYvtaMH3dH9xDX9ddqlQpI6dvO4OU09fVZcrk+f+P/vTTT77oDjIAt20ZkZG98847ieb0rYDsW9uNHDlSxSVKlPD4fPpxY8aMMXL69Ue85bPPPkuyDe+YPn26ivv162fkHnvssUSPs18XRb+f2NhYL/XOXerVq6fit956y8g1adJExfYtq5Jas56UvHnzqrh58+ZGbvLkyUY7W7Zsid6Pvsb3+vXrqepLeuMbWwAAAACAozGwBQAAAAA4WoadilywYEGjXa5cORXPmDHDyJUpUyZV54iOjjbaEyZMUPGaNWuMHFv6eI8+VUpEpE+fPipu166dkdO3ByhZsqTH57BPedy0aZOKk5rGBffQp7+nZLosEqpcubLRbty4sYrtr403b95U8cyZM43cyZMnvd85ZAjFixf3dxcCxokTJ1QcFhZm5EJCQlRsX6KjW7dundHesmWLilevXm3k4uLiVOyLqcfwv99//91oJ/V85vNwyunjlvLlyyd6uzfeeMNoX7p0KVXn06c3V61a1cjZlwrqNm/ebLRnz56tYv1zdEbGpz0AAAAAgKMxsAUAAAAAOBoDWwAAAACAo/l1ja1+OWoRkblz56rYvqYrtet39LWWkyZNMnL2bV/0y1ojbaKiooz2jh07VFyjRo1Ej7NvBWRfa63TtwJatmyZkbNfjh6BrU6dOkZ74cKF/umIQ+XOndto25+numPHjql40KBBvuoSMpitW7eq2L6mnTV53lW/fn0Vt2nTxsjp6+lOnTpl5PTt9M6dO2fk9LXxCDwfffSR0W7VqpWfehLYevfu7fNz2F8XvvzySxXbPzs7ZYsfHd/YAgAAAAAcjYEtAAAAAMDRfD4VuVatWkZ78ODBKq5Zs6aRe+SRR1J1jqtXr6p42rRpRm706NEqvnLlSqruHyl39OhRo/3MM8+o+JVXXjFyw4cP9+g+p06darT1y5D/9ddfKe0iXC4oKMjfXQACxt69e1UcExNj5OxLiR577DEVx8fH+7ZjLqRvAfLpp58aOXsb8MQff/xhtPft22e0y5Ytm57dcZ0ePXqouF+/fkaue/fuab7/2NhYo62Pi/RlIiIJp53rr91uwDe2AAAAAABHY2ALAAAAAHA0BrYAAAAAAEcLsizL8uiGqVyvNnbsWKOtr7FNin2+/9q1a1V8+/ZtI6dv43P+/PkU9tB5PCyZx1iLmHF4s7aBVld9DYuIubXFvHnzjJx9nbevOf05a9/e5/PPP1dxvXr1jNyhQ4dUXKJECd92LANwem19wf5c/Pjjj432Dz/8oGL7ejP7e78/UVv34r3WnZzwnA0JCTHa+uvlBx98YOTy5Mmj4tWrVxu5DRs2qHjNmjVG7sSJE2nsZcbjaW35xhYAAAAA4GgMbAEAAAAAjubzqcjwPidMtUDqMD3KnXjOuhe1TShnzpxGe/ny5Ua7cePGKl61apWRe+GFF1Ts7y36qK178V7rTjxn3YupyAAAAACAgMDAFgAAAADgaAxsAQAAAACOxhpbB2INgXux7sedeM66F7VNnn3N7ahRo1Tcu3dvI1exYkUV+3vrH2rrXrzXuhPPWfdijS0AAAAAICAwsAUAAAAAOBpTkR2IqRbuxfQod+I5617U1r2orXvxXutOPGfdi6nIAAAAAICAwMAWAAAAAOBoDGwBAAAAAI7m8RpbAAAAAAAyIr6xBQAAAAA4GgNbAAAAAICjMbAFAAAAADgaA1sAAAAAgKMxsAUAAAAAOBoDWwAAAACAozGwBQAAAAA4GgNbAAAAAICjMbAFAAAAADja/wNaGKEKYvxhHAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Required imports - run this first!\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Visualization helper function (provided)\n",
        "def show_mnist_samples(dataset, num_samples=8):\n",
        "    \"\"\"Display a grid of MNIST samples\"\"\"\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
        "    for i in range(num_samples):\n",
        "        img, label = dataset[i]\n",
        "        axes[i].imshow(img.squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f'Label: {label}')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def show_image_grid(images, title=\"Generated Images\", nrow=4):\n",
        "    \"\"\"Helper function to display a grid of images\"\"\"\n",
        "    images = images.cpu()\n",
        "    num_images = min(16, images.shape[0])\n",
        "    ncol = min(nrow, num_images)\n",
        "    nrow_actual = (num_images + ncol - 1) // ncol\n",
        "\n",
        "    fig, axes = plt.subplots(nrow_actual, ncol, figsize=(ncol*2, nrow_actual*2))\n",
        "    if nrow_actual == 1:\n",
        "        axes = [axes] if ncol == 1 else axes\n",
        "    elif ncol == 1:\n",
        "        axes = [[ax] for ax in axes]\n",
        "\n",
        "    for i in range(num_images):\n",
        "        row, col = i // ncol, i % ncol\n",
        "        if nrow_actual == 1:\n",
        "            ax = axes[col] if ncol > 1 else axes\n",
        "        else:\n",
        "            ax = axes[row][col] if ncol > 1 else axes[row]\n",
        "        ax.imshow(images[i, 0], cmap='gray', vmin=-1, vmax=1)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(num_images, nrow_actual * ncol):\n",
        "        row, col = i // ncol, i % ncol\n",
        "        if nrow_actual == 1:\n",
        "            ax = axes[col] if ncol > 1 else axes\n",
        "        else:\n",
        "            ax = axes[row][col] if ncol > 1 else axes[row]\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_mnist_samples(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c23c261",
      "metadata": {
        "id": "2c23c261"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Implement a Naive Direct Generator (25 minutes)\n",
        "\n",
        "### Task 2.1: Build Your First Generator\n",
        "\n",
        "**Your Mission**: Create a neural network that maps random noise directly to MNIST digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a71ea91",
      "metadata": {
        "id": "2a71ea91"
      },
      "outputs": [],
      "source": [
        "class DirectGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim=64, image_size=28):\n",
        "        super().__init__()\n",
        "        self.image_size = image_size\n",
        "        self.latent_dim = latent_dim\n",
        "        self.network = nn.Sequential(nn.Linear)\n",
        "\n",
        "        # TODO: Implement your generator architecture\n",
        "        # Required components:\n",
        "        # 1. Input layer: Linear(latent_dim, 256)\n",
        "        # 2. Activation: ReLU() or LeakyReLU()\n",
        "        # 3. Hidden layer: Linear(256, 512)\n",
        "        # 4. Another activation function\n",
        "        # 5. Output layer: Linear(512, image_size * image_size)\n",
        "        # 6. Final activation: Tanh() (since MNIST is normalized to [-1,1])\n",
        "        #\n",
        "        # Architecture flow: latent_dim → 256 → 512 → 784 → reshape to (1,28,28)\n",
        "\n",
        "        # TODO: Create self.network as a nn.Sequential with the layers above\n",
        "        self.network = None\n",
        "\n",
        "    def forward(self, z):\n",
        "        # TODO: Implement the forward pass\n",
        "        # Steps:\n",
        "        # 1. Pass z through your network\n",
        "        # 2. Reshape to image format: output.view(-1, 1, self.image_size, self.image_size)\n",
        "        #\n",
        "        # Input: z of shape (batch_size, latent_dim)\n",
        "        # Output: images of shape (batch_size, 1, image_size, image_size)\n",
        "        pass\n",
        "\n",
        "# Test your implementation\n",
        "generator = DirectGenerator().to(device)\n",
        "test_z = torch.randn(4, 64).to(device)\n",
        "\n",
        "try:\n",
        "    test_output = generator(test_z)\n",
        "    print(f\"Input shape: {test_z.shape}\")\n",
        "    print(f\"Output shape: {test_output.shape}\")\n",
        "    print(f\"Output range: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
        "\n",
        "    # This should pass:\n",
        "    assert test_output.shape == (4, 1, 28, 28), f\"Expected (4,1,28,28), got {test_output.shape}\"\n",
        "    print(\"✓ Generator architecture test passed!\")\n",
        "except Exception as e:\n",
        "    print(f\"Generator test failed: {e}\")\n",
        "    print(\"Make sure to implement the network architecture and forward method!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999f073f",
      "metadata": {
        "id": "999f073f"
      },
      "source": [
        "### Task 2.2: Implement Training Loop\n",
        "\n",
        "**Your Mission**: Train your generator using statistical matching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb038bdb",
      "metadata": {
        "id": "fb038bdb"
      },
      "outputs": [],
      "source": [
        "def train_direct_generator(generator, dataloader, epochs=5, device='cpu'):\n",
        "    \"\"\"\n",
        "    Train generator using statistical matching approach.\n",
        "\n",
        "    The Challenge: How do you train a generator without paired data?\n",
        "\n",
        "    Solution - Statistical Matching:\n",
        "    Train the generator so its outputs have similar statistics to real MNIST images.\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
        "    generator.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (real_images, _) in enumerate(dataloader):\n",
        "            real_images = real_images.to(device)\n",
        "            batch_size = real_images.size(0)\n",
        "\n",
        "            # TODO: Implement your training step\n",
        "            # Steps:\n",
        "            # 1. Generate random latent codes: z = torch.randn(batch_size, generator.latent_dim).to(device)\n",
        "            # 2. Generate fake images: fake_images = generator(z)\n",
        "            # 3. Compute statistics for real images: real_mean, real_std\n",
        "            # 4. Compute statistics for fake images: fake_mean, fake_std\n",
        "            # 5. Compute loss as L1 distance between statistics\n",
        "            # 6. Backpropagate: loss.backward(), optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # TODO: Your implementation here\n",
        "            pass\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
        "                print(f'  Real mean: {real_mean.item():.4f}, std: {real_std.item():.4f}')\n",
        "                print(f'  Fake mean: {fake_mean.item():.4f}, std: {fake_std.item():.4f}')\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch} completed. Average loss: {avg_loss:.6f}')\n",
        "\n",
        "        # Generate and show samples every epoch\n",
        "        with torch.no_grad():\n",
        "            sample_z = torch.randn(16, generator.latent_dim).to(device)\n",
        "            samples = generator(sample_z)\n",
        "            show_image_grid(samples, title=f'Direct Generator - Epoch {epoch}')\n",
        "\n",
        "    return generator\n",
        "\n",
        "# Train your generator (uncomment after implementing)\n",
        "# print(\"Training Direct Generator...\")\n",
        "# trained_generator = train_direct_generator(generator, train_loader, epochs=3, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba646419",
      "metadata": {
        "id": "ba646419"
      },
      "source": [
        "### Task 2.3: Evaluate Your Direct Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdfe24fa",
      "metadata": {
        "id": "bdfe24fa"
      },
      "outputs": [],
      "source": [
        "def evaluate_generator(generator, device='cpu', num_samples=100):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of generator quality and diversity\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate samples for evaluation\n",
        "        z = torch.randn(num_samples, generator.latent_dim).to(device)\n",
        "        samples = generator(z)\n",
        "        samples = samples.cpu()\n",
        "\n",
        "        # 1. Visual Quality Assessment\n",
        "        print(\"=== Visual Quality Assessment ===\")\n",
        "        show_image_grid(samples[:16], title=\"Direct Generator Samples\")\n",
        "\n",
        "        # 2. Statistical Analysis\n",
        "        print(\"\\n=== Statistical Analysis ===\")\n",
        "        sample_mean = torch.mean(samples).item()\n",
        "        sample_std = torch.std(samples).item()\n",
        "\n",
        "        # Compare to real MNIST statistics\n",
        "        real_images = next(iter(test_loader))[0][:num_samples]\n",
        "        real_mean = torch.mean(real_images).item()\n",
        "        real_std = torch.std(real_images).item()\n",
        "\n",
        "        print(f\"Generated - Mean: {sample_mean:.4f}, Std: {sample_std:.4f}\")\n",
        "        print(f\"Real MNIST - Mean: {real_mean:.4f}, Std: {real_std:.4f}\")\n",
        "        print(f\"Mean difference: {abs(sample_mean - real_mean):.4f}\")\n",
        "        print(f\"Std difference: {abs(sample_std - real_std):.4f}\")\n",
        "\n",
        "        # 3. Diversity Measurement\n",
        "        print(\"\\n=== Diversity Measurement ===\")\n",
        "        # Flatten images and compute pairwise distances\n",
        "        flattened = samples.view(num_samples, -1)\n",
        "\n",
        "        # Compute diversity as average pairwise distance\n",
        "        diversity_scores = []\n",
        "        for i in range(min(50, num_samples)):  # Sample subset for efficiency\n",
        "            for j in range(i+1, min(50, num_samples)):\n",
        "                dist = torch.norm(flattened[i] - flattened[j]).item()\n",
        "                diversity_scores.append(dist)\n",
        "\n",
        "        avg_diversity = np.mean(diversity_scores)\n",
        "        print(f\"Average pairwise distance: {avg_diversity:.4f}\")\n",
        "\n",
        "        # 4. Pixel Value Distribution Comparison\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.hist(samples.numpy().flatten(), bins=50, alpha=0.7, label='Generated', density=True)\n",
        "        plt.hist(real_images.numpy().flatten(), bins=50, alpha=0.7, label='Real', density=True)\n",
        "        plt.title('Pixel Value Distribution')\n",
        "        plt.xlabel('Pixel Value')\n",
        "        plt.ylabel('Density')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        # Show a few samples vs real images for comparison\n",
        "        for i in range(4):\n",
        "            plt.subplot(2, 4, i+1)\n",
        "            plt.imshow(samples[i, 0], cmap='gray')\n",
        "            plt.title('Generated')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(2, 4, i+5)\n",
        "            plt.imshow(real_images[i, 0], cmap='gray')\n",
        "            plt.title('Real')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'mean_diff': abs(sample_mean - real_mean),\n",
        "            'std_diff': abs(sample_std - real_std),\n",
        "            'diversity': avg_diversity\n",
        "        }\n",
        "\n",
        "# Evaluate your generator (uncomment after training)\n",
        "# print(\"Evaluating Direct Generator...\")\n",
        "# direct_gen_results = evaluate_generator(trained_generator, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b10a934",
      "metadata": {
        "id": "3b10a934"
      },
      "source": [
        "**Discussion Questions:**\n",
        "1. What do the generated images look like? Are they recognizable digits?\n",
        "2. How do the statistics compare to real MNIST?\n",
        "3. What are the main limitations you observe?\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3: Implement Progressive Noise Addition (20 minutes)\n",
        "\n",
        "### Task 3.1: Build Your Noise Scheduler\n",
        "\n",
        "**Your Mission**: Implement the forward diffusion process with different noise schedules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d98d6227",
      "metadata": {
        "id": "d98d6227"
      },
      "outputs": [],
      "source": [
        "class NoiseScheduler:\n",
        "    def __init__(self, num_timesteps=1000, device='cpu'):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.device = device\n",
        "\n",
        "    def get_beta_schedule(self, schedule_type=\"linear\"):\n",
        "        \"\"\"\n",
        "        TODO: Implement different noise schedules\n",
        "\n",
        "        Beta schedules control how much noise is added at each timestep.\n",
        "\n",
        "        Linear Schedule (implement first):\n",
        "        - Start: beta_start = 1e-4 (very little noise)\n",
        "        - End: beta_end = 0.02 (substantial noise, but not overwhelming)\n",
        "        - Use torch.linspace to create smooth progression\n",
        "\n",
        "        Cosine Schedule (advanced):\n",
        "        - From \"Improved Denoising Diffusion Probabilistic Models\" paper\n",
        "        - Creates smoother noise addition schedule\n",
        "\n",
        "        Exponential Schedule:\n",
        "        - Exponential increase in noise\n",
        "        - Useful for faster noise addition in early steps\n",
        "        \"\"\"\n",
        "\n",
        "        if schedule_type == \"linear\":\n",
        "            # TODO: Implement linear schedule\n",
        "            # Hint: Use torch.linspace(beta_start, beta_end, self.num_timesteps)\n",
        "            pass\n",
        "\n",
        "        elif schedule_type == \"cosine\":\n",
        "            # TODO: Implement cosine schedule (advanced - optional)\n",
        "            # Hint: Research the DDPM improved paper for exact formula\n",
        "            pass\n",
        "\n",
        "        elif schedule_type == \"exponential\":\n",
        "            # TODO: Implement exponential schedule (advanced - optional)\n",
        "            # Hint: Use torch.exp with log-space interpolation\n",
        "            pass\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown schedule type: {schedule_type}\")\n",
        "\n",
        "    def precompute_coefficients(self, schedule_type=\"linear\"):\n",
        "        \"\"\"\n",
        "        TODO: Precompute coefficients for efficient noise addition and removal\n",
        "\n",
        "        Key quantities to compute:\n",
        "        - betas: noise schedule\n",
        "        - alphas: 1 - betas\n",
        "        - alpha_cumprod: cumulative product of alphas\n",
        "        - sqrt_alpha_cumprod: square root for reparameterization\n",
        "        - sqrt_one_minus_alpha_cumprod: for noise coefficient\n",
        "\n",
        "        Steps:\n",
        "        1. Get betas from schedule\n",
        "        2. Compute alphas = 1.0 - betas\n",
        "        3. Compute alpha_cumprod = torch.cumprod(alphas, dim=0)\n",
        "        4. Compute square root coefficients for forward process\n",
        "        \"\"\"\n",
        "\n",
        "        self.betas = self.get_beta_schedule(schedule_type)\n",
        "\n",
        "        # TODO: Compute the derived quantities\n",
        "        # self.alphas = 1.0 - self.betas\n",
        "        # self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        # self.sqrt_alpha_cumprod = torch.sqrt(self.alpha_cumprod)\n",
        "        # self.sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alpha_cumprod)\n",
        "\n",
        "    def add_noise(self, x0, timesteps, noise=None):\n",
        "        \"\"\"\n",
        "        TODO: Implement noise addition for given timesteps\n",
        "\n",
        "        This implements the forward diffusion process:\n",
        "        q(x_t | x_0) = N(sqrt(alpha_cumprod_t) * x_0, (1 - alpha_cumprod_t) * I)\n",
        "\n",
        "        Reparameterization: x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1 - alpha_cumprod_t) * noise\n",
        "\n",
        "        Implementation steps:\n",
        "        1. Generate noise if not provided: noise = torch.randn_like(x0)\n",
        "        2. Extract coefficients for given timesteps\n",
        "        3. Reshape coefficients for broadcasting\n",
        "        4. Apply the reparameterization formula\n",
        "        5. Return noisy images and the noise that was added\n",
        "        \"\"\"\n",
        "\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "\n",
        "        # TODO: Extract coefficients for the given timesteps\n",
        "        # sqrt_alpha_cumprod_t = self.sqrt_alpha_cumprod[timesteps]\n",
        "        # sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alpha_cumprod[timesteps]\n",
        "\n",
        "        # TODO: Reshape for broadcasting (batch_size,) -> (batch_size, 1, 1, 1)\n",
        "        # sqrt_alpha_cumprod_t = sqrt_alpha_cumprod_t.view(-1, 1, 1, 1)\n",
        "        # sqrt_one_minus_alpha_cumprod_t = sqrt_one_minus_alpha_cumprod_t.view(-1, 1, 1, 1)\n",
        "\n",
        "        # TODO: Apply forward diffusion formula\n",
        "        # noisy_x = sqrt_alpha_cumprod_t * x0 + sqrt_one_minus_alpha_cumprod_t * noise\n",
        "\n",
        "        # return noisy_x, noise\n",
        "        pass\n",
        "\n",
        "# Visualization function (provided)\n",
        "def visualize_forward_process(image, scheduler, timesteps_to_show, title=\"Forward Diffusion Process\"):\n",
        "    \"\"\"\n",
        "    Visualize how a clean image gradually becomes pure noise\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, len(timesteps_to_show), figsize=(15, 3))\n",
        "    if len(timesteps_to_show) == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, t in enumerate(timesteps_to_show):\n",
        "        # Add noise for this timestep\n",
        "        t_tensor = torch.tensor([t]).to(image.device)\n",
        "        noisy_img, _ = scheduler.add_noise(image, t_tensor)\n",
        "\n",
        "        # Display the noisy image\n",
        "        img_to_show = noisy_img[0, 0].cpu()  # Remove batch and channel dimensions\n",
        "        axes[i].imshow(img_to_show, cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[i].set_title(f'Step {t}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compare_noise_schedules(image, scheduler, timesteps_to_show, schedules=[\"linear\"]):\n",
        "    \"\"\"\n",
        "    Compare different noise schedules side by side\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(len(schedules), len(timesteps_to_show), figsize=(15, 3*len(schedules)))\n",
        "\n",
        "    if len(schedules) == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i, schedule_type in enumerate(schedules):\n",
        "        try:\n",
        "            scheduler.precompute_coefficients(schedule_type)\n",
        "\n",
        "            for j, t in enumerate(timesteps_to_show):\n",
        "                t_tensor = torch.tensor([t]).to(image.device)\n",
        "                noisy_img, _ = scheduler.add_noise(image, t_tensor)\n",
        "\n",
        "                img_to_show = noisy_img[0, 0].cpu()\n",
        "                axes[i, j].imshow(img_to_show, cmap='gray', vmin=-1, vmax=1)\n",
        "                if i == 0:\n",
        "                    axes[i, j].set_title(f'Step {t}')\n",
        "                if j == 0:\n",
        "                    axes[i, j].set_ylabel(schedule_type.capitalize())\n",
        "                axes[i, j].axis('off')\n",
        "        except:\n",
        "            # If schedule not implemented, show placeholder\n",
        "            for j, t in enumerate(timesteps_to_show):\n",
        "                axes[i, j].text(0.5, 0.5, f'{schedule_type}\\nNot Implemented',\n",
        "                               ha='center', va='center', transform=axes[i, j].transAxes)\n",
        "                axes[i, j].axis('off')\n",
        "\n",
        "    plt.suptitle('Comparison of Noise Schedules')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test your scheduler implementation\n",
        "scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
        "\n",
        "try:\n",
        "    scheduler.precompute_coefficients(\"linear\")\n",
        "\n",
        "    # Test with a sample image\n",
        "    test_image = next(iter(test_loader))[0][:1].to(device)  # Get one image\n",
        "    test_timesteps = torch.tensor([0, 250, 500, 750, 999]).to(device)\n",
        "\n",
        "    print(\"Testing noise addition...\")\n",
        "    for t in test_timesteps:\n",
        "        noisy_img, noise = scheduler.add_noise(test_image, torch.tensor([t]).to(device))\n",
        "        print(f\"Timestep {t}: Image range [{noisy_img.min():.3f}, {noisy_img.max():.3f}]\")\n",
        "\n",
        "    print(\"✓ Noise scheduler test passed!\")\n",
        "\n",
        "    # Visualize the forward process\n",
        "    timesteps_to_show = [0, 100, 300, 500, 700, 999]\n",
        "    print(\"Visualizing forward diffusion process...\")\n",
        "    visualize_forward_process(test_image, scheduler, timesteps_to_show)\n",
        "\n",
        "    # Compare different schedules (will show \"Not Implemented\" for unimplemented ones)\n",
        "    print(\"Comparing different noise schedules...\")\n",
        "    compare_noise_schedules(test_image, scheduler, timesteps_to_show,\n",
        "                           schedules=[\"linear\", \"cosine\", \"exponential\"])\n",
        "\n",
        "    # Reset to linear schedule for the rest of the lab\n",
        "    scheduler.precompute_coefficients(\"linear\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Noise scheduler test failed: {e}\")\n",
        "    print(\"Make sure to implement get_beta_schedule and precompute_coefficients!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251643c3",
      "metadata": {
        "id": "251643c3"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Implement Simple Denoising Network (25 minutes)\n",
        "\n",
        "### Task 4.1: Build a U-Net Denoiser\n",
        "\n",
        "**Your Mission**: Create a U-Net architecture that can predict noise given a noisy image and timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04da1908",
      "metadata": {
        "id": "04da1908"
      },
      "outputs": [],
      "source": [
        "class SimpleDenoiser(nn.Module):\n",
        "    def __init__(self, in_channels=1, time_embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "\n",
        "        # TODO: Implement U-Net architecture with time embedding\n",
        "\n",
        "        # Time embedding: converts timestep to feature vector\n",
        "        # This is provided as it's complex and not the main learning objective\n",
        "        self.time_embedding = nn.Sequential(\n",
        "            nn.Embedding(1000, time_embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_embed_dim, time_embed_dim)\n",
        "        )\n",
        "\n",
        "        # TODO: Implement Encoder (downsampling path)\n",
        "        # Architecture suggestion:\n",
        "        # encoder1: Conv2d(in_channels + time_embed_dim, 64) -> ReLU -> Conv2d(64, 64) -> ReLU\n",
        "        # encoder2: MaxPool2d(2) -> Conv2d(64, 128) -> ReLU -> Conv2d(128, 128) -> ReLU\n",
        "        # encoder3: MaxPool2d(2) -> Conv2d(128, 256) -> ReLU -> Conv2d(256, 256) -> ReLU\n",
        "\n",
        "        # TODO: Implement Bottleneck\n",
        "        # bottleneck: MaxPool2d(2) -> Conv2d(256, 512) -> ReLU -> Conv2d(512, 512) -> ReLU\n",
        "\n",
        "        # TODO: Implement Decoder (upsampling path)\n",
        "        # decoder3: ConvTranspose2d(512, 256) -> ReLU\n",
        "        # decoder3_conv: Conv2d(256 + 256, 256) -> ReLU -> Conv2d(256, 256) -> ReLU\n",
        "        # decoder2: ConvTranspose2d(256, 128) -> ReLU\n",
        "        # decoder2_conv: Conv2d(128 + 128, 128) -> ReLU -> Conv2d(128, 128) -> ReLU\n",
        "        # decoder1: ConvTranspose2d(128, 64) -> ReLU\n",
        "        # decoder1_conv: Conv2d(64 + 64, 64) -> ReLU -> Conv2d(64, 64) -> ReLU\n",
        "\n",
        "        # TODO: Implement Final output layer\n",
        "        # final: Conv2d(64, in_channels, 1)  # 1x1 conv to get back to input channels\n",
        "\n",
        "    def forward(self, x, timesteps):\n",
        "        \"\"\"\n",
        "        TODO: Implement forward pass with skip connections\n",
        "\n",
        "        Args:\n",
        "            x: noisy images (batch_size, channels, height, width)\n",
        "            timesteps: timestep for each image (batch_size,)\n",
        "\n",
        "        Returns:\n",
        "            predicted_noise: same shape as x\n",
        "\n",
        "        Steps:\n",
        "        1. Embed timesteps and expand to image dimensions\n",
        "        2. Concatenate time embedding with input\n",
        "        3. Pass through encoder (save features for skip connections)\n",
        "        4. Pass through bottleneck\n",
        "        5. Pass through decoder with skip connections\n",
        "        6. Return predicted noise\n",
        "        \"\"\"\n",
        "\n",
        "        # Get batch size and spatial dimensions\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Embed timesteps (provided - complex time embedding logic)\n",
        "        time_emb = self.time_embedding(timesteps)  # (batch_size, time_embed_dim)\n",
        "        time_emb = time_emb.view(batch_size, self.time_embed_dim, 1, 1)  # Add spatial dims\n",
        "        time_emb = time_emb.expand(-1, -1, height, width)  # Broadcast to image size\n",
        "\n",
        "        # Concatenate time embedding with input\n",
        "        x_with_time = torch.cat([x, time_emb], dim=1)\n",
        "\n",
        "        # TODO: Implement encoder path (save features for skip connections)\n",
        "        # enc1 = self.encoder1(x_with_time)\n",
        "        # enc2 = self.encoder2(enc1)\n",
        "        # enc3 = self.encoder3(enc2)\n",
        "\n",
        "        # TODO: Implement bottleneck\n",
        "        # bottleneck = self.bottleneck(enc3)\n",
        "\n",
        "        # TODO: Implement decoder path (with skip connections)\n",
        "        # Handle potential size mismatches with F.interpolate if needed\n",
        "        # dec3 = self.decoder3(bottleneck)\n",
        "        # dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection\n",
        "        # dec3 = self.decoder3_conv(dec3)\n",
        "\n",
        "        # TODO: Continue with decoder2 and decoder1\n",
        "\n",
        "        # TODO: Final output\n",
        "        # predicted_noise = self.final(dec1)\n",
        "\n",
        "        # return predicted_noise\n",
        "        pass\n",
        "\n",
        "# Test your denoiser implementation\n",
        "try:\n",
        "    denoiser = SimpleDenoiser().to(device)\n",
        "    test_noisy = torch.randn(4, 1, 28, 28).to(device)\n",
        "    test_timesteps = torch.randint(0, 1000, (4,)).to(device)\n",
        "\n",
        "    test_output = denoiser(test_noisy, test_timesteps)\n",
        "    print(f\"Denoiser input shape: {test_noisy.shape}\")\n",
        "    print(f\"Denoiser output shape: {test_output.shape}\")\n",
        "    print(f\"Timesteps: {test_timesteps}\")\n",
        "\n",
        "    # Verify the denoiser works correctly\n",
        "    assert test_output.shape == test_noisy.shape, f\"Expected {test_noisy.shape}, got {test_output.shape}\"\n",
        "\n",
        "    # Test that different timesteps produce different outputs\n",
        "    same_image = torch.randn(1, 1, 28, 28).to(device)\n",
        "    t1 = torch.tensor([100]).to(device)\n",
        "    t2 = torch.tensor([900]).to(device)\n",
        "\n",
        "    out1 = denoiser(same_image, t1)\n",
        "    out2 = denoiser(same_image, t2)\n",
        "\n",
        "    different_outputs = not torch.allclose(out1, out2, atol=1e-5)\n",
        "    print(f\"Different timesteps produce different outputs: {different_outputs}\")\n",
        "    assert different_outputs, \"Denoiser should produce different outputs for different timesteps!\"\n",
        "\n",
        "    print(\"✓ Denoiser architecture test passed!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Denoiser test failed: {e}\")\n",
        "    print(\"Make sure to implement the U-Net architecture and forward method!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f7ebad",
      "metadata": {
        "id": "f7f7ebad"
      },
      "source": [
        "### Task 4.2: Train the Denoising Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5f4dd79",
      "metadata": {
        "id": "f5f4dd79"
      },
      "outputs": [],
      "source": [
        "def train_denoiser(denoiser, scheduler, dataloader, epochs=3, device='cpu'):\n",
        "    \"\"\"\n",
        "    TODO: Implement training for the denoising model\n",
        "\n",
        "    Training process:\n",
        "    1. Take clean images from dataloader\n",
        "    2. Sample random timesteps for each image\n",
        "    3. Add noise using the scheduler\n",
        "    4. Train denoiser to predict the added noise\n",
        "    5. Use MSE loss between predicted and actual noise\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = torch.optim.Adam(denoiser.parameters(), lr=0.001)\n",
        "    denoiser.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, (clean_images, _) in enumerate(dataloader):\n",
        "            clean_images = clean_images.to(device)\n",
        "            batch_size = clean_images.size(0)\n",
        "\n",
        "            # TODO: Implement training step\n",
        "            # Steps:\n",
        "            # 1. Sample random timesteps: t = torch.randint(0, scheduler.num_timesteps, (batch_size,)).to(device)\n",
        "            # 2. Add noise: noisy_images, noise = scheduler.add_noise(clean_images, t)\n",
        "            # 3. Predict noise: predicted_noise = denoiser(noisy_images, t)\n",
        "            # 4. Compute MSE loss: loss = F.mse_loss(predicted_noise, noise)\n",
        "            # 5. Backpropagate: loss.backward(), optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # TODO: Your implementation here\n",
        "            pass\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item():.6f}')\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f'Epoch {epoch} completed. Average loss: {avg_loss:.6f}')\n",
        "\n",
        "        # Test denoising on a sample image\n",
        "        with torch.no_grad():\n",
        "            test_clean = clean_images[:4]\n",
        "            test_t = torch.tensor([250, 500, 750, 999]).to(device)\n",
        "            test_noisy, test_noise = scheduler.add_noise(test_clean, test_t)\n",
        "            predicted_noise = denoiser(test_noisy, test_t)\n",
        "\n",
        "            # Show results\n",
        "            visualize_denoising_results(test_clean, test_noisy, test_noise, predicted_noise, test_t, epoch)\n",
        "\n",
        "    return denoiser\n",
        "\n",
        "def visualize_denoising_results(clean, noisy, true_noise, predicted_noise, timesteps, epoch):\n",
        "    \"\"\"Visualize denoising results during training\"\"\"\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "\n",
        "    for i in range(4):\n",
        "        # Original clean image\n",
        "        axes[0, i].imshow(clean[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[0, i].set_title(f'Clean Image')\n",
        "        axes[0, i].axis('off')\n",
        "\n",
        "        # Noisy image\n",
        "        axes[1, i].imshow(noisy[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[1, i].set_title(f'Noisy (t={timesteps[i]})')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "        # True noise\n",
        "        axes[2, i].imshow(true_noise[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[2, i].set_title('True Noise')\n",
        "        axes[2, i].axis('off')\n",
        "\n",
        "        # Predicted noise\n",
        "        axes[3, i].imshow(predicted_noise[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[3, i].set_title('Predicted Noise')\n",
        "        axes[3, i].axis('off')\n",
        "\n",
        "    plt.suptitle(f'Denoising Results - Epoch {epoch}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Train your denoiser (uncomment after implementing)\n",
        "# print(\"Training Denoiser...\")\n",
        "# if 'scheduler' in locals() and hasattr(scheduler, 'betas'):\n",
        "#     trained_denoiser = train_denoiser(denoiser, scheduler, train_loader, epochs=3, device=device)\n",
        "# else:\n",
        "#     print(\"Please implement and test the noise scheduler first!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7a6ad4",
      "metadata": {
        "id": "4f7a6ad4"
      },
      "source": [
        "### Task 4.3: Implement Reverse Sampling Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "548ea69c",
      "metadata": {
        "id": "548ea69c"
      },
      "outputs": [],
      "source": [
        "def reverse_sampling(denoiser, scheduler, num_samples=16, device='cpu'):\n",
        "    \"\"\"\n",
        "    TODO: Implement the reverse sampling process to generate images\n",
        "\n",
        "    This is the magic of diffusion models - start with pure noise and gradually denoise!\n",
        "\n",
        "    Process:\n",
        "    1. Start with pure noise: x_T ~ N(0, I)\n",
        "    2. For t from T-1 down to 0:\n",
        "       a. Predict noise: predicted_noise = denoiser(x_t, t)\n",
        "       b. Remove predicted noise to get x_{t-1}\n",
        "       c. Add small amount of random noise (except at t=0)\n",
        "    3. Return final denoised images\n",
        "\n",
        "    Simplified reverse formula:\n",
        "    x_{t-1} = (x_t - sqrt(1-alpha_cumprod_t) * predicted_noise) / sqrt(alpha_cumprod_t)\n",
        "    \"\"\"\n",
        "\n",
        "    denoiser.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # TODO: Step 1 - Start with pure noise\n",
        "        # x = torch.randn(num_samples, 1, 28, 28).to(device)\n",
        "\n",
        "        # TODO: Step 2 - Reverse process loop\n",
        "        # for t in reversed(range(scheduler.num_timesteps)):\n",
        "        #     # Create timestep tensor for all samples\n",
        "        #     t_tensor = torch.full((num_samples,), t, dtype=torch.long).to(device)\n",
        "        #\n",
        "        #     # Predict noise\n",
        "        #     predicted_noise = denoiser(x, t_tensor)\n",
        "        #\n",
        "        #     # Remove noise (simplified formula)\n",
        "        #     alpha_cumprod_t = scheduler.alpha_cumprod[t]\n",
        "        #     sqrt_alpha_cumprod_t = torch.sqrt(alpha_cumprod_t)\n",
        "        #     sqrt_one_minus_alpha_cumprod_t = torch.sqrt(1 - alpha_cumprod_t)\n",
        "        #\n",
        "        #     x = (x - sqrt_one_minus_alpha_cumprod_t * predicted_noise) / sqrt_alpha_cumprod_t\n",
        "        #\n",
        "        #     # Add small random noise (except for final step)\n",
        "        #     if t > 0:\n",
        "        #         noise = torch.randn_like(x) * 0.1  # Small noise injection\n",
        "        #         x = x + noise\n",
        "        #\n",
        "        #     # Show progress every 200 steps\n",
        "        #     if t % 200 == 0:\n",
        "        #         print(f\"Reverse sampling step {t}\")\n",
        "        #         if t % 400 == 0:  # Show images every 400 steps\n",
        "        #             show_image_grid(x[:16], f\"Reverse Sampling - Step {t}\")\n",
        "\n",
        "        # TODO: Return final generated images\n",
        "        # return x\n",
        "        pass\n",
        "\n",
        "def compare_all_generators():\n",
        "    \"\"\"\n",
        "    Compare all implemented generation approaches\n",
        "    \"\"\"\n",
        "    print(\"=== Comparing All Generation Approaches ===\\n\")\n",
        "\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "\n",
        "    # 1. Direct Generator\n",
        "    if 'trained_generator' in locals():\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(4, trained_generator.latent_dim).to(device)\n",
        "            direct_samples = trained_generator(z)\n",
        "\n",
        "        for i in range(4):\n",
        "            axes[0, i].imshow(direct_samples[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "            axes[0, i].set_title('Direct Generator')\n",
        "            axes[0, i].axis('off')\n",
        "    else:\n",
        "        for i in range(4):\n",
        "            axes[0, i].text(0.5, 0.5, 'Direct Generator\\nNot Trained',\n",
        "                           ha='center', va='center', transform=axes[0, i].transAxes)\n",
        "            axes[0, i].axis('off')\n",
        "\n",
        "    # 2. Real MNIST (for reference)\n",
        "    real_samples = next(iter(test_loader))[0][:4]\n",
        "    for i in range(4):\n",
        "        axes[1, i].imshow(real_samples[i, 0], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[1, i].set_title('Real MNIST')\n",
        "        axes[1, i].axis('off')\n",
        "\n",
        "    # 3. Diffusion Generator (if trained)\n",
        "    if 'trained_denoiser' in locals() and 'scheduler' in locals():\n",
        "        try:\n",
        "            diffusion_samples = reverse_sampling(trained_denoiser, scheduler, num_samples=4, device=device)\n",
        "            for i in range(4):\n",
        "                axes[2, i].imshow(diffusion_samples[i, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "                axes[2, i].set_title('Diffusion Model')\n",
        "                axes[2, i].axis('off')\n",
        "        except:\n",
        "            for i in range(4):\n",
        "                axes[2, i].text(0.5, 0.5, 'Diffusion Model\\nSampling Failed',\n",
        "                               ha='center', va='center', transform=axes[2, i].transAxes)\n",
        "                axes[2, i].axis('off')\n",
        "    else:\n",
        "        for i in range(4):\n",
        "            axes[2, i].text(0.5, 0.5, 'Diffusion Model\\nNot Trained',\n",
        "                           ha='center', va='center', transform=axes[2, i].transAxes)\n",
        "            axes[2, i].axis('off')\n",
        "\n",
        "    # 4. Noise Progression (Forward Process)\n",
        "    test_img = real_samples[:1]\n",
        "    timesteps = [250, 500, 750, 999]\n",
        "    if 'scheduler' in locals() and hasattr(scheduler, 'betas'):\n",
        "        for i, t in enumerate(timesteps):\n",
        "            noisy_img, _ = scheduler.add_noise(test_img, torch.tensor([t]).to(device))\n",
        "            axes[3, i].imshow(noisy_img[0, 0].cpu(), cmap='gray', vmin=-1, vmax=1)\n",
        "            axes[3, i].set_title(f'Noise Process t={t}')\n",
        "            axes[3, i].axis('off')\n",
        "    else:\n",
        "        for i in range(4):\n",
        "            axes[3, i].text(0.5, 0.5, 'Noise Process\\nNot Implemented',\n",
        "                           ha='center', va='center', transform=axes[3, i].transAxes)\n",
        "            axes[3, i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test reverse sampling (uncomment after implementing denoiser)\n",
        "# print(\"Testing reverse sampling...\")\n",
        "# if 'trained_denoiser' in locals() and 'scheduler' in locals():\n",
        "#     generated_images = reverse_sampling(trained_denoiser, scheduler, num_samples=16, device=device)\n",
        "#     show_image_grid(generated_images, \"Diffusion Model Generated Images\")\n",
        "# else:\n",
        "#     print(\"Please train the denoiser first!\")\n",
        "\n",
        "# Compare all approaches\n",
        "compare_all_generators()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d96b484",
      "metadata": {
        "id": "3d96b484"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: Training Integration and Comparison (10 minutes)\n",
        "\n",
        "### Task 5.1: Complete Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbda062",
      "metadata": {
        "id": "1bbda062"
      },
      "outputs": [],
      "source": [
        "def full_training_pipeline():\n",
        "    \"\"\"\n",
        "    Complete training pipeline for diffusion model\n",
        "    \"\"\"\n",
        "    print(\"=== Full Diffusion Model Training Pipeline ===\\n\")\n",
        "\n",
        "    # Step 1: Initialize components\n",
        "    print(\"1. Initializing components...\")\n",
        "    scheduler = NoiseScheduler(num_timesteps=1000, device=device)\n",
        "\n",
        "    try:\n",
        "        scheduler.precompute_coefficients(\"linear\")\n",
        "        print(\"✓ Noise scheduler initialized\")\n",
        "    except:\n",
        "        print(\"❌ Noise scheduler implementation incomplete\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        denoiser = SimpleDenoiser().to(device)\n",
        "        # Test denoiser\n",
        "        test_x = torch.randn(2, 1, 28, 28).to(device)\n",
        "        test_t = torch.randint(0, 1000, (2,)).to(device)\n",
        "        test_out = denoiser(test_x, test_t)\n",
        "        assert test_out.shape == test_x.shape\n",
        "        print(\"✓ Denoiser initialized\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Denoiser implementation incomplete: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 2: Train denoiser\n",
        "    print(\"\\n2. Training denoiser...\")\n",
        "    try:\n",
        "        trained_denoiser = train_denoiser(denoiser, scheduler, train_loader, epochs=2, device=device)\n",
        "        print(\"✓ Denoiser training completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Denoiser training failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Generate samples\n",
        "    print(\"\\n3. Generating samples...\")\n",
        "    try:\n",
        "        generated_samples = reverse_sampling(trained_denoiser, scheduler, num_samples=16, device=device)\n",
        "        show_image_grid(generated_samples, \"Final Generated Images\")\n",
        "        print(\"✓ Sample generation completed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Sample generation failed: {e}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n🎉 Full pipeline completed successfully!\")\n",
        "    return trained_denoiser, scheduler\n",
        "\n",
        "# Run full pipeline (uncomment after implementing all components)\n",
        "# trained_model, trained_scheduler = full_training_pipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de44088",
      "metadata": {
        "id": "4de44088"
      },
      "source": [
        "### Task 5.2: Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b78dcb5",
      "metadata": {
        "id": "7b78dcb5"
      },
      "outputs": [],
      "source": [
        "def analyze_model_performance():\n",
        "    \"\"\"\n",
        "    Analyze and compare different generation approaches\n",
        "    \"\"\"\n",
        "    print(\"=== Model Performance Analysis ===\\n\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # 1. Direct Generator Analysis\n",
        "    if 'trained_generator' in locals():\n",
        "        print(\"Analyzing Direct Generator...\")\n",
        "        direct_results = evaluate_generator(trained_generator, device, num_samples=100)\n",
        "        results['direct'] = direct_results\n",
        "\n",
        "        print(f\"Direct Generator Results:\")\n",
        "        print(f\"  Mean difference from real: {direct_results['mean_diff']:.4f}\")\n",
        "        print(f\"  Std difference from real: {direct_results['std_diff']:.4f}\")\n",
        "        print(f\"  Diversity score: {direct_results['diversity']:.4f}\")\n",
        "\n",
        "    # 2. Diffusion Model Analysis\n",
        "    if 'trained_model' in locals():\n",
        "        print(\"\\nAnalyzing Diffusion Model...\")\n",
        "        try:\n",
        "            diffusion_samples = reverse_sampling(trained_model, trained_scheduler, num_samples=100, device=device)\n",
        "\n",
        "            # Compute similar metrics\n",
        "            sample_mean = torch.mean(diffusion_samples).item()\n",
        "            sample_std = torch.std(diffusion_samples).item()\n",
        "\n",
        "            real_images = next(iter(test_loader))[0][:100]\n",
        "            real_mean = torch.mean(real_images).item()\n",
        "            real_std = torch.std(real_images).item()\n",
        "\n",
        "            diffusion_results = {\n",
        "                'mean_diff': abs(sample_mean - real_mean),\n",
        "                'std_diff': abs(sample_std - real_std),\n",
        "                'diversity': 0.0  # Would need to compute pairwise distances\n",
        "            }\n",
        "            results['diffusion'] = diffusion_results\n",
        "\n",
        "            print(f\"Diffusion Model Results:\")\n",
        "            print(f\"  Mean difference from real: {diffusion_results['mean_diff']:.4f}\")\n",
        "            print(f\"  Std difference from real: {diffusion_results['std_diff']:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Diffusion analysis failed: {e}\")\n",
        "\n",
        "    # 3. Comparison\n",
        "    if len(results) > 1:\n",
        "        print(f\"\\n=== Comparison ===\")\n",
        "        for method, metrics in results.items():\n",
        "            print(f\"{method.capitalize()} Generator:\")\n",
        "            print(f\"  Overall score: {metrics['mean_diff'] + metrics['std_diff']:.4f} (lower is better)\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run performance analysis (uncomment after training models)\n",
        "# performance_results = analyze_model_performance()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52846dc0",
      "metadata": {
        "id": "52846dc0"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Reflection and Next Steps (5 minutes)\n",
        "\n",
        "### Task 6.1: Lab Reflection\n",
        "\n",
        "**Discussion Questions** (Work with your partner):\n",
        "\n",
        "1. **Architecture Comparison**:\n",
        "   - Which generator produced better quality images?\n",
        "   - What are the trade-offs between direct and diffusion-based generation?\n",
        "\n",
        "2. **Training Challenges**:\n",
        "   - What was the most difficult part to implement?\n",
        "   - How did the training dynamics differ between approaches?\n",
        "\n",
        "3. **Mathematical Understanding**:\n",
        "   - How does the noise schedule affect image quality?\n",
        "   - Why does the reverse sampling process work?\n",
        "\n",
        "4. **Practical Insights**:\n",
        "   - What would you change to improve generation quality?\n",
        "   - How might these techniques scale to larger, more complex images?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec89b86",
      "metadata": {
        "id": "9ec89b86"
      },
      "outputs": [],
      "source": [
        "def summarize_lab_achievements():\n",
        "    \"\"\"\n",
        "    Summary of what you've accomplished today\n",
        "    \"\"\"\n",
        "    print(\"=== Lab 1 Achievements Summary ===\\n\")\n",
        "\n",
        "    achievements = [\n",
        "        \"🏗️  Built a direct neural network generator\",\n",
        "        \"📊 Implemented statistical matching training\",\n",
        "        \"🔄 Created progressive noise addition schedules\",\n",
        "        \"🧠 Constructed a U-Net denoising architecture\",\n",
        "        \"⚡ Implemented reverse diffusion sampling\",\n",
        "        \"📈 Compared multiple generation approaches\",\n",
        "        \"🤝 Formed your course project team\"\n",
        "    ]\n",
        "\n",
        "    print(\"Core implementations completed:\")\n",
        "    for achievement in achievements:\n",
        "        print(f\"  {achievement}\")\n",
        "\n",
        "    print(f\"\\n🎓 Technical skills developed:\")\n",
        "    print(f\"   • Understanding of generative model fundamentals\")\n",
        "    print(f\"   • Hands-on experience with U-Net architectures\")\n",
        "    print(f\"   • Practical knowledge of diffusion processes\")\n",
        "    print(f\"   • Training and evaluation of generative models\")\n",
        "\n",
        "    print(f\"\\n🔬 Key insights gained:\")\n",
        "    print(f\"   • Why direct generation is challenging\")\n",
        "    print(f\"   • How progressive noise addition helps training\")\n",
        "    print(f\"   • The power of reverse diffusion sampling\")\n",
        "    print(f\"   • Trade-offs between different approaches\")\n",
        "\n",
        "    # Create a visual summary\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "\n",
        "    # Progress flowchart\n",
        "    stages = {\n",
        "        'Problem': ['Direct Generation\\nChallenges'],\n",
        "        'Solution 1': ['Statistical\\nMatching'],\n",
        "        'Solution 2': ['Progressive\\nNoise Addition'],\n",
        "        'Architecture': ['U-Net\\nDenoiser'],\n",
        "        'Training': ['Noise Prediction\\nObjective'],\n",
        "        'Inference': ['Reverse\\nSampling']\n",
        "    }\n",
        "\n",
        "    y_positions = [0.8, 0.65, 0.5, 0.35, 0.2, 0.05]\n",
        "    colors = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\n",
        "\n",
        "    for i, (stage, concepts) in enumerate(stages.items()):\n",
        "        y = y_positions[i]\n",
        "        color = colors[i]\n",
        "\n",
        "        # Stage label\n",
        "        ax.text(0.1, y, stage, fontsize=14, weight='bold', color=color)\n",
        "\n",
        "        # Concepts\n",
        "        for j, concept in enumerate(concepts):\n",
        "            x_pos = 0.3 + j * 0.2\n",
        "            ax.text(x_pos, y, concept, fontsize=11, ha='center',\n",
        "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=color, alpha=0.3))\n",
        "\n",
        "    # Add arrows showing progression\n",
        "    for i in range(len(y_positions) - 1):\n",
        "        ax.arrow(0.5, y_positions[i] - 0.05, 0, -0.05, head_width=0.02, head_length=0.01,\n",
        "                fc='gray', ec='gray', alpha=0.7)\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_title('Lab 1: From Direct Generation to Diffusion Models', fontsize=16, weight='bold')\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create final summary\n",
        "summarize_lab_achievements()\n",
        "\n",
        "print(\"\\n🎯 Next Steps:\")\n",
        "print(\"   • Lab 2: Mathematical foundations and forward process optimization\")\n",
        "print(\"   • Lab 3: Advanced architectures and training techniques\")\n",
        "print(\"   • Lab 4: State-of-the-art diffusion models and applications\")\n",
        "print(\"   • Mini-project: Apply diffusion models to your chosen domain\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a8cad8",
      "metadata": {
        "id": "d6a8cad8"
      },
      "source": [
        "---\n",
        "\n",
        "## Implementation Checklist\n",
        "\n",
        "### Core Implementations (Students Must Complete):\n",
        "\n",
        "**✅ Essential TODOs:**\n",
        "- [ ] `DirectGenerator` architecture and forward pass\n",
        "- [ ] Direct generator training loop with statistical matching\n",
        "- [ ] `get_beta_schedule()` for linear noise schedule\n",
        "- [ ] `precompute_coefficients()` for diffusion math\n",
        "- [ ] `add_noise()` forward diffusion implementation\n",
        "- [ ] `SimpleDenoiser` U-Net architecture and forward pass\n",
        "- [ ] Denoiser training loop with noise prediction\n",
        "- [ ] `reverse_sampling()` generation process\n",
        "\n",
        "**✅ Provided Starter Code:**\n",
        "- [ ] All visualization functions (image grids, forward process, comparisons)\n",
        "- [ ] Evaluation and analysis infrastructure\n",
        "- [ ] Training progress monitoring\n",
        "- [ ] Performance comparison tools\n",
        "- [ ] Mathematical helper functions\n",
        "\n",
        "---\n",
        "\n",
        "## Submission Requirements\n",
        "\n",
        "### What to Submit\n",
        "\n",
        "Submit your completed Jupyter notebook (.ipynb file) with:\n",
        "\n",
        "**✅ Working Implementations:**\n",
        "- Functional direct generator with training\n",
        "- Complete noise scheduler with linear schedule\n",
        "- Working U-Net denoiser architecture\n",
        "- Successful reverse sampling implementation\n",
        "\n",
        "**✅ Generated Results:**\n",
        "- Sample images from direct generator\n",
        "- Forward diffusion process visualization\n",
        "- Denoised images during training\n",
        "- Final generated images from diffusion model\n",
        "\n",
        "**✅ Analysis and Comparison:**\n",
        "- Performance metrics for both approaches\n",
        "- Discussion of results and observations\n",
        "- Identification of strengths/limitations\n",
        "\n",
        "**✅ Code Quality:**\n",
        "- Clean, commented implementations\n",
        "- Proper error handling\n",
        "- Clear variable naming\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}